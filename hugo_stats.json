{
  "htmlElements": {
    "tags": [
      "a",
      "article",
      "blockquote",
      "body",
      "br",
      "button",
      "circle",
      "code",
      "defs",
      "details",
      "div",
      "em",
      "footer",
      "h1",
      "h2",
      "h3",
      "h4",
      "h5",
      "head",
      "header",
      "html",
      "img",
      "input",
      "li",
      "line",
      "lineargradient",
      "link",
      "main",
      "meta",
      "ol",
      "p",
      "path",
      "polyline",
      "pre",
      "rect",
      "script",
      "section",
      "span",
      "stop",
      "strong",
      "summary",
      "svg",
      "table",
      "tbody",
      "td",
      "th",
      "thead",
      "title",
      "tr",
      "ul"
    ],
    "classes": [
      "absolute",
      "active",
      "bg-blue-100",
      "bg-gray-100",
      "bg-white",
      "border",
      "border-b",
      "border-gray-200",
      "border-transparent",
      "bottom-44",
      "capitalize",
      "center",
      "close-search",
      "container",
      "cursor-pointer",
      "dark:bg-gray-700",
      "dark:bg-gray-800",
      "dark:bg-gray-900",
      "dark:border-gray-700",
      "dark:border-transparent",
      "dark:hover:bg-gray-700",
      "dark:hover:border-gray-300",
      "dark:hover:text-gray-300",
      "dark:hover:text-white",
      "dark:prose-dark",
      "dark:text-black",
      "dark:text-gray-400",
      "dark:text-gray-600",
      "dark:text-gray-800",
      "dark:text-white",
      "disabled",
      "flex",
      "flex-1",
      "flex-col",
      "flex-wrap",
      "font-bold",
      "font-extrabold",
      "font-light",
      "font-medium",
      "font-semibold",
      "gap-2",
      "gap-4",
      "gap-x-3",
      "gap-y-2",
      "grid",
      "grid-cols-1",
      "h-52",
      "h-8",
      "h-full",
      "hidden",
      "highlight",
      "hover:bg-gray-200",
      "hover:border-gray-800",
      "hover:text-gray-800",
      "hover:text-gray-900",
      "hover:underline",
      "icon",
      "icon-tabler",
      "icon-tabler-chevron-down",
      "inline-block",
      "items-center",
      "items-end",
      "justify-between",
      "justify-center",
      "language-dropdown",
      "language-switcher",
      "left-0",
      "lg:gap-16",
      "lg:gap-6",
      "lg:gap-8",
      "lg:grid-cols-3",
      "lg:prose-lg",
      "max-w-3xl",
      "max-w-4xl",
      "max-w-5xl",
      "max-w-full",
      "max-w-screen-lg",
      "mb-2",
      "mb-4",
      "mb-8",
      "md:flex",
      "md:flex-row",
      "md:grid-cols-2",
      "md:hidden",
      "md:items-center",
      "md:justify-between",
      "md:p-0",
      "md:relative",
      "md:w-auto",
      "min-h-screen",
      "ml-2",
      "mobile-menu",
      "mobile-menu-button",
      "mr-1",
      "mt-16",
      "mt-2",
      "mt-4",
      "mt-6",
      "mt-8",
      "mx-1",
      "mx-auto",
      "my-2",
      "my-4",
      "my-8",
      "no-results",
      "object-contain",
      "object-cover",
      "opacity-60",
      "open-search",
      "p-1",
      "p-12",
      "p-2",
      "p-6",
      "page-item",
      "page-link",
      "pagination",
      "pb-4",
      "pb-6",
      "place-items-center",
      "prose",
      "pt-6",
      "px-2",
      "px-3",
      "px-4",
      "px-6",
      "px-8",
      "py-0.5",
      "py-1",
      "py-12",
      "py-2",
      "py-3",
      "py-32",
      "py-4",
      "relative",
      "right-0",
      "right-4",
      "right-8",
      "rotate-6",
      "rounded",
      "rounded-full",
      "rounded-lg",
      "search-list",
      "search-results",
      "search-ui",
      "select-none",
      "shadow",
      "shadow-sm",
      "space-y-2",
      "text-2xl",
      "text-3xl",
      "text-4xl",
      "text-black",
      "text-center",
      "text-gray-500",
      "text-gray-800",
      "text-gray-900",
      "text-lg",
      "text-sm",
      "text-white",
      "text-xl",
      "toc",
      "toggle-dark-mode",
      "top-0",
      "top-4",
      "top-full",
      "transform",
      "transition-colors",
      "w-1/2",
      "w-full",
      "z-10"
    ],
    "ids": [
      "1-grobid-介绍",
      "1-下载graphrag",
      "1-下载数据",
      "1-什么是autogen",
      "1-先验分布",
      "1-克隆项目",
      "1-增加故障点原地恢复的功能",
      "1-安装环境",
      "1-构造带引用的长上下文问答任务",
      "1-模型初始化",
      "1-正态分布的表示",
      "1-理解covariance-matrix",
      "1-腾讯元宝",
      "1-训练代码实现",
      "11-多变量高斯分布",
      "11-常见的高斯核",
      "11-核心需求",
      "12-可调节参数的高斯核又被叫做isotropic-squared-exponential-kernel",
      "12-技术可行性",
      "12-看图可知",
      "2-github-copilot",
      "2-grobid-安装",
      "2-准备好ollama模型",
      "2-初始化",
      "2-加载预训练权重",
      "2-后验分布-posterior",
      "2-多元高斯分布的边缘分布和条件分布",
      "2-安装环境",
      "2-开发全链路追踪",
      "2-数据集预处理",
      "2-构建基准测试longbench-cite",
      "2-核函数是什么有什么类型",
      "2-核心概念",
      "2-评估微调后的模型",
      "21-可定制化智能体customizable-and-conversable-agents",
      "22-对话编程范式conversation-programming",
      "3-grobid-使用",
      "3-下载meta发布的原生的llama模型",
      "3-从高斯分布抽样",
      "3-修改模型结构",
      "3-其他基础知识",
      "3-创建数据加载器",
      "3-在新数据上使用模型进行推理",
      "3-已知先验知识如何计算后验分布",
      "3-智能体能力体系",
      "3-生成索引",
      "3-看模型微调之前的结果和loss",
      "3-评估",
      "3-部署的时候要考虑用户正在使用",
      "31-web-端",
      "31-三大能力来源",
      "311-大型语言模型能力",
      "312-人类参与能力",
      "313-工具执行能力",
      "32-api-调用",
      "32-智能体分类体系",
      "4--训练代码实现",
      "4-grobid_client的语法",
      "4-同步执行与异步执行",
      "4-实现评估工具",
      "4-对话编程机制",
      "4-将原版的转换成hf格式",
      "41-编程模式",
      "42-自动回复机制",
      "43-控制机制",
      "5-grobid_client的参数解释",
      "5-上下文管理",
      "5-总结",
      "TableOfContents",
      "a",
      "ablation-study-消融实验",
      "agent-记忆借鉴人类记忆的部分",
      "ai-agent运作的原理",
      "alce-automatic-long-form-citation-evaluation-自动长文本引用评估",
      "batching-批处理策略",
      "benchmark",
      "benchmark-longbench-cite",
      "blog",
      "coala",
      "cofcoarse-to-fine",
      "contribution",
      "cross-lingual-retrieval",
      "dataset-sources",
      "dataset类",
      "dataset预处理",
      "decoder",
      "decoder和encoder对比",
      "dense-retrieval密集检索",
      "die-schatten-werden-länger",
      "embedding-models",
      "encoder",
      "encoder-decoder",
      "epoch",
      "exact-gp",
      "exact-gp-and-approximate-gp",
      "future-work",
      "gpregressor",
      "gpt的几个概念",
      "grouped-tasks",
      "hub上的数据集",
      "ich-gehoer-mir",
      "kitsch",
      "llm-as-judge",
      "mama-wo-bist-du",
      "milch",
      "mrl",
      "multi-agent-research-system擅长什么",
      "multi-agent-research-system有什么缺点",
      "multi-agent-提示工程实践",
      "multi-agent-的架构",
      "multi-agent的优劣势",
      "multi-lingual-retrieval",
      "multi-stage-training",
      "multi-vector-retrieval多向量检索",
      "multilingual-long-doc-retrieval",
      "orchestrator-worker模式解析",
      "part-1-benchmarking-agents-via-digital-automation",
      "part-2-building-language-agents-that-reason-to-act",
      "part-3-principled-framework-for-language-agents",
      "prolog",
      "qkv从矩阵的角度看self-attention",
      "react",
      "rerank-方法",
      "reranking-models",
      "rlhf",
      "sd环节",
      "self-attention",
      "self-instruct",
      "self-knowledge-distillation",
      "seq2seq",
      "sharegpt",
      "single-prompt",
      "sparselexical-retrieval稀疏检索",
      "spherical-linear-interpolation-球状线性差值slerp",
      "table-of-contents",
      "teach-and-discover-knowledge",
      "tokenizer的整个工作过程",
      "tools",
      "top-k-采样",
      "train-models-for-agents",
      "training-process",
      "tree-of-thoughts",
      "webshop",
      "wenn-ich-tanzen-will",
      "一",
      "一kan是什么",
      "一个vector对应一个label的情况即输入和输出一样多也叫做sequence-labeling",
      "一前情提要",
      "一打好框架构建dummy类",
      "一记忆的形成和存储",
      "一设置docker-repository",
      "一配置环境",
      "七gpt将输出张量转为文本",
      "三",
      "三-大模型帮你理解kan网络",
      "三gelu函数和前馈神经网络",
      "三个阶段对应三个agent",
      "三参考",
      "三训练",
      "三配置docker环境",
      "不同的向量检索方法",
      "两个问题",
      "为什么general-text-embedding发展如此迅速",
      "为什么把陌生人作为切入口",
      "为什么研究性的工作适合用multi-agent-来做",
      "为什么要在乎另一个人的上下文",
      "二",
      "二kan能做什么",
      "二安装docker-engine",
      "二安装redash",
      "二层归一化",
      "二短期记忆和长期记忆",
      "二评估文本生成",
      "五transformer块",
      "交互流程",
      "交叉熵损失的计算",
      "人工评估的重要性",
      "人类评估",
      "什么决定multi-agent的性能",
      "从小样本集上就开始做评估",
      "以retrieval-任务数据合成为例",
      "何园",
      "佩妮阿姨为什么命运偏爱的不是她",
      "使用步骤",
      "先决条件",
      "先创建集合create-collection",
      "八了解模型架构及其规模",
      "六gpt架构的真正实现",
      "共鸣",
      "关于交叉熵损失的理解",
      "关键概念",
      "关键概念-1",
      "关键概念-2",
      "关键概念-3",
      "其他",
      "内积方法",
      "准备数据",
      "划分数据集split",
      "创建collection",
      "初唐境界是人生最大的意义",
      "初始化",
      "剧外八卦",
      "剧情",
      "单变量高斯分布",
      "参考",
      "参考文献",
      "反向传播",
      "反思",
      "反馈模型reward-model",
      "可以躺平",
      "同步执行优点",
      "同步执行挑战",
      "和深度学习训练不一样的地方",
      "哈希值进行完整性校验",
      "四",
      "四参考",
      "四处看看",
      "四快捷连接",
      "因果注意力causal-attention",
      "因果注意力机制",
      "困惑度",
      "坚持走",
      "多变量高斯分布",
      "多头注意力机制",
      "多头注意力机制代码例子",
      "大模型-finetune-的两个路线",
      "大运河博物馆",
      "如何做训练",
      "如何将向量模型从english拓展到multi-lingual",
      "如何形成对生活的理解",
      "如何更有效的利用人类的反馈",
      "如何评估trae-agent",
      "如果label是string怎么变成id",
      "学习目标",
      "实践",
      "实践-1",
      "实践-2",
      "实践-3",
      "实验",
      "对文本向量化和存储embed_text",
      "寻求最佳policy",
      "导入数据",
      "导入数据import-data",
      "导入数据集",
      "带可训练权重的自注意力机制缩放点积注意力-scaled-dot-product-attention",
      "序列决策-sequential-decision-making",
      "序列经过模型处理之后的数据是什么",
      "应用",
      "异步执行优点",
      "异步执行挑战",
      "强化学习的难题",
      "很远也能到达",
      "德拉科一个孤独的灵魂如何幸福",
      "快也是慢",
      "怎么得到wqwk和wv",
      "总结",
      "总结-1",
      "总结三个阶段",
      "打分规则",
      "换房的决定",
      "提前准备",
      "搬家后的忙碌",
      "数据",
      "数据不均衡",
      "数据合成",
      "数据均衡",
      "数据过滤",
      "数据集-longcite-45k",
      "文本生成前几章讲过的",
      "文档",
      "新生活的开始",
      "方法一确定关键输入对输出的影响",
      "方法二找出影响输出的关键训练语料",
      "智能体评估",
      "最后的目标",
      "术语和概念",
      "权重参数",
      "构建大语言模型的三个阶段",
      "架构图",
      "检索",
      "检索生成",
      "概览",
      "模型",
      "模型-longcite-8b-和-longcite-9b",
      "模型效果",
      "模型输入的数据",
      "模型输出的数据",
      "波特家族",
      "洛哈特太想出名了怎么办",
      "消融实验",
      "温度缩放",
      "爱",
      "独一无二",
      "珍惜线下",
      "理解输入与输出",
      "生产可靠性和工程挑战",
      "生态工具",
      "用map来对批量数据进行tokenzie",
      "痛苦的卷",
      "瘦西湖",
      "盛唐",
      "目标要大",
      "相加方法",
      "知名的ai-agent",
      "短期记忆",
      "短期记忆是如何转化为长期记忆的",
      "离别",
      "第一个任务电子衣橱",
      "第一天大运河博物馆东关街",
      "第一种方法",
      "第一类方法直接对transformer进行分析",
      "第一阶段准备数据",
      "第一阶段数据准备",
      "第一阶段的局限性",
      "第一阶段自我学习积累实力",
      "第三个任务做ppt",
      "第三天何园皮市街朱自清故居钟书阁",
      "第三阶段参与实战打磨技巧",
      "第三阶段微调模型",
      "第三阶段的训练细节",
      "第三阶段评估大语言模型",
      "第二个任务把河童做成绘本",
      "第二天瘦西湖鉴真樱花大道",
      "第二种方法",
      "第二类方法直接请语言模型提供解释",
      "第二阶段修改模型",
      "第二阶段名师指点发挥潜力",
      "第二阶段模型微调",
      "第二阶段的局限性",
      "简化版的自注意力机制",
      "简单最好",
      "细节",
      "经典文章",
      "结构和原理",
      "结果",
      "编码器-解码器",
      "背景",
      "背景知识",
      "自注意模块计算两个向量的关联性",
      "营造社区",
      "要有目标",
      "计算covariance",
      "计算mean",
      "训练",
      "训练目标",
      "训练细节",
      "训练细节-1",
      "训练资料的组成部分",
      "设置温度",
      "评估什么",
      "评估和结果",
      "语料",
      "语言模型的三个阶段",
      "课程",
      "赢在起点",
      "输入单序列",
      "输入单序列与输入多序列",
      "运作原理",
      "通勤的烦恼",
      "通过api调用",
      "通过hugging-face导入",
      "那么该如何看见陌生人呢",
      "重新学习深度学习的感想",
      "鉴真樱花大道",
      "长期记忆",
      "阶段一补丁生成",
      "阶段三补丁选择",
      "阶段二补丁修剪",
      "马尔福家族"
    ]
  }
}
