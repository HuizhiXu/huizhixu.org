
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"author":"Huizhi","categories":null,"contents":"市面上的Redash教程太混乱了，官方发布了不同的安装方式，但是写得不是很明白。基本上都会有一个重复安装和卸载的过程，是正常的。\n这次安装的经验就是：\n千万不要从Redash的Github Master分支上拉代码，比较痛苦。\n考虑用不用Docker部署的条件是：看需不需要进行二次开发，不需要就可以进行Docker部署\nCentOS也是一个类Linux的系统，和Ubuntu一样。注意它不是指mac的操作系统。\n一、配置环境 系统环境（这个仅供参考） 系统版本： Ubuntu-22.04 目标安装目录： /opt/redash Postgresql账号/密码： postgres/abcdef123456 环境变量env文件： /opt/redash/.env 配置docker环境 #将当前用户加入docker组 sudo usermod -aG docker $USER #启动docker服务并配置自启 sudo systemctl start docker \u0026amp;\u0026amp; sudo systemctl enable docker 二、安装Redash 选定安装目录，这里是/opt/redash sudo mkdir /opt/redash sudo chown -R ${USER} /opt/redash cd /opt/redash 创建env文件，写入下列内容 #/opt/redash/env/内容 PYTHONUNBUFFERED=0 REDASH_LOG_LEVEL=INFO REDASH_REDIS_URL=redis://redis:6379/0 POSTGRES_PASSWORD=aaa123456 REDASH_COOKIE_SECRET=wo3urion23i4un2l34jm2l34k REDASH_SECRET_KEY=u2o34nlfksjelruirk REDASH_DATABASE_URL=\u0026#34;postgresql://postgres:abcdef123456@postgres/postgres\u0026#34; ORACLE_HOME=\u0026#34;/usr/lib/oracle/12.2/client64\u0026#34; LD_LIBRARY_PATH=\u0026#34;/usr/lib/oracle/12.2/client64/lib\u0026#34; REDASH_FEATURE_ALLOW_CUSTOM_JS_VISUALIZATIONS=\u0026#34;true\u0026#34; REDASH_ADDITIONAL_QUERY_RUNNERS=\u0026#34;redash.query_runner.oracle,redash.query_runner.python\u0026#34; 创建docker-compose.yml，写入下列内容 这里我只改了image的内容:image: redash/redash:10.1.0.b50633，这个image是在github的release分支上 找到的。\nversion: \u0026#34;2\u0026#34; x-redash-service: \u0026amp;redash-service #现在image的值为中文开源版的tag如果要使用官方的镜像，在docker hub上查看官方tag，然后替换。 image: image: redash/redash:10.","date":"2023-07-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230720redash%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95-copy/","tags":["tech","Redash"],"title":"2023-07-20Redash V10安装（在Ubuntu系统上用docker部署安装）"},{"author":"Huizhi","categories":null,"contents":"一、设置Docker Repository 升级apt-get到最新 sudo apt-get update sudo apt-get install ca-certificates curl gnupg 添加Docker的官方GPG key sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg 设置仓库 echo \\ \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 二、安装Docker Engine 升级apt-get到最新 sudo apt-get update 安装最新版本的Docker Engine， containerd和Docker Compose sudo apt-get install docker-ce docker-ce-cli containerd.","date":"2023-07-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230719ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85docker/","tags":["tech","Redash"],"title":"2023-07-19Ubuntu上安装Docker"},{"author":"Huizhi","categories":null,"contents":"想了好久要不要写这一篇，因为它传述的价值观和人生观和我们（全世界）的教育都有点不一样。而我也是看了才发现，还能这样？\n大家小时候的教育，都是叫我们认真学习，对人有礼貌，尊敬老师，关心同学。\n长大了，要认真工作，热爱生活，对人友好（包含孝敬父母、广交朋友），成家立业。\n如果上面不能做到，那至少要做到作息规律，不点外卖。\n如果还不能做到，那至少态度要做到积极向上。\n假设有一个人犯了罪，大家也还是希望他能够改过自新，积极生活。“改过自新”这四个字，就有一种积极的感觉在里面。\n这种积极的观念深入人心，使得整个社会在人生态度上的中立面，是倾向积极这方面的。\n但是恒子奶奶，一个92岁高龄的日本女士，在书里面传达的是让人更加轻松的人生态度。\n书名叫做《宝藏系列：人生通透+活得通透》，微信读书APP有。\n1. 在工作中是否有过人际关系的烦恼？\n恒子奶奶答：还没有碰过严重的问题。因为我觉得最根本的是，人不要有太多的期望。\n比如，“我想如何”，“给我什么”之类的话，尽量不要多说。\n无论是职场还是家庭，“只要是别人给予的东西，自己就应该感谢对方”。如果以这样的心态和别人相处，人际关系就不可能出现大问题。\n如果在工作生活中埋怨别人不作为，不帮忙，“上司不指导我”，“同事不带我”，认为“下属就该如此”，或者“上司应该这么做”，有这种“别人这样做是理所当然”的想法，那么一定会失落和烦恼。\n因为每个人除了职责，他的角色本质是“人”。在现在这个时代，一定要尊重别人的“人”性。\n2. 在养育孩子方面，要有“追求家庭和睦”的想法。\n其他家庭都这么做，世间大众都这么做，“别人家都去了夏令营/补课/学钢琴，我们家小孩去不了，太焦虑了。”\n父母彼此的关系，婆媳关系出现问题，“为什么自己的丈夫不帮忙做家务，孩子成绩不好？为什么别人家都有保姆或者长辈来带小孩？”。\n一旦和别人家比较，或者家里发生纠纷、家庭氛围不好，就会影响孩子们的心理状态。维持家庭和睦，放弃“为什么只有自己家这样”的想法，这样焦虑才不会影响孩子们。就算给予不了孩子们太多，但是只要让他们感受到父母在爱护他们，他们依然是有安全感和满足的。\n家庭和睦带给孩子人生的裨益更多。\n3. 您不认为要通过工作实现自我价值，只要维持生活就可以了吗？\n恒子奶奶：是的，我既没有想要出人头地，又不求他人认可。只要完成领导安排的工作，并能获得报酬，我就心满意足了。现在有很多人被“自我实现”的观念所困扰，对他人和自己的要求都很高。\n4. 面对性格不合的人或没有礼貌的人，你是否会感到心烦意乱呢？\n恒子奶奶：在任何场合都可能碰到没有礼貌或者性格不合的人，我唯一能做的就是保持距离，尽量减少与他们相处。想要改变别人很难，毕竟我也不是什么了不起的人，没有资格要求别人改变。\n5. 如果不可避免要经常打交道的同事没有完成任务，您会怎么办？\n恒子奶奶：遇到这种情况，我会通过沟通来解决。首先，我会避免使用攻击性语言。比如，”我是这样考虑的，你呢“，”你那样做有什么理由吗“等。\n如果确实需要对方改变，可以建议他，”我认为这种方法更好，你可否试着改变一下“，并且需要注意，我们在提出要求时不要指责对方，而是拜托对方。不要从一开始就把对方视为敌人，也不要一味地指责对方，而是在沟通中了解彼此的立场。\n6. 有时候朋友越多烦恼越多\n恒子奶奶：我认为最好不要跟合不来的人交往。当今时代，社交网络十分发达，人们很容易建立”广而浅“的人际关系，但是人际关系复杂会带来压力。要意识到我们被灌输了”必须拓展人脉，广交朋友“的观念。\n有的人不知不觉就成为了朋友，而且交往了很久。\n有的人短暂联系，也没有关系。\n7. 独处并不丢人，是自然的事情。\n有一些人在周末独自度过的时候，在社交网络上看到朋友和别人愉快地吃饭玩耍的照片，于是内心厌恶自己。\n实际上，他人如何生活，对我丝毫不重要。有的人虽然朋友众多，表面上很开心，但内心却小心翼翼，局促不安。如果是独自一人，就不必顾忌他人，可以悠闲舒适地度日。\n而且，有些人在独处的时候，总想着要做点什么，如果不做点什么，就觉得自己虚度了时光。“必须做什么”这样的想法让人无法享受独处，其实，什么都不用提前计划去做，在独处的过程中想到什么就去做，这样才会感受到自由。\n上面就是看书的时候对我很有帮助的认识。\n在职场中，我们经常会有期望。我们期待一个良好的环境，老板战略清晰，同事可靠，流程清晰且不冗余，大家能和谐相处。\n实际的职场环境里，老板的想法东一榔头西一棒子，同事连简单的任务都完成不好，流程这个东西要么不存在，要么就等上几个月，和同事之间因为利益冲突而勾心斗角。成年人生活在一个这么残忍的环境，怎么会没有压力呢？\n而恒子奶奶的这套做法，既不要求自我实现，又对别人没有期待，也不用广交朋友，还能享受独处。这样不仅职场压力没有了，人生压力也没有了，是真正意义上的躺平。\n当然，我知道，要相信这些道理是很难的，要完全做到这个也是很难的。有些人志存高远，有些人喜欢热闹。但是它提供了一个保持生活清净的可能性。偶尔我们也需要用这些来宽慰自己。","date":"2023-07-11T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20230711%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E8%81%8C%E5%9C%BA%E5%8E%8B%E5%8A%9B/","tags":["life"],"title":"2023-07-11最近觉得很好用的两个AI产品"},{"author":"Huizhi","categories":null,"contents":"本来过年的时候想着今年要读一些书，每个月都至少读一本，结果今天发现已经三月份了，我还在读第一本书。\n虽然没有读书，但是看了很多别人写的材料，博客，听了很多访谈。当然，主要因为chatGPT的爆火，公众号每天都充斥着关于大模型的知识。 有一些大咖表面上在聊chatGPT，其实是趁机卖自己的课和书；还有一些人觉得这是个绝佳的创业的机会；还有一些号，明明和他八竿子打不着关系，还要硬往上面蹭。\n还记得去年冬天，行业加上疫情的原因，一片萧瑟，各大媒体都在唱衰，说是 AI 寒冬，结果过了个年就变成了AI盛世。 也不知道这个热火朝天的劲头能持续多久。\nchatGPT会是一个变革性的产品吗？就像信息时代让我们的生活发生翻天覆地的变化那样，人工智能时代也会让我们有完全不同的生活体验吗？ 还很难说。人工智能只是少数人的游戏，因为目前来说，他太昂贵了。 先进的技术带来高效，但是与固有的生产方式相比，它并不一定能省钱。\n另外，通过chatGPT，加深了我对认知的重视程度。 一开始我总是在想 openAI 和用户数据之间的关系，想着他们会不会用户输入的数据进行清洗，在此基础上做后续模型升级。 我以为只有花钱用上chatGPT的那些人，他们的数据才是有价值的数据。这种认知就错了。数据永远都是有用的，但是对于每一个组织的作用是不同的。 而openAI不需要去深究这些数据，他们只需要提供接口服务就可以收割全世界。当然他们肯定想做更多。所以认知上的差异，会导致决策差异。\n不过，我还有另外一些疑问。机器和人对齐，增强了机器的性能。这其实是仿生学，哪种动物在哪方面做得好，我们就利用这方面，学习这方面。 但是，对于人来说， 我们人类除了学习用文字写出来的知识，我们也学会了一种\u0026quot;上下文\u0026quot;。一个小学生，在学校不仅仅学了加减乘除，也学会了如何和同学打交道，每一件事情的发生都在他身上有影响。 那么机器如何拥有这种能力呢？\n推荐我读的一些博客文章：\nWhy everybody feels like theyr\u0026rsquo;e faking it? https://www.newyorker.com/magazine/2023/02/13/the-dubious-rise-of-impostor-syndrome Fix the machine, not the person by aaron Swartz\nhttp://www.aaronsw.com/weblog/productivity Life in Suburbia: Land of Cliche http://www.aaronsw.com/weblog/suburbia how to work hard http://paulgraham.com/hwh.html 做大事的人除了有天分，还有练习以及时间投入。\n《微积分的力量》\n里面引用了两段话\nI do not remember having felt, as a boy, any passion for mathematics, and such notions as I may have had of the career of a mathematician were far from noble.","date":"2023-06-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/treasure/20230302%E5%AD%A3%E8%AF%BB/","tags":["emoji"],"title":"2023-03-02 季读——2023年（一）"},{"author":"Huizhi","categories":null,"contents":"今天早上醒来，看到朋友圈大家都在（给娃）庆祝儿童节，发现居然已经到了6月了。想起自己的季读还没写多少。😄\n由于行业技术的快速发展，这个季度基本都在一个提高认知、拓宽眼界的技术氛围里。因此，我在这段时间里阅读的大部分文章都与技术有关。\n《Prompt Tuning的万字综述》 （https://wjn1996.blog.csdn.net/article/details/120607050） 读了四个多小时，没有全部读完。之前对Prompt Tuning的理解仅仅停留在第三章：如何构建模板，但是到第三章只是入门，后面还有更多的细节。最精髓的就是一句话：prompt的本质是参数有效性学习。\n《LLM as Controller 无限拓展LLM的能力边界》（https://zhuanlan.zhihu.com/p/626736120） 作者的抽象能力很强，它描述了LangChain那一套如何运作的，让我很受启发。\n它把大模型回答问题的这个过程抽象成一个系统，这个系统只包括LLM和Agent，其中LLM理解输入并且将输入转化成不同的指令，Agent接收指令并行动。假设LLM理解能力很强，100%理解输入的意思，Agent 力量很强，100%能够执行命令。那么，理论上这个LLM+Agent的组合能够做任何事情。\n那这样的话，可以进一步缩小空间，问题变成下面两个问题：第一，LLM对输入进行理解之后，转变成怎样的指令，才能被更好地被Agent执行？第二，Agent要去哪里执行指令，才能找到更好的回答？\n下面是一些例子，越往下，功能越多，也越难做。这一层一层叠加，真是牛啊~\nVisual ChatGPT：单一任务——agent去一些基于视觉的模型里面找答案。\nHuggingGPT：多重任务——agent根据不同的输入去不同的模型（基于hf hub）里面找答案。\nToolformer：多重任务——agent的范围更大，是网络上的不同的API（例如谷歌搜索、谷歌翻译）等。\nAutoGPT：多重任务——llm和agent能自我迭代。agent会不断反馈，llm根据反馈的答案调整生成更好的指令，形成正向反馈。\n《它帮大语言模型消除“幻觉”，一个月内三家向量数据库创业公司获新融资》（https://mp.weixin.qq.com/s/Fhz2O03JkdqZWug2cF7v_A） 为啥大家的目光最近会聚集在向量数据库上面呢？主要是由于大模型的缺陷。\n向量数据库是怎么用的呢？以下是一个理解。\n假设我们现在有一堆文档，内容是某个保险领域的所有条款。用户提出一些问题，例如，用户问：老人在什么情况下可以投某种保险？我们想要GPT4在这堆文档中找出答案，回答用户。\n首先，要知道的是，GPT4输入的token长度是有限制的。大模型只能输入几千个token，但这堆保险文档有几百万个token，大模型它没法一次读啊。\n很容易想到，大模型没法一次读，那就拆解让它读多次就好啦~\n但是，拆解也是不OK的。主要原因是一，这几百万个token截断之后再拼起来的效果不好。二，太贵了，太慢了。调用GPT4几万次只为回答一个问题，没有人会这么做。\n那么就用到向量数据库了。向量数据库会存向量，也是就一堆拥有很多中括号和小数的数值。它一般用来做相似度查找。\n我们可以把上面的文档都存在向量数据库里，把用户的问题也转化为向量，然后去搜相似的文本。文本找出来了之后，再传给大模型，让它去分析，给出答案。\n这样大模型的输入是不是一下子从百万级变成了万千级，而且向量数据库搜索的效率也很快，所以理论上整个流程就打通了。\n这个本地知识库，其实在每个领域都能应用，特别是文本资料很多的法律、保险、金融领域。\n但是，向量数据库是一个中间产物。如果我们基于某一个专业领域的大量数据训练了一个大模型，它本身读了很多这个领域的知识，所有的知识点它都了然于心，那么就不需要向量数据库了。或者，如果大模型的输入支持百万、千万数量级的token，也不需要向量数据库了。未来有一天或许能实现呢？\n《最早出发的中国大模型创业者：“贫穷限制了我们的想象力”》 这篇文章我读了好多遍，虽然我经历没有周博士那么多，看得没有那么远，但在一些方面也感同身受。还蛮佩服他19年出来创业的，因为后面就是AI寒冬了。那时AI领域像是一潭死气沉沉的冬水，大家发现算法也不是万能的呀，还不如规则和廉价人工好使（狗头）。然后今年，情况明显不一样了，突然变成了so-called ”AI盛世“。但是大模型真的能快速落地吗？\n特别是在周博士在的金融领域，有两个限制：一是对结果要求非常准确；二是国内的企业（专指国企央企）有信创的需求。这种情况下，如何去研发大模型，要研发怎样的大模型呢？\n他这里面说“贫穷限制了想象力”，在1980年做开发的时候需要考虑节省算力。刚好《黑客与画家》里面也写了“他们在编程的时候需要删去一部分代码，为了节省内存”，但是大模型需要的就是海量数据+超乎想象的算力。\n看完文章，我深深地感受到：时代的局限带给单个个体的影响，近乎一种残忍，哪怕是超级有能力的人，也无法跳出禁锢，上一代人没有能力在千亿参数层级去想象，这就已经决定了结果。\n《疯狂的幻方：一家隐形AI巨头的大模型之路》（https://mp.weixin.qq.com/s/T-ccVKG_LS4OvUXQIfsoeg） 一家宣称不做垂类和应用，只做研究的公司。钱和算力都有，就是不知道有没有技术，哈哈持怀疑态度。（真羡慕他们的算力啊~ ）\n最近的技术变化得实在太快了。在非常tough地学习了一天的新知识之后，精疲力尽地，第二天早上起来，发现又出来新的东西了。而且很有可能，昨天学的过时了。。。\n所以，也看了不会过时的文章和书：\n《从权力和垄断的演化机制，看投资(一）》 这篇文章的最重要的话就是：\n权力斗争的关键就是 i) 在关键环节上，尽可能让自己不可替代。ii) 同时在与自己合作的关键环节上，确保有替代者可互相制约。\n这篇文章是纠结要不要跳槽的那时候读的，让我看清了自己在前公司的地位：我的可替代性太强了。这也提醒我了，要去慢慢地构建自己的生态圈。\n《软件工程》——李爱萍\n《黑客与画家》\n这两本书的内容下次写吧。","date":"2023-06-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/treasure/20230602%E5%AD%A3%E8%AF%BB/","tags":["emoji"],"title":"2023-06-02 季读——2023年（二）"},{"author":"Huizhi","categories":null,"contents":"第一个产品是ora.ai（之前域名是ora.sh），在这个上面可以创建专业领域的机器人。\nora上面五花八门的机器人都有，解读法律条款的机器人，帮助人们保持心理健康的机器人，图书馆机器人等。\n爱学习的我，当然主要是用机器人来学习。刚好最近在学bash语言，于是建了个bash学习机器人。\n我分步骤告诉bash机器人，你先给我讲bash的基础知识，然后给我出题，然后批改答案，看我写的对不对，然后给出更好的答案，以及解释他的答案。\n基础知识学会了，然后询问他中级语法、高级语法等。bash很容易，三天加起来一共学了没几个小时，差不多就会了。\n这里面每一轮操作都都包括下面的环节：(基础|中级|高级)知识学习-\u0026gt;小测验-\u0026gt;批改-\u0026gt;点评我给的答案-\u0026gt;给出自己的答案，并解释答案。 这是很明显的AI改变教育领域的例子，这些步骤已经形成了一个闭环。\n然后我又建立了一个FastAPI的机器人。最近换工作了，接手的项目用FastAPI写的，这次接手并没有遇到很难的地方。除了FastAPI本身具有框架简单易读、接口文档有条理测试简单的优点外，FastAPI小助手帮了不少忙。 首先，代码里面有任何不懂的地方，我都会提问，而且是刨根问底地问。其次进行功能拓展的时候，如果写出Bug了，我也会在上面问可能的解决方法。最后，我又把FastAPI的官方文档看了一遍，不懂的也问它。这一次效率明显比去年3月更高了。\nOra调用的是OpenAI的GPT4，它和OpenAI网站上的GPT4相比，有两个优点：\n免费 专业领域的小机器人能够让我们更能够集中注意力，专注于某一小块的问题。 在OpenAI网站上，chatgpt的页面开了很多session，用来问不同领域的问题，注意力经常会被分散，Ora则不会出现这个问题。 第二个产品是Call Annie，它的网址是https://callsam.ai/，这个应用自称是大家24小时的AI朋友，大家可以和它打电话闲聊。它很适合所有需要练习口语的小学生，大学生，以及像我这样的职场人。\n它最大的优点是能够用国内号码注册。iPhone上可以接通视频，其他设备包括网页端只能接通音频。我比较推荐音频接通，因为视频里面的“真人”不是很“真”。\n除了这两款产品之外，还有一些集成工具，例如you.com，poe等。这些都不错，但是我的需求没有那么大，只是偶尔会用一下。","date":"2023-06-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20230518%E6%9C%80%E8%BF%91%E8%A7%89%E5%BE%97%E5%BE%88%E5%A5%BD%E7%94%A8%E7%9A%84%E4%B8%A4%E4%B8%AAai%E4%BA%A7%E5%93%81/","tags":["life"],"title":"2023-05-18最近觉得很好用的两个AI产品"},{"author":"Huizhi","categories":null,"contents":"在GPU上运行中文LLaMa模型，主要是按照 https://github.com/ymcui/Chinese-LLaMA-Alpaca 这个仓库的方法。 中文LLaMa模型和中文Alpaca的区别是：中文LLaMa在英文llama的基础上扩充了中文词表并且使用了中文数据进行二次训练。中文LLaMa只能进行单轮问答。中文Alpaca经过instruct-tuning 生成，可以进行多轮问答。本次实验主要是针对中文LLaMa模型。\n文档 模型部署和推理有四种方法，我选择的是用HF的inference接口来进行推理。\nhttps://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用Transformers推理 这里详细讲了用scripts/inference_hf.py来启动模型。\n原则上非常简单，直接运行下列的脚本，就可以进行推理。\nCUDA_VISIBLE_DEVICES={device_id} python scripts/inference_hf.py \\\\ --base_model path_to_original_llama_hf_dir \\\\ --lora_model path_to_chinese_llama_or_alpaca_lora \\\\ --with_prompt \\\\ --interactive 对参数进行解释：\nbase_model是Meta发布的原生llama模型 lora_model是 这个是LoRa生成的模型，可以在网盘下载，也可以用HF的模型调用（例如ziqingyang/chinese-llama-lora-7b）。模型调用比较简单推荐使用。 with_prompt 是否将输入与prompt模版进行合并。 interactive 以交互方式启动，以便进行多次单轮问答。 在实验之前，首先要搞清楚一些概念：\nLoRa和Alpaca模型是无法单独完成推理的，需要和META的原生LLAMA结合才能运行。 远程LLAMA模型META提供，LoRa和Alpaca模型这个项目提供。 为什么不能用lora模型单独推理，以我浅显的理解，它freeze了原来的模型，单独加了一些层，后续的中文训练都在这些层上做，所以需要进行模型融合。\n用huggingface的推理脚本，需要将模型转换成HF支持的格式。（Don’t worry 作者把脚本都写好了） 实践 下面用步骤的形式记录一下整个过程。\n1. 克隆项目 git clone git@github.com:ymcui/Chinese-LLaMA-Alpaca.git cd Chinese-LLaMA-Alpaca 2. 安装环境 pip install -r requirements.txt 这一步出现了ERROR: No matching distribution found for peft==0.3.0dev\n解决：最后安装了peft==0.2.0\n3. 下载meta发布的原生的Llama模型 可以下载泄露版本，需要用磁力链下载 。 泄露地址在这 也可以用HuggingFace上的7B模型 mkdir -p models/7B/ wget -P models/7B/ \u0026lt;https://huggingface.","date":"2023-06-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230427gpu%E8%BF%90%E8%A1%8Cllama%E6%A8%A1%E5%9E%8Bhf%E6%96%B9%E5%BC%8F-copy/","tags":["tech","chatGPT"],"title":"2023-04-27GPU运行LLaMa模型——用HF的方式推理"},{"author":"Huizhi","categories":null,"contents":"要有目标。 你需要有目标。短的也好，长的也好。认真定下的也好，别人那里捡的也好。就跟随机梯度下降需要有个目标函数一样。\n目标要大。 不管是人生目标还是目标函数，你最好不要知道最后可以走到哪里。如果你知道，那么你的目标就太简单了，可能是个凸函数。你可以在一开始的时候给自己一些小目标，例如期末考个80分，训练一个线性模型。但接下来得有更大的目标，财富自由也好，100亿参数的变形金刚也好，得足够一颗赛艇。\n坚持走。 不管你的目标多复杂，随机梯度下降都是最简单的。每一次你找一个大概还行的方向（梯度），然后迈一步（下降）。两个核心要素是方向和步子的长短。但最重要的是你得一直走下去，能多走几步就多走几步。\n痛苦的卷。 每一步里你都在试图改变你自己或者你的模型参数。改变带来痛苦。但没有改变就没有进步。你过得很痛苦不代表在朝着目标走，因为你可能走反了。但过得很舒服那一定在原地踏步。需要时刻跟自己作对。\n可以躺平。 你用你内心的激情来迈步子。步子太小走不动，步子太长容易过早消耗掉了激情。周期性的调大调小步长效果挺好。所以你可以时不时休息休息。\n四处看看。 每一步走的方向是你对世界的认识。如果你探索的世界不怎么变化，那么要么你的目标太简单，要么你困在你的舒适区了。随机梯度下降的第一个词是随机，就是你需要四处走走，看过很多地方，做些错误的决定，这样你可以在前期迈过一些不是很好的舒适区。\n快也是慢。 你没有必要特意去追求找到最好的方向和最合适的步子。你身边当然会有幸运之子，他们每一步都在别人前面。但经验告诉我们，随机梯度下降前期进度太快，后期可能乏力。就是说你过早的找到一个舒适区，忘了世界有多大。所以你不要急，前面徘徊一段时间不是坏事。成名无需太早。\n赢在起点。 起点当然重要。如果你在终点附近起步，可以少走很多路。而且终点附近的路都比较平，走着舒服。当你发现别人不如你的时候，看看自己站在哪里。可能你就是运气很好，赢在了起跑线。如果你跟别人在同一起跑线，不见得你能做更好。\n很远也能到达。 如果你是在随机起点，那么做好准备前面的路会非常不平坦。越远离终点，越人迹罕见。四处都是悬崖。但随机梯度下降告诉我们，不管起点在哪里，最后得到的解都差不多。当然这个前提是你得一直按照梯度的方向走下去。如果中间梯度炸掉了，那么你随机一个起点，调整步子节奏，重新来。\n独一无二。 也许大家有着差不多的目标，在差不多的时间毕业买房结婚生娃。但每一步里，每个人内心中看到的世界都不一样，导致走的路不一样。你如果跑多次随机梯度下降，在各个时间点的目标函数值可能都差不多，但每次的参数千差万别。不会有人关心你每次训练出来的模型里面参数具体是什么值，除了你自己。\n简单最好。 当然有比随机梯度下降更复杂的算法。他们想每一步看想更远更准，想步子迈最大。但如果你的目标很复杂，简单的随机梯度下降反而效果最好。深度学习里大家都用它。关注当前，每次抬头瞄一眼世界，快速做个决定，然后迈一小步。小步快跑。只要你有目标，不要停，就能到达。\n转自知乎 ","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230305%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9D%A5%E4%BC%98%E5%8C%96%E4%BA%BA%E7%94%9F/","tags":["tech","machine learning"],"title":"2023-03-05用随机梯度下降来优化人生【转载】"},{"author":"Huizhi","categories":null,"contents":" 写诗 帮我写程序 帮我debug 帮我构造数据 帮我优化Resume 梳理NLP知识时，解释不清晰的名词，并给出例子 ","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230301%E6%88%91%E9%83%BD%E7%94%A8chatgpt%E5%B9%B2%E4%BA%86%E5%95%A5/","tags":["tech"],"title":"2023-03-01我都用chatGPT干了啥【汇总】"},{"author":"Huizhi","categories":null,"contents":"昨天读了一篇文章：ChatGPT is a blurry JPEG of the web 。中文翻译在这：ChatGPT是网上所有文本的模糊图像 ，无比同意这篇文章说的，\u0026ldquo;有一种模糊是可以接受的，那就是用不同的词重新陈述信息；对于完全捏造的模糊，当我们寻找事实时，我们认为这是不可接受的\u0026rdquo;。这就是我使用chatGPT的感受。\n昨天和同事A和B闲聊，我和他们说起我调试chatGPT帮我写代码的事，我表示这个过程无比艰辛，因为它总是丢三落四的，提示了这个又忘记了那个，教了它很久，太累了。但是chatGPT总体来说还是让人很欣喜，因为它真的很聪明。\n同事A就说到，如果他是openAI老板，一定要请一些人工来选择这些答案，让这些答案更好更有人情味。同事B就表示，人少访问还可以这样做，但是现在全球有上亿用户，人工如何忙得过来，而且还有不同语言的问题，上哪去找这么多qualified的人。\n我也不相信这个后面是人工调试展示答案，但是又有一丝怀疑。每次输完问题等待答案的时候，会缓冲一些时间，然后答案里面的字一个个地出现在我眼前。这种情景就像有人在电脑对面和我交流，在打字。”字一一个个打印“，让人想起《流浪地球2》里面Moss明明已经知道三万个密码，本可以一秒钟填充，却还是要丫丫一个数字一个数字说出来。这里是不是巧合呢？\n同事A又说到，他以前在国外读博的时候，他们所里有个postdoc说某个软件可以感知到你的情绪并且显示出来，屏幕上会出现笑脸和哭脸。大家都竖大拇指，然而实际上，是有人在后面观察帮忙看着调按钮。\n同事B也补充说，人工智能的水分还是很大的。Siri刚出来的时候特别聪明，后来被爆出来后面是人工在接听和回应，后来去掉人工之后就很蠢了。\n我没有用过特别聪明的Siri，我用的时候siri就是傻傻的，除了定闹钟和成语接龙外其他功能都很少用。\n我们老板有句名言，”有多少人工就有多少智能“。如果chatGPT不开源，真的是雇了人只圈钱呢？\n哈哈那是不可能的。","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230220chatgpt%E6%9C%89%E5%8F%AF%E8%83%BD%E6%98%AF%E4%B8%AA%E9%AA%97%E5%B1%80%E5%90%97/","tags":["tech"],"title":"2023-02-20 chatGPT有可能是个骗局吗"},{"author":"Huizhi","categories":null,"contents":"先搞清楚几个基本概念：\nSeq2seq是一个概念，它的表现形式就是有encoder和decoder的一个结构。换言之，有encoder和decoder就可以说这是一个Seq2seq模型。编码器或者解码器具体可以用CNN、RNN、LSTM或者attention来构建。\ntransformer是一种基于Attention的Seq2seq。\nSeq2seq input是一个sequence， output也是一个sequence，但是维度由模型决定。 例子： 语音辨识：一串语音转为”今晚吃什么？“几个文字。 机器翻译 语音翻译：输入machine learning，输出”机器学习”。 为何不将”语音辨识“和”机器翻译“结合起来，因为有的语言没有文字 结构和原理 Seq2seq由一个encoder和一个decoder决定\nEncoder input是一个sequence， output也是一个sequence。 input vector加上positional encoding，然后经过multi-head attention，然后进行residual + layer normalization，然后经过FC，再做一次Add\u0026amp;Norm，是这个encoder的输出。这个过程会重复。 更细节的设计： input vector进来之后，经过self-attention，input和output相加，然后进行一层layer normalization。然后进入FC层，再进行一次Add\u0026amp;Norm（和自己相加\u0026amp;Normalization），这个输出就是一个block的输出。 residual connection：输入与输出相加，防止层级过高导致的梯度消失。 layer normalization： 输入一个向量，输出一个向量，不需要考虑batch中其他的向量。 对同一个feature，同一个example不同的dimension去计算。（这里feature就是example） 做法：计算它 的mean $m$和standard deviation $\\sigma$ $$ x_i^\\prime =\\frac {x_i - m} {\\sigma} $$\nbatch normalization： 对同一个dimension，不同的example，不同的feature去计算mean和standard deviation Decoder decoder有两种，一种叫auto-regressive。这里讲的都是auto-regressive。 decoder的输入： 在前边先加一个特殊的符号：BOS。 每个输入可以表示成一个one hot vector。例如”机器学习“加上BOS就是5个one hot vector。 decoder的输出： 想好decoder输出的单位是什么，假设我们做的是中文的语音辨识，那么decoder输出的就是中文，那么vocabulary就是中文的数目，常用4000个字。不同的语言输出的单位不一样，英文可以输出字母，word， subword作为单位。\n1个Input vector进去之后，出来1个output vector，它的长度和vocabulary的size是一样的。他会给vocabulary的每一个单位一个分数，分数最高的就是最后的输出。5个input vector，出来n个output vector。(n需要decoder自己决定)\ndecoder看到的输入，其实就是前一个时间自己的输出。\n这里有一个问题，如果输出错误，那么输入也会错误，会造成error propagation。 Decoder和Encoder对比 decoder与encoder结构类似 decoder有一层masked multi-head attention 之前的self-attention，都要看过完整的input之后才做决定。$b^1$是由$a^1$到$a^4$一起决定的。","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230216%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3seq2seq/","tags":["tech","chatGPT"],"title":"2023-02-16 如何理解Seq2seq"},{"author":"Huizhi","categories":null,"contents":"承认吧，现在全世界最火就是chatGPT。\n去参加了王建硕老师那边组织的关于chatGPT的讨论。\n会上的讨论：对新技术进行哲学思考无疑是最让我震撼的。正因为他们进行深度思考，才能真正看到事物的本质，才能正确判断事物的走向。 从心理学和教育学来看，也开拓了我的眼界。 从高效使用和商业化来看，它无疑会改变很多人的生活。 chatGPT的使用感受很不错。\n我试用的方面主要在让它生成一些小功能的代码，例如爬虫，处理数据，修bug等。之前google copilot出来的时候，我的感觉没有那么惊艳。copilot可以补全代码，但是很奇怪，它经常显示大段我不需要的代码，看着闹心。 如何让chatGPT有更好的效果呢？那就是写更好的prompt，也就是清晰简洁聚焦的提问。 昨天看了好几篇关于prompt的技术文章，原来国外的prompt的技术已经发展得很厉害了。 看了一些好的prompt的例子，给我一种扑面而来的熟悉感，这不就是高中时候老师出的题嘛。\u0026ldquo;请用XX写一篇关于XX的文章，文体限制在议论文，不超过800字。\u0026rdquo; 随着chatGPT兴起，产生了一种新的职业，叫做prompt engineer，他们的工作是研发出更好的提问方式，引导chatGPT给出的答案更符合用户的预期。 很多人的感受是和chatGPT对话，就像和一个小朋友交流。你不能给它一个宏大的问题，让它去解决。但是可以一步一步引导它，告诉它每一步做什么，最后达成自己的目标。 晚上我又看了几个chatGPT的应用，我看到的最惊艳的两个应用有两个。 第一个是语言学习，例如和azure结合起来，练习口语。或者让chatGPT修改文章，斟酌字词，问chatGPT某句话的某个词是什么意思，怎么理解。这个应用基本上是全世界人民的需求。只要有沟通和交流的需求在，就会想要学习新的语言。 试想一下，如果你有了语音版chatGPT，相当于你有了一个24小时的外教。当然openai和azure都需要api key，需要花一些钱。\n第二个是用chatGPT生成文本，然后把这些文本用AI工具转成图片，最后形成动画或视频。现在很多人在做的抖音号视频号（其实就是垃圾营销）是可以用这种方式生成的。 当然作为一个技术人员，我肯定对底层原理感兴趣。最主要的创新点在于in-context learning、强化学习以及庞大的数据输入。公司在年前就请某个大佬在公司做了chatGPT的介绍，不得不说公司领导的嗅觉还是很灵敏的。 搞了几年互联网和AI，大家最后发现，科技的前沿还是在美国。国内大小厂只能跟在国际大厂的屁股后面。 它的缺点在于：\n它的结果不准确。大量的例子表明它不擅长计算，并且文章写得稀里糊涂。所以它的结果是否是正确的，需要用户有能力做出判断。 模型未开源，可能永远也不会开源。 去年年底Notion推出Notion AI的时候，我就报名了。这个月7号收到官方的邮件说我可以使用了。我用Notion AI的感受也非常好。 它只要在notion里面打一个\u0026rsquo;/\u0026lsquo;就可以了。\n它的功能有很多：\n写博客、写文章 写诗 写文章总结 发朋友圈 翻译 改变文字的语气（严肃、轻松、正式、自信等） 所有的文章都可以变长变短 有一天晚上我给室友演示Notion AI，她是做别的行业，不太懂AI这些。那天晚上我刚好吃了咖喱牛腩，就让Notion AI以\u0026quot;咖喱牛腩\u0026quot;写一篇文章。 看着文字逐渐出来的时候，她不禁激动起来，惊呼\u0026quot;大厉害了，要是我有这样一个助手就好了\u0026quot;！ 有点感动，AI真的在改变我们的生活吧！","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230213chatgpt%E5%9C%A8%E6%94%BB%E9%99%B7%E6%89%80%E6%9C%89%E4%BA%BA/","tags":["tech","chatGPT"],"title":"2023-02-13 chatGPT 在攻陷所有人"},{"author":"Huizhi","categories":null,"contents":"理解输入与输出 输入有可能是一个 vector，有可能是多个 vector 输出： 一个序列对应一个 label。the whole sequence has a label 例子：在情感分析里面，This is good 对应的输入是多个 vector，输出为 positive，是一个vector。 一个 vector 对应一个 label。一个序列对应多个 label。 例子：在词性标注里面，This is good 对应的输入是多个 vector，输出为 代词，动词，形容词。 模型决定 label 的个数。seq2seq 任务 例子：在机器翻译里面，This is good 对应的输入是3个 vector，中文翻译是”不错“，输出为2个 vector。 一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling 例子： I saw a saw 如何解决 sequence labeling 的问题：用 fully connected network 对每一个 input vector 进行作用 弊端： 用 fully connected network 来输出，假设对 I saw a saw 做词性标注。对于 FC 层来说，两个 saw没有什么不同，但是他们实际上一个是动词，一个是名词。 解决思路：考虑更多的上下文。每一个 fc 层，都对所有的输入作用。或者给他一个 window，作用于相邻的几个 input vector。但是作用还是有限，计算也很复杂。 我们想考虑整个 sequence，但是不想把 sequence 所有的数据都包括在里面，就有了 self-attention。","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","tags":["tech","chatGPT"],"title":"2023-02-09 如何理解自注意力机制"},{"author":"Huizhi","categories":null,"contents":"数据均衡 做文本分类时，如果类别数量差别不大，可以用hugging face的Trainer类，训练代码如下：\nmodel = BertForSequenceClassification.from_pretrained(\u0026#34;bert-base-chinese\u0026#34;, num_labels=len(labels), problem_type=\u0026#34;multi_label_classification\u0026#34;, id2label=id2label, label2id=label2id) tokenizer = BertTokenizerFast.from_pretrained(\u0026#34;bert-base-chinese\u0026#34;) def compute_metrics(p): preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions result = multi_label_metrics( predictions=preds, labels=p.label_ids) return result training_args = TrainingArguments( output_dir=model_directory, learning_rate=5e-5, per_device_train_batch_size=2, per_device_eval_batch_size=2, num_train_epochs=3, dataloader_drop_last=True, weight_decay=0.01, save_steps=50, logging_steps=50 ) trainer = Trainer( model=model, args=training_args, train_dataset=data[\u0026#34;train\u0026#34;], eval_dataset=data[\u0026#34;train\u0026#34;], tokenizer=tokenizer, compute_metrics=compute_metrics ) trainer.train() trainer.evaluate() model_directory 是模型存储路径，data是数据。\n数据不均衡 如果类别数据不均衡时，例如 class A有1000个数据，class B有100个数据，也可以用上面的训练代码，但是预测B的效果不会很好。\n要解决数据不均衡的问题，可以考虑加一个class weight。加class weight的意思是给class B一个更高的权重，让模型预测的时候多考虑一下class B，方向往class B偏离。\n官网给了一个例子，需要我们继承Trainer类，自定义一个类，也就是这里的CustomTrainer，重写compute_loss 这个方法。","date":"2023-03-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20230131%E5%A6%82%E4%BD%95%E7%94%A8huggingface%E5%AF%B9%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%B1%BB%E5%88%AB%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/","tags":["tech"],"title":"2023-01-31 如何用HuggingFace对不均衡类别进行分类"},{"author":"Huizhi","categories":null,"contents":"hub上的数据集 （这里不是互联网上任意的数据集，专指Huggingface的hub上面的，就是可以用关键字直接下载的）\n数据集可以在https://huggingface.co/datasets 找到，另外也可以用**datasets.list_datasets() 来看有什么数据集，然后通过关键字下载。\nfrom datasets import list_datasets list_datasets(with_community_datasets = True, with_detaikls = False) 很多例子演示的时候，都是直接用hub上的数据集演示，但是我不知道这个数据集里面的构造，尽管照着例子运行成功了，但往往一头雾水。\n此时我要看看这个数据集里面到底有啥东西，可以导入dataset builder来看看。（这个例子里面我们导入的数据集是”rotten_tomatoes”）。\n!pip install datasets from datasets import load_dataset_builder ds_builder = load_dataset_builder(\u0026#34;rotten_tomatoes\u0026#34;) ds_builder.info.description Movie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.","date":"2022-10-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20221210huggingface%E7%9A%84dataset%E7%9A%84%E4%BD%BF%E7%94%A8/","tags":["tech"],"title":"2022-12-10 HuggingFace的Dataset的使用"},{"author":"Huizhi","categories":null,"contents":"最近检查以前写的代码，发现我给不同的功能函数或者变量起的名不是很精确。 比如数据处理这个阶段，就很容易取 data_process， get_data，process_data，data_preprocess，deal_with_data 这些名字。再比如很多类的主入口，我经常会写 run()、xx_driver() 等等。\n想一个名字看起来简单，但是新建文件那一刻抓耳挠腮肚子里墨水空空，想不到好名字，无奈最后写下写了很多遍的 get_data 。于是学习给不同的功能函数或者变量取一些适合的名字迫在眉睫。\n今天看了关于两篇起名建议的文章，一篇是《变量名不要起得他妈的那么长》，链接在这 。 我跟着这篇文章反省了一下，有时候为了区分不同情况，我就会用下划线连接好几个单词，这样的话总体长度很长，不是很 pythonic。\n作者给了几个建议：\n不要容易看出类型的名字后面加上类型，例如 name 就不要叫 namestring。\n写复数，不要用单数加 collection。例如 holidays 比 holiday_list 好些。\n我这个问题还挺严重的，因为我很喜欢写 xx_list，yy_dict 等。\n在写 func 的名字的时候，不需要把参数也写在功能函数名称里面，因为参数列表能够看出来要处理什么参数。例如 merge(table_cells) 比 merge_table_cells(x) 要好。\n要精确，不需要把每一个细节都写出。例如 recentlyUpdatedAnnualSalesBid 这里面每一个单词细节都值得推敲，看是不是为了确保独一性而加的，没有就要去掉。\n变量名不要包含能从上下文看出来的单词。如果类名里面已经包含的单词，在类方法就不用再写了。\n例如类名有 data，已经表明了这个类是和数据相关。那么方法可以直接写 process，不需要写 data_process。\n变量名不要包含无意义的单词。 这些单词包括：\ndata, state, amount, value, manager, engine, object, entity, and instance. Python 里面用类型注释很容易避免这些问题，就算用 results，不用 results_list 也可以很快看出 results 是一个 list，有时候是 list of list。\n第二篇文章是这个《起名的那些事儿》，链接在这儿 。\n对于起名他给的建议是：\n对于类名、接口名： 用名词，不用形容词。","date":"2022-10-30T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20221024%E5%A6%82%E4%BD%95%E5%9C%A8%E7%A8%8B%E5%BA%8F%E4%B8%AD%E8%B5%B7%E5%90%8D/","tags":["tech-general"],"title":"2022-10-24 在程序里起名有很多要注意的"},{"author":"Huizhi","categories":null,"contents":"在分词的过程中，碰到一个这样的句子：\n\u0026lsquo;公司产品品质持续提升，单晶硅片用料比例大幅高于行业平均，单晶硅料价格上涨。\u0026rsquo;\nimport hanlp tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH) sentence = \u0026#39;公司产品品质持续提升，单晶硅片用料比例大幅高于行业平均，单晶硅料价格上涨。\u0026#39; sen_list = tok(sentence) print(sen_list) [\u0026#39;公司\u0026#39;, \u0026#39;产品\u0026#39;, \u0026#39;品质\u0026#39;, \u0026#39;持续\u0026#39;, \u0026#39;提升\u0026#39;, \u0026#39;，\u0026#39;, \u0026#39;单晶\u0026#39;, \u0026#39;硅\u0026#39;, \u0026#39;片\u0026#39;, \u0026#39;用\u0026#39;, \u0026#39;料\u0026#39;, \u0026#39;比例\u0026#39;, \u0026#39;大幅\u0026#39;, \u0026#39;高于\u0026#39;, \u0026#39;行业\u0026#39;, \u0026#39;平均\u0026#39;, \u0026#39;，\u0026#39;, \u0026#39;单晶\u0026#39;, \u0026#39;硅\u0026#39;, \u0026#39;料\u0026#39;, \u0026#39;价格\u0026#39;, \u0026#39;上涨\u0026#39;, \u0026#39;。\u0026#39;] 可以看出来，这里“单晶硅片”，“单晶硅料”， 被分为了“单晶”“硅”“料”和“单晶”“硅”“片”。\n如果我们想要把“单晶硅”分出来。可以设置自定义词典。tok下面有两个参数：dict_force和dict_combine，通过设置这两个参数就可以达到自定义词典的效果。\ndict_force和dict_combine有什么区别：\ndict_force是强制模式，强制模式的优先级高于统计模型。如果强制模式用于所有文本，会对其他句子进行干扰，所以强制模式一般不用于所有文本，但是可以针对某个特定句子打补丁。\ndict_combine是合并模式，合并模式的优先级低于统计模型。就是说句子先用统计模型分词，然后在这个分词的基础上，再进行最长匹配并合并。\n先看一下dict_combine的例子：\ntok.dict_force = None tok.dict_combine = {\u0026#39;单晶硅\u0026#39;} sentence = \u0026#39;公司产品品质持续提升，单晶硅片用料比例大幅高于行业平均，单晶硅料价格上涨。\u0026#39; [\u0026#39;公司\u0026#39;, \u0026#39;产品\u0026#39;, \u0026#39;品质\u0026#39;, \u0026#39;持续\u0026#39;, \u0026#39;提升\u0026#39;, \u0026#39;，\u0026#39;, \u0026#39;单晶硅\u0026#39;, \u0026#39;片\u0026#39;, \u0026#39;用\u0026#39;, \u0026#39;料\u0026#39;, \u0026#39;比例\u0026#39;, \u0026#39;大幅\u0026#39;, \u0026#39;高于\u0026#39;, \u0026#39;行业\u0026#39;, \u0026#39;平均\u0026#39;, \u0026#39;，\u0026#39;, \u0026#39;单晶硅\u0026#39;, \u0026#39;料\u0026#39;, \u0026#39;价格\u0026#39;, \u0026#39;上涨\u0026#39;, \u0026#39;。\u0026#39;] 我们一般会用dict_combine，这样就把“单晶硅”分出来了。","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/know_how/20220802%E7%94%A8hanlp%E5%88%86%E8%AF%8D%E6%97%B6%E5%A6%82%E4%BD%95%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AF%8D%E5%85%B8/","tags":["tech"],"title":"2022-08-02 用 HanLP 分词时如何自定义词典"},{"author":"Huizhi","categories":null,"contents":"从照片上看，整个公寓没有什么烟火气，房子内部还处于毛坯房的状态，是混凝土水泥这种灰蒙蒙的色调。屋内的摆设非常简陋，几乎只有床、小桌子、热水瓶这种生活必需品。没有电，用的是类似于手电筒的东西，光线非常微弱。\n可以想象，住在这里的人几乎不会过上什么幸福生活。他们的身影也是非常落寞。\n最近停贷的事情，也是越闹越大。很多人在付了首付，已经开始还贷的情况下遇到房子变成了烂尾楼。不停贷的话相当于把钱往水里扔，停贷的话个人征信遇上这种事情，对普通人来说，几乎等于个人破产了。\n联想到最近这一年的见闻，我突然意识到，将来碰上维权的可能性很大了。\n去年听某个亲戚说在维权，他买的房子本来是带优质小学的，所谓优质，就是生源和师资都比较好，师生比也很令人满意。但是最近小学突然扩增，使得一个班级六十多人，很拥挤，很不方便，自己的小朋友很不习惯，但是毫无办法，业主去拉条幅抗议也无效。\n今年6月解封的前两个礼拜，小区群内经常看到有人发这种消息：XX理发店/美容院/健身房关了(跑了/倒闭了)，要维权的人请进群，然后附上一个二维码。\n我没有问过维权后续，但是基本能猜得到。还能有什么办法？不就是群内互相安慰，然后自认倒霉吗。\n回到这组照片，我很钦佩Thomas在拍下《火车上的中国》被人民日报转载之后还去选择拍反映民生多艰的问题。这也是我最近的一个思考点。在上海，特别在浦东，带着相机出门拍照，最后看存储卡里，总免不了会有三件套的照片。数了数，我已经在以下这么多地方拍了三件套了。除了在家里拍三件套，我还在以下地方拍过：\n外滩、北外滩、乍浦桥、东方路天桥、旅顺路、环球金融中心旁边\n我产生了厌倦之意，不想把它当成重要的拍摄对象了。它那么瞩目，让所有人都夸它，但是上海不是只有这一面。\n我提醒自己，以后要多拍一些其他的东西。","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20220719thomas%E6%8B%8D%E7%9A%84%E7%83%82%E5%B0%BE%E6%A5%BC/","tags":["life"],"title":"2022-07-19 Thomas 拍的烂尾楼"},{"author":"Huizhi","categories":null,"contents":"过去的一个礼拜的上海是今年最漂亮的上海，随便刷一刷小红书，就可以看见满屏的”绝美晚霞“，“火烧云”，”绝美朝霞“，”绝美天空“， ”感觉在欧洲“。欧洲的风景那么美丽，除了历史的沉淀，蓝天白云的加分也是不少。第一次意识到上海也可以像欧洲那么美丽，哈哈。\n据说这是台风的功劳，台风要来，云就飘动得很快，天空就很通透。天气这么好，大家都出去撒欢了，前天晚上我和楚哥去乍浦桥拍照，刚好是太阳落山后的蓝调时间，天还没有黑，天空还是类似白天的湛蓝的颜色，大朵白云依偎在陆家嘴三件套旁边，晚风撩人。桥两边都有人在聊天，看夜景，也有大片空出来的地方，是大家都自觉为他们空出来的，因为有人在拍婚纱照。在乍浦桥上，很有以前在博物馆岛的感觉。\n复工之后我们基本每个周末都出去玩。特别珍惜出门的机会。这一个月之内我们去了鲁迅公园、世纪公园、上海野生动物公园、自然博物馆。真是每一个周末都没浪费啊。\n我和老同学还见了一次面。那次经历真的很曲折。在没有恢复堂食的时候，我听说天安千树可以堂食，于是我们就打算去那里碰碰运气。\n我们到了之后，发现还是不可以堂食，但是商场5楼靠近露台摆了很多桌子，可以在商场里面叫外卖在这些桌子上吃。我们看中了一家叫“山石榴”的餐馆，打算一个人在桌子这边占位，另外两个人分别去买吃的喝的。\n等我买完回到5楼时，吓了一跳，所有的桌子都消失了。\n后来我才知道，有人来突击检查，这些桌子临时被撤了，检查的人一走就可以摆上，一来就撤，简直像游击战。没办法，最后我们在外面的露台吃的饭。露台上很热闹，有人在跳舞，有人在玩滑板。也有很多人和我们一样在吃饭。那天不是很热，也有风，“山石榴”的味道也挺好，总体还不错。\n在天安千树，我还见到了江对面大名鼎鼎的中远两湾城。曾经看过一篇关于中远两湾城自己组织换物业的报导，太令人震撼了。因为据说这个小区是上海最大的小区，类似于北京的天通苑，小区内部就可以完成循环。这次疫情，中远两湾城也是重灾区，因为人口密度太大了。这次看到这个小区也是意料之外。\n被关了这么久，好像一切都恢复正常了，但偶尔在路上看到有人穿着志愿者蓝色的防护服会觉得”违和“，蓝色的防护服不应该出现在”太平盛世“，他应该只属于那两个月。\n可是有时候又问，那两个月真的过去了吗？现在真的是”太平盛世“吗？\n好像也不是，昨天我们办公楼要被封，大家都卷着细软跑路。今天我楼下住户成了次密接，他要在家隔离7天，也不知道我们楼栋会不会封。我们总说要珍惜生命，因为你永远不知道明天和意外哪一个先来。但是在上海，肯定是意外先来。所有经历过这次上海疫情的人一定会对“知足常乐，活在当下”体会更深。","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20220707%E6%96%B0%E7%8E%B0%E5%AE%9E%E6%98%AF%E6%B4%BB%E5%9C%A8%E5%BD%93%E4%B8%8B/","tags":["life"],"title":"2022-07-07 新现实是活在当下"},{"author":"Huizhi","categories":null,"contents":"高温黄色警报 雷电黄色警报 暴雨蓝色警报 大风黄色警报 冰雹黄色警报\n但是今日出现了彩虹","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20220629%E4%BB%8A%E6%97%A5%E4%B8%8A%E6%B5%B7/","tags":["life"],"title":"2022-06-29 今日上海"},{"author":"Huizhi","categories":null,"contents":"学着学着突然就emo了，觉得在互联网行业一直要学习新的东西，这个过程是无休止的，何时是个头啊。\n正沮丧着，在浏览器上我发现了一个很久没有打开的标签文件夹——blog，里面是我这几年收藏的博客和专题网站。于是我开始一个个点开，看有哪些人还在更新。\n其中有一个网站，我看了好久。\n日志 - Yihui Xie | 谢益辉 看完了我很有感触。这种默默写博客的做法，就是我一直向往却没有做到的。非常敬佩，因为博主在努力构建自己的精神世界，和庞大的信息世界做对抗。\n我看过一篇文章，说每个人对不同的媒介有不同的接受度。有的人能从图像中看到丰富的信息，有的人在视频中最容易学新知识，有的人对文字的处理速度很高。有的人在做vlog，有的人还在坚持写博客。在我心里文字是最简朴，也是最丰富的。\n这个博主，很令人敬佩的另一个原因是，他毕业后在RStudio工作，8年了还没换工作呢。我们行业的人都挺浮躁的，因为大家都想去知名度更高的公司，报酬更高的公司，“跳来跳去“被认为是升职加薪的关键，但是也有一些人让人看到他们的内驱力是”做好事情“。\n我要向这种做法学习，不要那么浮躁。\n下班后也和一个同事聊了聊。我问他经常要学习新的东西，不会感到厌倦吗？他说要保持竞争力，学习是必须的。而且什么框架不重要，重要的是弄清楚框架的设计理念是什么，这个框架为什么这么受欢迎？它的优缺点在哪？重要的是可以把写代码当成爱好，这样就不会烦了。\n我意识到我的做法是有问题的，我想着学完那个教程，我就会了。学的过程中我内心不断在念叨，”还有7个部分没学“，”好的，这章过了“。唉，我干嘛要这样啊，现在又不用考试要求面面俱到。\n如果带着好奇心去学的话，那就是，”有了这个功能，我还想加点需求，那么又该怎么实现呢“，”为什么这么设计呢，和Flask又有什么不同哇“，这样用好奇心驱动的话，收获的应该是好奇心得到满足的永不厌倦吧。","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20220302%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0fastapi%E4%BA%86/","tags":["life"],"title":"2022-03-02 开始学习 FastAPI 了"},{"author":"Huizhi","categories":null,"contents":"从12月底，就开始对摄影产生了一点兴趣。这种兴趣的由来，其实是有迹可循的。来上海四个月，每个礼拜出门都是去商场吃饭，然后逛逛。时间久了，就觉得周末的生活很乏味，开始讨厌起逛商场来。\n我虽然不知道会在上海住多久，但是希望自己能利用这段时间好好的感受一下上海的生活，去看一些上海的街头的景象，看一些上海的日出，日落，去周边玩，认识一些朋友。\n这个兴趣的由来，也是随着我找好的瑜伽学习场地未果的结果。学习瑜伽能够认识一些小伙伴，但是瑜伽班真的太贵啦，看了一下我身边的瑜伽馆，动不动就要交费上万元，想想还是算了。\n从12月份到现在，每天晚上睡前我都在看一些摄影小知识。其中，知乎上的500px的答案是我阅读的最久的。他经常给一些提问者的照片进行点评/点拨，让读者明白照片好在哪，插在哪。在这些答案中，对我最有启发的是：拒绝随手拍。\n我是一个很喜欢随手拍的人，美其名曰“记录生活”。但是500px的回答说的好：你会给你碰到的一个路人写传记吗？你会觉得你随便碰到的一个路人都是值得记录的对象吗？ 风景也是一样的，我们生活中很多平平无奇的东西，习以为常的东西，是不值得花大心思记录的。\n他还给了一个令人思考的提问：如果这个场景值得拍，你想表达什么？\n此后，当我习惯性的拿起手机，想随手拍，“记录生活”的时候，两个问题总会出现在我的心中：这个场景是路人甲吗？我想表达什么？问了自己这两个问题之后，我就认识到了没什么值得拍的。我就放下了手机。\n我也找了一些生活中适合拍的题材，例如办公楼能拍什么？公园能拍什么？结论是有限的。这些习以为常的地方，还是很难有大片感。\n但是又有人提出，可以用话题的形式来拍。例如，早上的街道，午后晒太阳的人们，不同天气的公园，或者以颜色来作为拍照的主题，或者以线条，形状来作为拍照的主题。\n我自己呢，很想拍两组照片：一组是我上班会路过的一条买菜的街，早上挤满了买菜的大爷大妈。二是办公区这边工作的白领和外卖员。\n我还学到一个新手知识就是，在初期可以去模仿别人拍照，看别人拍了什么，用了什么参数，看自己能不能把它复现出来，同时也思索，为什么别人设定这样的参数，取这样的景，这样长期以往，自己对于参数的设置，什么光线用什么参数都了然于心。\n周末去三个店看了一下相机：索尼尼康和富士。最喜欢的还是a7c，因为他比较小，比较轻。还有一款a7m3太重了，实在是很难长时间拿着拍照。","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20220110%E5%BC%80%E5%A7%8B%E5%AF%B9%E6%8B%8D%E7%85%A7%E4%BA%A7%E7%94%9F%E5%85%B4%E8%B6%A3/","tags":["life"],"title":"2022-01-10 开始对拍照产生兴趣"},{"author":"Huizhi","categories":null,"contents":"回国后，我如愿以偿的成为了一名又苦又累的Python后端开发工程师。一眨眼，入职已经两个多月了。我只能说，路是自己选的，苦也是自己选的，人生无非是苦中作乐。\n记得当初我要找后端开发的工作时，一个朋友劝我，“不要去做后端，等你做久了就会发现，很无聊的，后端就是体力活。”\n我头一偏，“我不，我觉得后端还挺好玩的。”\n这个又苦又累的过程，从4月初准备面试时就开始了。\n在我分析了几份job description的过程中发现，苍天，要准备的东西太多了吧。编程语言Python的特性与使用要熟悉吧？否则你怎么好意思说你是写Python的。数据库不能只会增删改查吧？高级应用建个索引，弄个查询优化要会吧？常见的数据结构和算法要知道吧，Leetcode刷题至少要刷个100道吧，不然连笔试都过不了。git， linux操作要知道一些吧？没用过可不行啊！做Python后端，Django/Flask要用过吧，至少自己得做个小网站出来。设计模式要稍微知道一点吧？Docker/openstack/k8s这些都最好有涉猎吧。\n照着jd我分析了下我的技能：每个以前都用过一点，但每个都不深入。\n于是我开始了准备的过程：每天刷题，同时看《流畅的Python》，同时在b站上看数据库，同时在看django实践。\n准备了两个月之后（在此谢谢前司轻松的在家办公氛围，让我有很多时间准备面试），一个朋友劝我不要一味的准备，不管有没有准备好都要先去面试，在面试中成长就是最快的。于是我的简历被内推了，我也有了第一次面试。\n第一次国内面试第一感觉就是流程好快，简历刚发，就来面试通知了。第二感觉就是，真的长见识。就算我不是他们要找的人，公司不是我要找的公司，我也能从面试中知道这个公司在做什么，需要什么样的人，而我合不合适。多面同一行业的岗位几次，也基本知道这个行业大家在做什么，现在缺什么人。这种反馈让我不断地调整自己的定位和期望。\n很快，我又投了很多职位，接下来的两周，基本上每天都有一两个面试。每一次面试我都会记录问的问题，然后好好总结，这样，我也有了一份属于我自己的面试宝典。\n时间很快，6月底快到了，我马上就要回国了。这个月让我印象最深的是一个周五的上午，是我在Cariad工作的最后一天。我记得这一天，因为心情起伏非常大，上午我有个面试，刚挂电话，楚哥特别着急的和我说我们回国的航班取消了，我有点慌了，马上打开手机查最近的能回国的票，一查最近都没有，然后电脑那发出会议提醒，还有5分钟就是我的线上离职欢送会了。我赶紧进入页面，气氛一片祥和，同事们都带着微笑，我心里在焦虑我的机票还没搞定，房子月底到期还要另外找住的地方，却还要假装镇定的说感谢巴拉巴拉。\n后来我们另外买了票，也找到了住处，这一天的感觉却是很久都忘不掉。我们同组的同事很nice，给我准备了拜仁的球迷商店的券作为离职礼物，非常感谢他们，也很感恩人生中有这段共事的日子。\n扯远了，离职之后，我有更多的时间来准备面试和面试了。我也开始更有针对性地投简历。首先我把城市设为上海和南京，其次，我不再接受外包的面试（在初期，外包会有很多面试的机会），最后，我把眼光看向很多我觉得有潜力的感兴趣的小公司，尽一切力量去挖掘这这些公司的现状。\n最后连隔离的时候都一直都在面试，每天给自己打鸡血，白天面试晚上总结，幸运的是最后也拿到了几个offer，大部分都是和AI有关的公司，最终来了现在这个公司。\n但是咋说，大家都知道现在是AI的寒冬吧。事实上，因为前几年投资者被AI忽悠的太厉害了，投了很多钱给机器学习/深度学习，最后却发现落地很难，于是现在涉及到AI的投资就变得很谨慎。最典型的例子就是商汤科技几年亏损两百多个亿，却还没有盈利的能力，让人唏嘘啊。\n所以车企的小伙伴想转行到互联网，我都劝不要转，除非是真的热爱。在德国的话汽车行业的福利已经足够好了，在国内的话新能源汽车自动驾驶才是风口，搞手机的搞通讯的搞互联网的都去搞汽车了。要转行，三思啊！\n而且我在上海已经过上了传说中的995（有可能周六也要上半天班）的内卷生活，想要wlb的人肯定觉得很煎熬。回国以后我偶尔会感叹“我离世界很远”，但是又觉得“踏实过好当下每一天也很好”。\n我也说不准换国家换行业换岗位这个决定对我是利大于弊还是弊大于利，只是觉得，我喜欢什么，就应该去亲近什么。喜欢阅读，就多去读书，喜欢聊天，就多交朋友。不管这件事情最后给我带来什么，它至少给我带来了乐趣。在这些乐趣中，我才算真正倾听了内心的声音。如黑塞在《德米安》书中说的，“对每个人而言，真正的职责只有一个：找到自我。然后在心中坚守其一生，全心全意，永不停息。所有其它的路都是不完整的，是人的逃避方式，是对大众理想的懦弱回归，是随波逐流，是对内心的恐惧。”\n共勉。","date":"2021-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/life/20211031%E5%9B%9E%E5%9B%BD%E5%86%85%E5%8D%B7/","tags":["emoji"],"title":"2021-10-30 回国内卷"},{"author":"Huizhi","categories":null,"contents":"你好，很高兴遇到你。\n我是一个对技术热忱，喜欢阅读，喜欢思考的工程师。\n我的优点是：执行力强\n我的缺点是：执行力弱\n人就是这么矛盾！","date":"2021-07-15T00:00:00Z","permalink":"https://huizhixu.github.io/chs/page/about/","tags":null,"title":"关于我"},{"author":"Huizhi","categories":null,"contents":"《地久天长》主要讲了这样一个故事：\n上世纪八十年代，刘耀军家和沈英明家是好友，两人的妻子王丽云和李海燕在同一家工厂上班，两家的孩子在同一天出生甚至一起庆祝生日，两家相处和睦友好。\n后来，王丽云意外地怀上了二胎，她和丈夫都很想要这个孩子，但迫于当时计划生育政策的压力，一直没敢和别人说，商量要不去乡下生这个孩子，但又愁巨额罚款。有一天王丽云晕倒了，李海燕也知道了她怀孕这件事。李海燕在厂里的计生委工作，作为计生委副主任的她劝王丽云打掉了二胎。这是当时政策下最明智的做法。\n刘耀军虽然恨自己没有能力保住这个孩子，但也没有办法，他发了一通脾气后，也接受了这个事实。日子依然不紧不慢的过着，两家依然是好友。\n两家的孩子刘星和沈浩从小就是彼此的玩伴，刘星是一个胆小内敛的人，同龄的沈浩却生性爱冒险。他经常拉着刘星到处玩。\n有一天，两孩子在水库玩耍时，不幸发生了，刘星溺水身亡，刘耀军家从此失去了孩子。\n李海燕非常自责，她大声呵责沈浩，问当时的情况，骂沈浩不听话，带刘星去水库边玩。加上当时王丽云的二胎是她劝说打掉的，就更难过了。而王丽云在上一次手术上大出血，已经不可能再怀孕了。\n李海燕为这事自责，刘耀军和王丽云却没有责怪他们，他们反而劝李海燕和沈英明不要再在孩子面前提起这事了。渐渐地，以前两家相聚时温馨快乐，失子之后，刘耀军和王丽云没有心情去聚会了，沈英明家也不好叫他们。两家就这样慢慢的生疏。\n那个年代，工人铁饭碗不保，下岗和下海是最热门的话题。刘耀军和王丽云在这件事情的打击下，下定决心搬走。\n搬来搬去，他们定居在一个海岛上，在那里收养了一个孩子，也取名叫刘星。新刘星非常叛逆，想自己出去闯。在冲突日复一日的加剧下，刘耀军给了新刘星自己的身份证和一些钱。同时，为了治妻子的病，他和妻子又回到了包头市当时的筒子楼。\n沈英明当年决定下海，十几年后作为房地产开发商的他赚了很多钱。他的儿子沈浩也成为了一名医生。\n多年以后，李海燕重病住院，她一辈子都对自己劝王丽云打胎的事情耿耿于怀。她临终前终于见到了王丽云，她流着泪，说了最后一句话，“丽云，现在可以生了，咱有钱了，可以交罚款了。”\n而在那栋几十年没变的筒子楼里，沈浩向刘耀军夫妇交待了当年的实情：是他把刘星推下水的。刘耀军和李海燕原谅了他。最后，他们去了儿子刘星的坟头，这对中年夫妻在坟头就相望无言地那么坐着，像是对之前的一切有一个交待。\n说说我对这个电影的一些理解吧。\n电影的时长是三个小时，刚开始觉得节奏非常慢，镜头里大片段的沉默，主人公脸上的愁眉不展，生动的火车和机械加工噪音。他们一家在海边的生活孤独而没有生机，是那种不需要说一个字，你看着画面就能感受到的沉闷。与后面切回二十年前的有音乐有好友有说有笑的生活的对比非常明显。 片子这么长是非常有必要的，因为它对之前和之后的生活都进行了非常详细的描写。这种描写不是通过言语，而是通过画面。台词不多，但是观众会懂那种情绪。这种不动声色的描写是最让人记忆深刻的。有一种片子是那种我会在此后的生活不断想起它，例如《一一》，这部电影就是属于那种。这就是它的魅力。 电影涉及的话题非常广，基本上融合了八十年代的热门话题——“国企改制”、“下海热潮”、“计划生育”和“失独家庭”。这对我们没经历那段历史的人来说是一个很好的了解途径。4. 电影非常感人，主人公非常善良，不论怎样的变故两人都安之若素，从来没想过去责怪别人。最后李海燕临终前的那句话出来的时候我哭了，片中反复出现的一首歌便是《友谊地久天长》，成年人的友谊大概就是这样呀：各有各的难处，却会为对方找想。 电影节刚开始的时候，我本来打算看的是娄烨导演的《风中有朵雨做的云》以及张艺谋导演的《一秒钟》。后来《风中有朵雨做的云》的时间腾不开，张导演的影片又撤了，我们就去现场买了这部影片的票。得知电影有三个小时，我很忐忑，不知道这是一部什么样的影片。后来证实了它是一个巨大的惊喜。我当时不知道它会得奖，但她感动了现场观众，放映结束后掌声经久不息。\n（图为主创团队进场后和大家打招呼）","date":"2022-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/treasure/20190202%E6%9F%8F%E6%9E%97%E7%94%B5%E5%BD%B1%E8%8A%82%E5%9C%B0%E4%B9%85%E5%A4%A9%E9%95%BF%E8%A7%82%E5%90%8E%E6%84%9F/","tags":["Film"],"title":"2019-02-02 柏林电影节《地久天长》观后感"},{"author":"Huizhi","categories":null,"contents":"今天看了一节网课，是齐文昱老师的英美文学课堂，主要内容是”为什么读英文经典？“。\n下面是正文。\n我开始是从培训做起的。一开始做应试的培训，比如说，跟留学有关的，像托福考试，像SAT，SAT是美国的高考，这样一些考试。但后来我发现啊，我们有一个更便捷的方便法门，让中国人在学英语当中，少一些痛苦，少一些郁闷，和彷徨在里面。那么这个方便法门是什么呢？\n我的感受啊，世间最有效的学会英语的方法，是，不如你从一开始，就走正确的路子，从第一天开始，从内心中爱上英语。这句话你能理解吗？\n当你爱上一样东西之后，他对你来说就不再有任何的枯燥和乏味在里面，所有的那些考试，就相对来说就轻松的太多太多了。而我见过太多太多的中国学生，在不同的培训机构奔波，学托福，背一遍单词，异常的痛苦，形容憔悴，满脸菜色，然后背完之后，把单词扔到一边去，然后要考别的考试，比如说，要考GRE，要考SAT，然后又开始痛苦的背单词的过程。这单词不管怎么背，你会发现，背的速度赶不上忘的速度。背了很长很长时间之后，只会背A字母下面的有数的那几个单词。\n所以呢，作为一个老师来讲，我感觉，最大的成就感在什么地方？不在于帮助你的学生取得他想要的成绩。而是：一个老师，如果让一个学生经过你的引导和点化，从上你的课开始，发自内心的爱上英语，那么我觉得这样的课程，善莫大焉。所以呢，从过去，到现在，我一直坚持着这样的理念，做这样一个教学。\n那么今天我们讲的专题呢，是跟名著阅读有关系的。这里面包括西方英文的一些名著和中文的一些典籍。这个也是我从小到现在一直坚持在读的一些东西。我也希望引导我的学生去读这样一些东西。但，读什么，不重要。怎么读，其实很关键。我也总在尽我可能的把在这些典籍当中我看到的世界，描绘给学生。一个小时，虽然很短，但我也想了很久，能够给你们讲些什么，能够让你们获得什么。我发现，不如就用这一个小时短短的时间，把这些年，我在那些英文的典籍当中，所感受到的一些东西，拿给你们看。也许，说不定，从今天晚上你就会发现，英语没有那么枯燥了。那么这个就是今天晚上一个想做的事情。\n读这些典籍啊，就这些美好的英语啊，你需要把它慢慢转化为你自身的语言的素养。那么，在阅读的同时呢，熟悉这门语言，亲近这门语言，你会发现两个不争的事实。第一，英语是一门非常非常美丽的语言，和汉语一样美丽。第二呢，英语和汉语是相通的。他们其实彼此血脉相容。\n但是在以往呢，咱们的教学当中呢，大家听得更多的是，总在强调英语和汉语是不一样的，英语该怎么思维，汉语该怎么思维。\n我们以往强调的是他们的差异，但我看到的是他们的想通。换句话说，如果你静心读这些东西，你发现，体会一个语言的美妙，可以从这样八个维度来体会他。所以我专门做了这样一个梳理。\nEnunciation Delicacy 音韵之美\nGlossary Delicacy 文藻之美\nRhetorical Delicay 辞令之美\nInterpretation Delicacy 译文之美\nNatural Delicacy 天成之美\nNourishment Delicacy 灵养之美\nUsage Delicacy 功用之美\nDedication Delicacy 法源之美\n哪八个维度呢，看一下。 第一个叫Enunciation Delicacy ， 音韵之美；第二个叫Glossary Delicacy 文藻之美；然后呢，Rhetorical Delicay， 辞令之美；Interpretation Delicacy 译文之美；Natural Delicacy 叫天成之美；后面非常非常关键的一个，叫Nourishment Delicacy，灵养之美；后面Usage Delicacy 功用之美；最后一个叫Dedication Delicacy 法源之美。\n等到你理解完这八个维度，语言融入你的精神血脉，成为你灵魂的一部分的时候，你会发现，所有的考试，和那些功利性的目标，其实也就不成问题了。我特别希望，我能把我这些年的感受，在这八个维度之间，中文和英文的典籍都给你讲一遍，但时间真的不够。所以呢，我就集中，重点讲里面的四个方面，跟你在一起去交流。\n我想重点讲这几个方面。第一个是 Enunciation Delicacy ，就是音韵之美。世界上其实所有的语言，都是美丽的。你去静听这样一门语言，你就会发现，他就像山间的流水，淙淙的在安静的流淌，穿过山林，穿过草场，然后汇聚在一起，是你头脑当中恒久的关于春天，最美好的记忆。中文是这样的，其实英文也是这样的。这个英文的音韵之美啊，我想说不仅仅体现在语音啊，朗读啊，演讲啊这几方面。甚至于包括写作当中，也会有音韵之美。为什么呢，我想举这样一个例子，在我们中文当中，有没有押韵一说啊。 中文是非常铿锵顿挫的文字，有押韵的说法。 中文里面，你读那些唐诗，宋词，因为音韵，显得特别特别的动听。其实呢，这个英文当中也押韵。所以为什么有些时候，没那么婉转，可能跟押韵有很大的关系。而且，作为基本的常识，英语的音韵有两种常见的方试，第一种呢，是在开头押韵。第二种就是结尾押韵。时间很短，我们就举个例子好了。\nAs for the portray of an autumn, one of is always presented","date":"2021-10-14T00:00:00Z","permalink":"https://huizhixu.github.io/chs/treasure/20170802%E7%B2%BE%E8%87%B4%E8%8B%B1%E4%BC%A6%E9%A3%8E%E9%9B%85%E4%B8%AD%E5%9B%BDenduringdelicacy/","tags":["Literature"],"title":"2017-08-02 Enduring Delicacy 精致英伦 风雅中国"},{"author":null,"categories":null,"contents":"2023-07-31 有一种乐趣叫折腾博客 哈哈哈，我发现我文章没写多少，时间都花在创建/修改/更换博客、博客域名、托管服务器和博客主题上。但不得不说，这个过程真的很有意思啊。\n我之前博客主题用的是github-styles，它的页面和github一模一样。左边是个人头像和个人信息，右边上面是置顶文章，然后是文章提交热力图，再往下就是文章的时间线，一目了然。它还能切换light/dark模式.\n一切都挺好的，但是最近觉得有点不太方便了，因为我想将文章归类存储和搜索，将生活的感想和技术上的总结分开，此外还想加一个英文和德语的界面。\n但是还是打算用Hugo，hugo真的是很方便啊，之前不太懂的我也跟着教程做出来了。于是打开Hugo的主题页面开始浏览，看了几个，最后决定用这个叫做blist主题的，它支持multi-lingual，页面也很漂亮。\n我把主题 git clone到现在的博客之后，按照步骤开始操作，但是没有生效。别着急，一般都不会一次性生效的。\n我又重新安装操作了好几遍，并且大胆地删了一些上个主题的遗留的文件夹，但是怎么改动博客都运行不起来了。\n于是我去github看了一下。发现有两个repo都是和博客相关的。事实上距离上一次折腾博客已经有了一年多了，我早已经忘记为什么要分两个repo了。而且我设置了一个自动化提交，只要运行一个bash文件就可以写文章。\n于是我又重新开始看教程，敲敲打打，修修补补。\n在这个过程中看到有人吐槽hugo的文档写的不清晰，能够感受到他的愤怒了，哈哈。\nWhy Hugo\u0026amp;rsquo;s Documentation Sucks 不过过了看Hugo的文档之外，Hugo论坛hugo discourse 也是很有帮助的。\n我最后遇到的问题是，博客在本地启动，页面显示一切正常。但是在域名启动，页面显示就很奇怪，我不懂前端，猜测是部分css文件没有奇效。找了很多答案，都没有改变。最后在这个论坛上提问，很快就有人回答了。太感谢了！\n互联网世界的美妙就在于这吧。有超级多的资料可以学习，有前辈程序员写的教程，有各种论坛答疑讨论，有开源仓库的无私奉献。享受这些美好，同时也要不断地帮助别人。","date":null,"permalink":"https://huizhixu.github.io/chs/life/20230731%E6%9B%B4%E6%8D%A2%E5%8D%9A%E5%AE%A2%E7%9A%AE%E8%82%A4/","tags":null,"title":""}]