<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on 徐慧志的个人博客</title>
    <link>https://huizhixu.github.io/chs/tags/ai/</link>
    <description>Recent content in ai on 徐慧志的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>chs</language>
    <lastBuildDate>Thu, 27 Apr 2023 18:31:50 +0800</lastBuildDate><atom:link href="https://huizhixu.github.io/chs/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>2023-04-27GPU运行LLaMa模型——用HF的方式推理</title>
      <link>https://huizhixu.github.io/chs/know_how/20230427gpu%E8%BF%90%E8%A1%8Cllama%E6%A8%A1%E5%9E%8Bhf%E6%96%B9%E5%BC%8F/</link>
      <pubDate>Thu, 27 Apr 2023 18:31:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20230427gpu%E8%BF%90%E8%A1%8Cllama%E6%A8%A1%E5%9E%8Bhf%E6%96%B9%E5%BC%8F/</guid>
      <description>在GPU上运行中文LLaMa模型，主要是按照 https://github.com/ymcui/Chinese-LLaMA-Alpaca 这个仓库的方法。 中文LLaMa模型和中文Alpaca的区别是：中文LLaMa在英文llama的</description>
    </item>
    
    <item>
      <title>2023-03-05用随机梯度下降来优化人生【转载】</title>
      <link>https://huizhixu.github.io/chs/know_how/20230305%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9D%A5%E4%BC%98%E5%8C%96%E4%BA%BA%E7%94%9F/</link>
      <pubDate>Sun, 05 Mar 2023 18:31:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20230305%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%9D%A5%E4%BC%98%E5%8C%96%E4%BA%BA%E7%94%9F/</guid>
      <description>要有目标。 你需要有目标。短的也好，长的也好。认真定下的也好，别人那里捡的也好。就跟随机梯度下降需要有个目标函数一样。 目标要大。 不管是人生目标</description>
    </item>
    
    <item>
      <title>2023-02-16 如何理解Seq2seq</title>
      <link>https://huizhixu.github.io/chs/know_how/20230216%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3seq2seq/</link>
      <pubDate>Thu, 16 Feb 2023 18:31:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20230216%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3seq2seq/</guid>
      <description>先搞清楚几个基本概念： Seq2seq是一个概念，它的表现形式就是有encoder和decoder的一个结构。换言之，有encoder和dec</description>
    </item>
    
    <item>
      <title>2023-02-13 chatGPT 在攻陷所有人</title>
      <link>https://huizhixu.github.io/chs/know_how/20230213chatgpt%E5%9C%A8%E6%94%BB%E9%99%B7%E6%89%80%E6%9C%89%E4%BA%BA/</link>
      <pubDate>Mon, 13 Feb 2023 20:31:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20230213chatgpt%E5%9C%A8%E6%94%BB%E9%99%B7%E6%89%80%E6%9C%89%E4%BA%BA/</guid>
      <description>承认吧，现在全世界最火就是chatGPT。 去参加了王建硕老师那边组织的关于chatGPT的讨论。 会上的讨论：对新技术进行哲学思考无疑是最让我</description>
    </item>
    
    <item>
      <title>2023-02-09 如何理解自注意力机制</title>
      <link>https://huizhixu.github.io/chs/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Thu, 09 Feb 2023 08:31:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
      <description>理解输入与输出 输入有可能是一个 vector，有可能是多个 vector 输出： 一个序列对应一个 label。the whole sequence has a label 例子：在情感分析里面，This is</description>
    </item>
    
  </channel>
</rss>
