<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on 徐慧志的个人博客</title>
    <link>https://huizhixu.github.io/chs/tags/bayesian/</link>
    <description>Recent content in Bayesian on 徐慧志的个人博客</description>
    <generator>Hugo</generator>
    <language>chs</language>
    <lastBuildDate>Tue, 05 Mar 2024 20:01:50 +0800</lastBuildDate>
    <atom:link href="https://huizhixu.github.io/chs/tags/bayesian/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>改进量的期望 Expected Improvement</title>
      <link>https://huizhixu.github.io/chs/know_how/20240305expected-improvement/</link>
      <pubDate>Tue, 05 Mar 2024 20:01:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20240305expected-improvement/</guid>
      <description>&lt;p&gt;在看正文之前，先复习一下期望（Expectation）：&lt;/p&gt;&#xA;&lt;p&gt;在统计学和概率论中，期望是一个衡量随机变量取值的中心趋势的指标。&lt;/p&gt;&#xA;&lt;p&gt;对于一个连续随机变量&lt;em&gt;X&lt;/em&gt;，其期望值可以通过以下公式计算：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Optimization</title>
      <link>https://huizhixu.github.io/chs/know_how/20240203%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/</link>
      <pubDate>Sat, 03 Feb 2024 17:01:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20240203%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/</guid>
      <description>&lt;p&gt;贝叶斯优化有重要的两步步：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;构造代理模型（surrogate model）&lt;/li&gt;&#xA;&lt;li&gt;由获取函数（acquisition function）来生成采样建议&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;贝叶斯优化中，因为不知道目标函数的closed-form，所以需要构造一个代理模型（surrogate model）来近似目标函数。记住，代理模型对目标函数的潜在分布进行建模。通常用gaussian process来作为代理模型，也可以用random forest来作为代理模型。（任何模型，只要它为函数提供后验估计，可以用来作为surrogate model）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gaussian Process Regression with GPyTorch</title>
      <link>https://huizhixu.github.io/chs/know_how/20231217gaussian_process_regression_gpytorch/</link>
      <pubDate>Sun, 17 Dec 2023 17:01:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20231217gaussian_process_regression_gpytorch/</guid>
      <description>&lt;p&gt;这个例子主要是利用GPytorch，来实现高斯过程回归。&lt;/p&gt;&#xA;&lt;h1 id=&#34;计算mean&#34;&gt;计算Mean&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;zero mean function &lt;code&gt;gpytorch.means.ZeroMean()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;constant mean function &lt;code&gt;gpytorch.means.ConstantMean()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;linear mean function &lt;code&gt;gpytorch.means.LinearMean()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h1 id=&#34;计算covariance&#34;&gt;计算Covariance&lt;/h1&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;RBFKernel &lt;code&gt;gpytorch.kernels.RBFKernel()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;adding a scaling coefficient: &lt;code&gt;kernels.ScaleKernel(gpytorch.kernels.RBFKernel())&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;一般会在核函数的输出上添加缩放系数。&lt;/p&gt;&#xA;&lt;p&gt;在核函数的输出上添加缩放系数是为了调整核函数的影响力。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gaussian Process in Practice 高斯过程实践</title>
      <link>https://huizhixu.github.io/chs/know_how/20231210gaussian_process_in_practice/</link>
      <pubDate>Sun, 10 Dec 2023 18:01:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20231210gaussian_process_in_practice/</guid>
      <description>&lt;p&gt;这个例子主要是利用高斯过程的先验分布，将样本绘制成曲线。然后更新参数，利用后验分布获得新的曲线。&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-先验分布&#34;&gt;1. 先验分布&lt;/h2&gt;&#xA;&lt;h4 id=&#34;11-多变量高斯分布&#34;&gt;1.1 多变量高斯分布&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;创建一个包含n个候选输入位置的列表${x_i，i=1,&amp;hellip;,n}$&lt;/li&gt;&#xA;&lt;li&gt;初始化均值向量μ和协方差矩阵K（含n x n个元素）&#xA;&lt;ul&gt;&#xA;&lt;li&gt;假设x_1和x_2是多维的矩阵。x_1是一个 m* d的矩阵，x_2是一个n&lt;em&gt;d的矩阵，那么K是一个m&lt;/em&gt;n的矩阵，$K[i,j] = k(x_1[i,:], x_2[j,:])$&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;执行Cholesky分解K=LL T来获得L&lt;/li&gt;&#xA;&lt;li&gt;通过LN（0,I）获得N（0,K）上的一个样本并存储在f_prior中&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;multivariante_samples01 和multivariante_samples02 这两个function的作用是一样的，只不过有两种写法。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel Function 核函数</title>
      <link>https://huizhixu.github.io/chs/know_how/20231207kernel_function/</link>
      <pubDate>Thu, 07 Dec 2023 18:01:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20231207kernel_function/</guid>
      <description>&lt;p&gt;这篇文章主要解决三个问题：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;正态分布的表示&lt;/li&gt;&#xA;&lt;li&gt;核函数是什么，有什么类型&lt;/li&gt;&#xA;&lt;li&gt;已知先验知识，如何计算后验分布&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;1-正态分布的表示&#34;&gt;1. 正态分布的表示&lt;/h2&gt;&#xA;&lt;p&gt;正态分布一般表示为$f \sim N(0,K)$，书上写作 $p(f|x) = N(f|0,K)$。&lt;/p&gt;</description>
    </item>
    <item>
      <title>书籍 Bayesian Optimization Theory and Practice using Python 之Gaussian Process</title>
      <link>https://huizhixu.github.io/chs/know_how/20231125gaussian_process/</link>
      <pubDate>Sat, 25 Nov 2023 18:01:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20231125gaussian_process/</guid>
      <description>&lt;h2 id=&#34;1-理解covariance-matrix&#34;&gt;1. 理解covariance matrix&lt;/h2&gt;&#xA;&lt;p&gt;Gaussian Process is a stochastic process used to characterize the distribution over function.&lt;/p&gt;&#xA;&lt;p&gt;GP将一组有限的参数theta从一个连空间拓展到一个连续无限空间的一个无限函数f。&lt;/p&gt;&#xA;&lt;p&gt;假设我们有两个变量，X1和X2，它俩符合multivariate Gaussian distribution。&lt;/p&gt;</description>
    </item>
    <item>
      <title>论文 Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics</title>
      <link>https://huizhixu.github.io/chs/know_how/20231120uncertainty/</link>
      <pubDate>Mon, 20 Nov 2023 18:31:50 +0800</pubDate>
      <guid>https://huizhixu.github.io/chs/know_how/20231120uncertainty/</guid>
      <description>&lt;p&gt;Abstract&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;types&#xA;&lt;ul&gt;&#xA;&lt;li&gt;第一种分类&#xA;&lt;ul&gt;&#xA;&lt;li&gt;data uncertainty (measurement noise)&lt;/li&gt;&#xA;&lt;li&gt;model uncertainty ( limited data)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;第二种分类&#xA;&lt;ul&gt;&#xA;&lt;li&gt;epistemic uncertainty&#xA;&lt;ul&gt;&#xA;&lt;li&gt;认知上的不确定性，通常是由于没有足够的知识（数据）而产生&lt;/li&gt;&#xA;&lt;li&gt;can be reducible&lt;/li&gt;&#xA;&lt;li&gt;分为两类&#xA;&lt;ul&gt;&#xA;&lt;li&gt;model-form uncertainty&#xA;&lt;ul&gt;&#xA;&lt;li&gt;由于模型的选择导致，例如architectures, activation functions or kernel functions&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;parameter uncertainty&#xA;&lt;ul&gt;&#xA;&lt;li&gt;在训练过程产生，由于数据不够导致&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;aleatory uncertainty&#xA;&lt;ul&gt;&#xA;&lt;li&gt;stems from physical systems, 具有随机性, cannot be reducible&lt;/li&gt;&#xA;&lt;li&gt;e.g. noises&lt;/li&gt;&#xA;&lt;li&gt;这种类型的不确定性在ML模型里面被看成是似然函数的一部分(a part of the likelihood function)&lt;/li&gt;&#xA;&lt;li&gt;也被叫做data uncertainty&lt;/li&gt;&#xA;&lt;li&gt;捕捉这种不确定性的方式有：同方差 homoscedastic和异方差 heteroscedastic&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;例子：&#xA;&lt;ul&gt;&#xA;&lt;li&gt;test data和train data不同分布：epistemic uncertainty (model performs poorer in extrapolation than in interpolation)&lt;/li&gt;&#xA;&lt;li&gt;测量数据由仪器导致的误差是aleatory Unc， 大试如果由于精度原因导致，则属于epistemic unc，因为提高精度可以减少这个误差&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;causes&lt;/li&gt;&#xA;&lt;li&gt;methods:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Gaussian process regression&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a ML method with UQ capability&lt;/li&gt;&#xA;&lt;li&gt;一般不用来quantify uncertainty of a final surrogate&lt;/li&gt;&#xA;&lt;li&gt;一般用来在高度不确定的采样空间里采样，来减少训练样本的数量&#xA;&lt;ul&gt;&#xA;&lt;li&gt;to build an accurate surrogate within some lower and upper bounds of input variables&lt;/li&gt;&#xA;&lt;li&gt;to find a globally optimally design for black-box objective function&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;一般不评估GPR的UQ质量&#xA;&lt;ul&gt;&#xA;&lt;li&gt;因为预测一般在pre-defined design bounds&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Bayesian neural network&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Monte Carlo dropout as an alternative to traditional Bayesian neural network&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;neural network ensemble&#xA;&lt;ul&gt;&#xA;&lt;li&gt;neural network ensemble consisting of multiple neural networks&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;deterministic UQ methods&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;metrics&#xA;&lt;ul&gt;&#xA;&lt;li&gt;classification&#xA;&lt;ul&gt;&#xA;&lt;li&gt;probability can be viewed as uncertainty&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;regression&#xA;&lt;ul&gt;&#xA;&lt;li&gt;confidence interval :&#xA;&lt;ul&gt;&#xA;&lt;li&gt;没看懂： prediction may be 120 ± 15, in weeks, which represents a two-sided 95% confidence interval (i.e.,∼1.96 standard deviations subtracted from or added to the mean estimate assuming the model-predicted RUL follows a Gaussian distribution).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
