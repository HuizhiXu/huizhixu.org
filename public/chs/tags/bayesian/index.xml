<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bayesian on 徐慧志的个人博客</title>
    <link>https://huizhixu.github.io/chs/tags/bayesian/</link>
    <description>Recent content in bayesian on 徐慧志的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>chs</language>
    <lastBuildDate>Sat, 03 Feb 2024 17:01:50 +0800</lastBuildDate><atom:link href="https://huizhixu.github.io/chs/tags/bayesian/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bayesian Optimization</title>
      <link>https://huizhixu.github.io/chs/know_how/20240203%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/</link>
      <pubDate>Sat, 03 Feb 2024 17:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20240203%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%98%E5%8C%96/</guid>
      <description>贝叶斯优化有重要的两步步：
构造代理模型（surrogate model） 由获取函数（acquisition function）来生成采样建议 贝叶斯优化中，因为不知道目标函数的closed-form，所以需要构造一个代理模型（surrogate model）来近似目标函数。记住，代理模型对目标函数的潜在分布进行建模。通常用gaussian process来作为代理模型，也可以用random forest来作为代理模型。（任何模型，只要它为函数提供后验估计，可以用来作为surrogate model）。
有了后验估计之后，就可以用获取函数Acquisition Function来生成采样建议。
获取函数，经典的方法有EI 和 upper confidence bound，新的方法有safe constraint。
获取函数是用来找到全局最优解，好的获取函数会尽可能快地找到全局最优解。
序列决策 Sequential Decision-Making 在贝叶斯优化中，是通过一次又一次地做序列决策来达到优化的目的的。
基于特定的策略，优化器将收集观察到的数据点，更新潜在的函数的后验信念，给出下一个采样点进行探索，不断重复迭代以上过程。通常设置最大值/最小值来寻找最优值。
贝叶斯优化可以看作是一个 sequential decision process under uncertainty的过程。
这个过程具体来说是这样的：
优化器收集观测数据（observed data），基于特定的策略（policy），更新函数概率分布的后验信念（posterior belief），提出下一个采样点（next sampling）来进行探索，在提议的位置收集额外的数据点并重复。 不断地收集新的数据点，我们对函数知道的越来越多。 注意，上述过程包含很多东西，每一块都有研究者深入研究。
收集观测数据：收集观测数据一般会用到observation model。Observation model就是我们对数据的采样方法（比如说Sobol或Latin hypercube），这里的目的就是希望每次训练模型的时候可以给训练算法present一些最有代表性的数据。 策略（policy）是指在贝叶斯优化中用于选择下一个采样点的决策规则。如何选择下一个点，就是说我们想对什么东西做优化——这里可以是最优的expected结果（expected improvement）或者是探索空间（expected hypervolume）等。策略也需要决定何时终止探索过程。 优化器（Optimizer）在贝叶斯优化中是整个流程的一个统称。 策略和优化器在贝叶斯优化中紧密配合。策略指导优化器选择下一个采样点，而优化器根据观测数据和模型更新概率分布。 更新函数概率分布的后验信念，通常需要一个surrogate model。包括高斯过程优化（Gaussian Process Optimization）、序列模型优化（Sequential Model-based Optimization）等。高斯过程使用高斯过程模型来建模，序列模型优化使用随机森林等算法来建模。 除此之外，还有外循环和内循环的概念。
外循环：返回最佳点的位置或者最佳值本身。这个额外的点通常可以加入到已有的数据集来迭代下一轮。
内循环：返回采样位置候选。通常通过最大化获取函数来完成。
寻求最佳Policy 获取函数是一个打分器，它对每个候选位置打分，选取最大得分位置。
如果获取函数有解析表达式，可以进行求gradient操作，那么可以将对目标函数的全局优化，转化成对获取函数的优化。
如果获取函数无法微分，可以使用Monte Carlo Approximation来近似计算。</description>
    </item>
    
    <item>
      <title>Gaussian Process Regression with GPyTorch</title>
      <link>https://huizhixu.github.io/chs/know_how/20231217gaussian_process_regression_gpytorch/</link>
      <pubDate>Sun, 17 Dec 2023 17:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20231217gaussian_process_regression_gpytorch/</guid>
      <description>这个例子主要是利用GPytorch，来实现高斯过程回归。
计算Mean zero mean function gpytorch.means.ZeroMean() constant mean function gpytorch.means.ConstantMean() linear mean function gpytorch.means.LinearMean() 计算Covariance RBFKernel gpytorch.kernels.RBFKernel() adding a scaling coefficient: kernels.ScaleKernel(gpytorch.kernels.RBFKernel()) 一般会在核函数的输出上添加缩放系数。
在核函数的输出上添加缩放系数是为了调整核函数的影响力。
例如，如果我们希望某个核函数的输出对预测结果的贡献更大，我们可以使用较大的缩放系数。相反，如果我们希望某个核函数的输出对预测结果的贡献较小，我们可以使用较小的缩放系数。
通过在核函数的输出上应用kernels.ScaleKernel()，我们可以乘以一个固定的缩放因子，以增加或减小核函数的输出。
exact GP and approximate GP Exact inference applies when the closed-form expression of the posterior is available. We can simple and quick to compute the posterior distribution using gpytorch.models.ExactGP. Approximate inference applies when the posterior distribution involves high-dimensional integrals. It is difficult and time-consuming to compute.</description>
    </item>
    
    <item>
      <title>Gaussian Process in Practice 高斯过程实践</title>
      <link>https://huizhixu.github.io/chs/know_how/20231210gaussian_process_in_practice/</link>
      <pubDate>Sun, 10 Dec 2023 18:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20231210gaussian_process_in_practice/</guid>
      <description>这个例子主要是利用高斯过程的先验分布，将样本绘制成曲线。然后更新参数，利用后验分布获得新的曲线。
1. 先验分布 1.1 多变量高斯分布 创建一个包含n个候选输入位置的列表${x_i，i=1,&amp;hellip;,n}$ 初始化均值向量μ和协方差矩阵K（含n x n个元素） 假设x_1和x_2是多维的矩阵。x_1是一个 m* d的矩阵，x_2是一个nd的矩阵，那么K是一个mn的矩阵，$K[i,j] = k(x_1[i,:], x_2[j,:])$ 执行Cholesky分解K=LL T来获得L 通过LN（0,I）获得N（0,K）上的一个样本并存储在f_prior中 multivariante_samples01 和multivariante_samples02 这两个function的作用是一样的，只不过有两种写法。
1.2 看图可知 从先验过程采样的五个例子，其中大多数函数的值落在95%的可信区间内。 import numpy as np from matplotlib import pyplot as plt %matplotlib inline # 设置随机种子以确保重复性 np.random.seed(8) def plot_gp(mu, cov, title_str, X, X_train=None, Y_train=None, samples=[] ): X = X.ravel() # X.ravel()用于将多维数组X展平为一维数组。 mu = mu.ravel() uncertainty = 1.96 * np.sqrt(np.diag(cov)) # 通过计算协方差矩阵的对角线元素的平方根，可以得到每个参数的标准差。乘以 1.96，可以得到一个置信区间，表示该参数的不确定性范围。 plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=0.</description>
    </item>
    
    <item>
      <title>Kernel Function 核函数</title>
      <link>https://huizhixu.github.io/chs/know_how/20231207kernel_function/</link>
      <pubDate>Thu, 07 Dec 2023 18:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20231207kernel_function/</guid>
      <description>这篇文章主要解决三个问题：
正态分布的表示 核函数是什么，有什么类型 已知先验知识，如何计算后验分布 1. 正态分布的表示 正态分布一般表示为$f \sim N(0,K)$，书上写作 $p(f|x) = N(f|0,K)$。
为啥要多写一个f呢？
因为这个分布是针对f的分布，换句话说这里的随机变量是f，再换句话就是说这个随机变量f遵守一个正态分布。
2. 核函数是什么，有什么类型 核函数就是协方差。
核函数$K(x_i, x_j)$
它计算在输入空间中任意两个点的相似度，可以用欧式距离表示。 它度量输入空间中两点$x_i$和$x_j$之间的统计关系。 它量化$x_j$的变化和$x_i$的相应变化之间的相关性。 选择不同核函数，表示数据点之间的相关性被用不同方式来衡量。
有几种常见的核：
高斯核 Gaussian kernel 1.1 常见的高斯核 $$ K_{ij} = k(x_i,x_j) = e^{-||X_i-X_j||^2}$$
这里把负平方距离的指数作为距离度量。当x_i和x_j距离非常远，我们有x_i-x_j 趋向于无穷大，此时k_{ij}趋向于0。当x_i和x_j相等，k_{ij}等于1。K是一个介于0和1之间的数，由此就可以表现点之间的相关性。
1.2 可调节参数的高斯核，又被叫做isotropic squared exponential kernel $$K_{ij} = k(x_i,x_j) = \sigma_f^2e^{-\frac{1}{2l^2}||X_i-X_j||^2}$$ 2. 略(以后补充，暂时不是重点)
3. 已知先验知识，如何计算后验分布 假设我们有三个无噪声观测值，$ D = {(x_1,f(x_1)), (x_2, f(x_2)),(x_3, f(x_3))}$。我们需要对这三个随机变量进行建模。假设mean vector 为 $\mu$， covariance matrix为$K$。
这三个变量遵循多元变量的高斯分布
基于这个数据集D，假设我们现在想知道另一个变量$x_4$（它对应的f值用$f_*(x_4)$表示）在其他位置的均值和方差的的分布。
问题：f 和f* 是同一个分布吗？ 不是，用不同的字母表示不同的分布。
f和f*的分布为</description>
    </item>
    
    <item>
      <title>书籍 Bayesian Optimization Theory and Practice using Python 之Gaussian Process</title>
      <link>https://huizhixu.github.io/chs/know_how/20231125gaussian_process/</link>
      <pubDate>Sat, 25 Nov 2023 18:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20231125gaussian_process/</guid>
      <description>1. 理解covariance matrix Gaussian Process is a stochastic process used to characterize the distribution over function.
GP将一组有限的参数theta从一个连空间拓展到一个连续无限空间的一个无限函数f。
假设我们有两个变量，X1和X2，它俩符合multivariate Gaussian distribution。
一个高斯分布可以用mean vector 和covariance matrix来表示。均值向量描述了从高斯分布重复采样的集中趋势，协方差矩阵描述了点之间的相关性。（The mean vector describes the central tendency if we were to sample from the Gaussian distribution repeatedly, and the covariance matrix describes how the features of the data are related to each other）
假设mean vector matrix K为：
K 可以告诉我们，当x1增加的时候，x2变化的大小和方向是如何变化的。K用点积来衡量x1维和x2维的相似性。
$$\sigma_{11}^2 = var(x_1) = E[(x_1-E[x_1])^2] = E[(x_1)^2]$$
$$\sigma_{12}^2 = \sigma_{21}^2 = E[(x_1-E[x_1])(x_2-E[x_2])] = E[x_1x_2]$$</description>
    </item>
    
    <item>
      <title>论文 Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics</title>
      <link>https://huizhixu.github.io/chs/know_how/20231120uncertainty/</link>
      <pubDate>Mon, 20 Nov 2023 18:31:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20231120uncertainty/</guid>
      <description>Abstract
types 第一种分类 data uncertainty (measurement noise) model uncertainty ( limited data) 第二种分类 epistemic uncertainty 认知上的不确定性，通常是由于没有足够的知识（数据）而产生 can be reducible 分为两类 model-form uncertainty 由于模型的选择导致，例如architectures, activation functions or kernel functions parameter uncertainty 在训练过程产生，由于数据不够导致 aleatory uncertainty stems from physical systems, 具有随机性, cannot be reducible e.g. noises 这种类型的不确定性在ML模型里面被看成是似然函数的一部分(a part of the likelihood function) 也被叫做data uncertainty 捕捉这种不确定性的方式有：同方差 homoscedastic和异方差 heteroscedastic 例子： test data和train data不同分布：epistemic uncertainty (model performs poorer in extrapolation than in interpolation) 测量数据由仪器导致的误差是aleatory Unc， 大试如果由于精度原因导致，则属于epistemic unc，因为提高精度可以减少这个误差 causes methods: Gaussian process regression a ML method with UQ capability 一般不用来quantify uncertainty of a final surrogate 一般用来在高度不确定的采样空间里采样，来减少训练样本的数量 to build an accurate surrogate within some lower and upper bounds of input variables to find a globally optimally design for black-box objective function 一般不评估GPR的UQ质量 因为预测一般在pre-defined design bounds Bayesian neural network Monte Carlo dropout as an alternative to traditional Bayesian neural network neural network ensemble neural network ensemble consisting of multiple neural networks deterministic UQ methods metrics classification probability can be viewed as uncertainty regression confidence interval : 没看懂： prediction may be 120 ± 15, in weeks, which represents a two-sided 95% confidence interval (i.</description>
    </item>
    
  </channel>
</rss>
