<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm on 徐慧志的个人博客</title>
    <link>https://huizhixu.github.io/chs/tags/llm/</link>
    <description>Recent content in llm on 徐慧志的个人博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>chs</language>
    <lastBuildDate>Mon, 22 Apr 2024 23:01:50 +0800</lastBuildDate><atom:link href="https://huizhixu.github.io/chs/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>用大语言模型打造AI Agent</title>
      <link>https://huizhixu.github.io/chs/know_how/20240421ai_agent/</link>
      <pubDate>Mon, 22 Apr 2024 23:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20240421ai_agent/</guid>
      <description>人类需要的不仅仅是大模型，而是能做复杂的多步骤的任务的大模型，Agent因此诞生了。
知名的AI Agent 1. AutoGPT: https://github.com/Significant-Gravitas/AutoGPT AutoGPT是一个由Significant Gravitas开发的开源项目,旨在创建一个自主的AI代理,能够持续地学习、成长并完成各种任务。
2. AgentGPT: https://agentgpt.reworkd.ai/ AgentGPT是一个由Reworkd.ai开发的项目,允许用户与大型语言模型代理进行交互,并指派任务和提供反馈。
3. 会自己玩Minecraft的 AI：
https://arxiv.org/abs/2305.16291 https://github.com/MineDojo/Voyager 这个AI系统能够在Minecraft游戏中自主采取行动、制定策略并完成任务,展现了强大的理解和规划能力。
**4. 由语言模型操控的机器人：Figure 01 **
Figure 01项目探索了由语言模型控制机器人的可能性。机器人能够根据语言指令采取行动。
视频链接: https://www.youtube.com/watch?v=Sq1QZB5baNw 论文： Inner Monologue: Embodied Reasoning through Planning with Language Models：https://arxiv.org/abs/2207.05608
5. 用大型语言模型自动驾驶: Talk2drive
https://arxiv.org/abs/2312.09397 Talk2Drive项目旨在利用大型语言模型来控制自动驾驶汽车,让汽车能够理解和执行自然语言指令。
AI Agent运作的原理 AI Agent的组成：
终极目标 记忆（经验） 从环境中得知状态 计划（短期目标） 行动 终极目标：这是Agent被赋予的最高层次目标或使命，是它所有行为和决策的根本驱动力。
记忆(经验)：Agent会保留自己过去的观察、行动和结果，作为累积的经验和知识库，用于指导未来的决策。这相当于人类的记忆能力。
从环境中得知状态：通过各种传感器，Agent能感知当前所处的环境状态，例如视觉、声音等输入。这让它能根据具体情况做出反应。
计划(短期目标)：基于终极目标、当前状态和过去经验，Agent会制定可实现的近期计划和子目标作为中介步骤。这需要策略规划和决策能力。
行动：Agent根据计划输出具体的行动命令，并在环境中执行，产生新的状态作为下一个循环的输入。行动的范围取决于Agent的机动性。
这种感知-规划-执行的循环让AI Agent有针对性地采取行动以实现目标。其中关键是Agent如何高效利用经验、规划未来行动路径。不同的Agent架构在具体实现方法上有所差异。
最后的目标 实现一个有执行力有记忆的ChatGPT， 例如MemGPT。（https://arxiv.org/abs/2310.08560）
注：以上笔记内容来自李宏毅教授的课程： INTRODUCTION TO GENERATIVE AI ，此篇文章内容包括第9讲。</description>
    </item>
    
    <item>
      <title>让AI村民组成虚拟村庄会发生什么事</title>
      <link>https://huizhixu.github.io/chs/know_how/20240414ai_virtual_town/</link>
      <pubDate>Sun, 14 Apr 2024 19:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20240414ai_virtual_town/</guid>
      <description>去年Agent很火的时候，就知道有斯坦福出的这个虚拟小镇的论文了，当时大家都很好奇，怎么能够让大语言模型来操纵agent做出非常复杂的行为呢？
事实上，该系统是基于规则的一种变体。 小镇里有各种各样的角色人物，它预先设定了各个角色人物一天的行程安排。每个角色都有自己明确的人物设定和说明。例如，医生一天的工作包括在诊所为患者诊治，整理药品,以及进食、休息等日常作息。如果角色被固定下来，那么他们一天的日程安排也就相对固定了。
之后，角色人物根据既定规划采取相应的行动。角色人物需依据规划进行下一步行动，该行动可视为状态的转变。
角色人物如何模拟当前情景？他们通过观察来进行。构建一个观察场景，其中包含观察者和被观察者。例如，在没有事件发生的情况下，医生作为观察者，而房间作为被观察者，医生的状态保持不变，即处于等待患者的状态。医生会定期执行观察动作。一旦患者到来，医生的状态便从“等待”转变为“看病”。患者的到来对医生的状态产生影响，这就是角色人物通过大型语言模型对外界行为做出反应。
角色人物如何知晓应做出何种反应？每当有物品出现在角色旁边，便会触发观察操作。角色人物会提出两个问题：第一，该物品与角色之间的关系是什么？第二，角色将对该物品采取何种行动？角色人物会持续提出这两个问题，而大语言模型则依据角色设定进行回答。
角色人物的日常生活往往是单调的，因为他们的行为模式是预先设定好的。为了打破这种单调，需要外部刺激，例如有人举办活动或参加聚会，这些活动会引发角色人物的后续反应。
如何改变角色人物的状态，进而改变其设定或生活？这需要通过反思（reflect）来实现。反思是对记忆的回顾，能够提炼出真正重要的内容。何时进行反思？当近期发生了许多重要事件时，这段记忆就显得尤为重要。如何评判事件的重要性？这由GPT来决定。反思的内容是什么？GPT会根据最近发生的事情，为机器生成三个问题，并引导机器检索记忆并作出回答。
为什么说这个小镇是基于规则的变体？人物设定带来的routine基本上是确定的，大语言模型能带来一些在语言上的小的改变，以及大语言模型本身作为一个决策器，去判断下一步的交互。
小镇网址： https://reverie.herokuapp.com/arXiv_Demo/ </description>
    </item>
    
    <item>
      <title>大型语言模型修炼史(第三阶段)</title>
      <link>https://huizhixu.github.io/chs/know_how/20240413the-history-of-cultivating-llm_second_part/</link>
      <pubDate>Sat, 13 Apr 2024 19:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20240413the-history-of-cultivating-llm_second_part/</guid>
      <description>第三阶段：参与实战，打磨技巧 如何克服第二阶段的局限性呢？
关键是用第一阶段的参数作为初始参数。
（贝叶斯定理这不就来了嘛！）
所以第三阶段是由第一阶段和第二阶段组合而成的：
第一阶段：通过网络上任何语料学习而来的，叫做预训练Pretrain
第二阶段：通过人类标注的学习，叫做Instruction Fine-tuning
第二阶段的最佳化这个过程找出来的参数和初始参数很不一样怎么办？
为什么会在意这个事情？
因为第一阶段之后，模型的能力很好：在多种语言上做预训练，只要教某一个语言的某一个任务，自动学会其他语言的同样任务。所以我们不希望模型的参数有很大的变化。
如何解决？
用Apapter， 例如LoRA。
**Adapter是指固定或者插入参数。Lora是这样做的：**假设已经有初始参数，在做最佳化的时候，初始参数完全不变，只在模型后面多加几层，最佳化是找这几层的参数。
目前大模型Finetune有两个路线：
Pretrain + Finetune 打造不同的专才（为不同的任务收集不同的语料，Bert 适合打造一堆专才） Pretrain + Finetune 打造一个通才 （收集涵盖不同任务的标注语料，放入一个模型） RLHF 让模型和使用者互动，用的就是RLHF（Reinforcement Learning from Human Feedback）这个技术，这句诗第三阶段。
这三个阶段示例如下：
第一阶段：Pretrain 输入：&amp;#34;输入：人工智&amp;#34; 输出：&amp;#34;能&amp;#34; 第二阶段：Instruction Fine-tuning 输入：&amp;#34;User：世界上有几大洲？AI：&amp;#34; 输出：&amp;#34;七大洲&amp;#34; 第三阶段：RLHF 输入：&amp;#34;User：世界上有几大洲？AI：&amp;#34; 输出：&amp;#34;七大洲&amp;#34; &amp;gt; &amp;#34;谁来告诉我呀&amp;#34; RLHF： 当人类告诉语言模型一个答案比另一个答案要好的时候，语言模型就微调它的参数。让人类觉得好的答案多出现，人类觉得不好的答案少出现。
从人类产生训练资料的角度来看，RLHF和Instruction Fine-tuning有什么不同？
两个阶段都需要人类介入 Instruction Fine-tuning 需要准备好问题和答案，人类比较辛苦 RLHF 只需要做判断，人类比较轻松 人类写出正确答案不容易，判断好坏却很容易。进一步来说，对不同答案进行排名比判断一个答案好坏要容易，因为答案好坏是相对的。 从模型学习的角度来看，RLHF和Instruction Fine-tuning有什么不同？
Instruction Fine-tuning 模型学的是怎样接下一个字 ，需要每一步都是对的，结果才可能是合理的。模型对整个结果没有通盘考量。（只问过程，不问结果） RLHF 模型进入新的思考模式，不管中间接龙的每一步如何，只管最后的结果。（只问结果，不问过程） AlphaGo：整体使用一个深度神经网络，但是每一步是要解决一个分类问题，从棋盘里面去选择哪一个点最好。
语言模型也是一样，整体是深度神经网络，每一步都是分类问题，文字接龙，去选择概率最大的那个字。
如何更有效的利用人类的反馈？ 反馈模型（Reward Model） 用人类的反馈去训练一个反馈模型（Reward Model），用反馈模型的输出来模拟人类的喜好，达到这样的效果：“如果是人类的话会这样判断”。</description>
    </item>
    
    <item>
      <title>大型语言模型修炼史（第一、二阶段）</title>
      <link>https://huizhixu.github.io/chs/know_how/20240405the-history-of-cultivating-llm/</link>
      <pubDate>Fri, 05 Apr 2024 20:01:50 +0800</pubDate>
      
      <guid>https://huizhixu.github.io/chs/know_how/20240405the-history-of-cultivating-llm/</guid>
      <description>注意：以下笔记内容来自李宏毅教授的课程： INTRODUCTION TO GENERATIVE AI ，此篇文章内容包括第6讲和第7讲。
背景知识 大模型的本质是文字接龙。输入一个未完成的句子，输出这个未完成的句子的下一个token。
大模型可以看成是一个函数。$$ f(未完成的句子)= 下一个token $$这个函数是一个有数十亿个未知参数的函数。
1. 那么怎么找出这些未知参数呢？需要训练资料。
通过训练资料找出参数的过程叫 training 或者 learning。
找出未知参数之后，通过函数来完成文字接龙的过程叫 testin 或者 inference。
最佳化 optimization：把大模型比作机器，机器训练前需要设定一些参数（例如学习率），这些参数叫超参数hyperparameter。通过这些超参数，机器可以找到最佳的参数。
2.贝叶斯思想
训练可能会失败，这意味着找到的参数不符合训练资料，例如，输入”床前明月“，函数输出的不是”光“。那么，就需要换一组超参数再训练一次。
也有可能出现，训练成功，但是测试失败，这就是Overfitting。
下面这段话是训练大模型的中心思想。 模型开始训练之前，除了要设置超参数，还要设置初始参数。一般来说，这些初始参数是随机化设置的。那我们也可以尝试给初始参数一些信息，这些初始参数是先验知识，我们给的先验知识越好，模型最后找到的参数会越接近我们想要的。
语言模型的三个阶段 把训练语言模型的历史分为三个阶段： （李宏毅教授除了用动漫举例子还会用武侠啊！！！）
自我学习，积累实力 名师指点，发挥潜力 参与实战，打磨技巧 第一阶段：自我学习，积累实力 训练资料的组成部分 需要多少训练资料语言模型才能学会做文字接龙呢？
这些训练资料，需要包括两方面，第一是语言知识——语法。根据研究表明，一亿个词就可以掌握语法。
第二是世界知识，也就是一些物理世界的知识。例如水的沸点是摄氏100度，而不是50度。（尽管50度也符合语言知识。）根据研究表明，300亿个词也不一定能掌握世界知识。
训练细节 训练资料哪里有：网络 训练方法：自监督学习：机器自己教自己，不需要人工介入。（训练语料同时也是Label） 提到了两篇论文 论文 Scaling Language Models: Methods, Analysis &amp;amp; Insights from Training Gopher（https://arxiv.org/abs/2112.11446）：讲述 Deepmind 训练模型 Gopher 的过程。 里面讲了如何进行训练资料的清理，例如，过滤html符号、过滤色情暴力等有害内容、去除低品质资料（使用语料品质分类器决定）、去除重复资料等。
论文 Deduplicating Training Data Makes Language Models Better(https://arxiv.org/abs/2107.06499) : 讲述了去重技术可以让大模型性能更好。</description>
    </item>
    
  </channel>
</rss>
