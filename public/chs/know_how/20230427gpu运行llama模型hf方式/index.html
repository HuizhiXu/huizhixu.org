<!doctype html><html lang=chs itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=../../../favicon.svg><title>2023-04-27GPU运行LLaMa模型——用HF的方式推理 - 徐慧志的个人博客</title><meta name=description content="很简单的运行LLaMa的方法"><meta name=author content="Huizhi"><meta name=generator content="Hugo 0.152.2"><link rel=stylesheet href="/css/styles.min.cc1204abf55b2794a944c9970dcfbbedd8cddb0c0451f7d9b088371efe0b6248.css" integrity crossorigin=anonymous><meta property="og:url" content="https://huizhixu.github.io/chs/know_how/20230427gpu%E8%BF%90%E8%A1%8Cllama%E6%A8%A1%E5%9E%8Bhf%E6%96%B9%E5%BC%8F/"><meta property="og:site_name" content="徐慧志的个人博客"><meta property="og:title" content="2023-04-27GPU运行LLaMa模型——用HF的方式推理"><meta property="og:description" content="很简单的运行LLaMa的方法"><meta property="og:locale" content="chs"><meta property="og:type" content="article"><meta property="article:section" content="know_how"><meta property="article:published_time" content="2023-04-27T18:31:50+08:00"><meta property="article:modified_time" content="2023-06-30T00:00:00+00:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Ai"><meta name=twitter:card content="summary"><meta name=twitter:title content="2023-04-27GPU运行LLaMa模型——用HF的方式推理"><meta name=twitter:description content="很简单的运行LLaMa的方法"><meta itemprop=name content="2023-04-27GPU运行LLaMa模型——用HF的方式推理"><meta itemprop=description content="很简单的运行LLaMa的方法"><meta itemprop=datePublished content="2023-04-27T18:31:50+08:00"><meta itemprop=dateModified content="2023-06-30T00:00:00+00:00"><meta itemprop=wordCount content="1944"><meta itemprop=keywords content="Tech,Ai"><meta name=lang content="chs"></head><body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><header class="container flex justify-between md:justify-between gap-4 flex-wrap p-6 mx-auto relative"><a href=https://huizhixu.github.io/chs/ class="capitalize font-extrabold text-2xl"><img src=../../../blist-logo.png alt=徐慧志的个人博客 class="h-8 max-w-full">
</a><button class="mobile-menu-button md:hidden">
<svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="4" y1="8" x2="20" y2="8"/><line x1="4" y1="16" x2="20" y2="16"/></svg></button><ul class="mobile-menu absolute z-10 px-6 pb-6 md:p-0 top-full left-0 w-full md:w-auto md:relative hidden md:flex flex-col md:flex-row items-end md:items-center gap-4 lg:gap-6 bg-white dark:bg-gray-800"><li><a href=../../../chs/know_how/>技术</a></li><li><a href=../../../chs/life/>生活见闻</a></li><li><a href=../../../chs/page/about/>关于</a></li><li><a href=../../../chs/link/>宝藏集结</a></li><li><a href=../../../chs/tags/>分类</a></li><li class="relative cursor-pointer"><span class="language-switcher flex items-center gap-2"><svg width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><line x1="3.6" y1="9" x2="20.4" y2="9"/><line x1="3.6" y1="15" x2="20.4" y2="15"/><path d="M11.5 3a17 17 0 000 18"/><path d="M12.5 3a17 17 0 010 18"/></svg>
<a>语言</a>
<svg width="14" height="14" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12" transform="rotate(180 12 12)"/></svg></span><div class="language-dropdown absolute top-full mt-2 left-0 flex-col gap-2 bg-gray-100 dark:bg-gray-900 dark:text-white z-10 hidden"><a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href=../../../en/ lang=en>English</a>
<a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href=../../../de/ lang=de>Deutsch</a></div></li><li class="grid place-items-center"><span class="open-search inline-block cursor-pointer"><svg width="20" height="20" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></span></li><li class="grid place-items-center"><span class="toggle-dark-mode inline-block cursor-pointer"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="3"/><line x1="12" y1="5" x2="12" y2="5.01"/><line x1="17" y1="7" x2="17" y2="7.01"/><line x1="19" y1="12" x2="19" y2="12.01"/><line x1="17" y1="17" x2="17" y2="17.01"/><line x1="12" y1="19" x2="12" y2="19.01"/><line x1="7" y1="17" x2="7" y2="17.01"/><line x1="5" y1="12" x2="5" y2="12.01"/><line x1="7" y1="7" x2="7" y2="7.01"/></svg></span></li></ul></header><main class=flex-1><article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4"><h1 class="text-2xl font-bold mb-2">2023-04-27GPU运行LLaMa模型——用HF的方式推理</h1><h5 class="text-sm flex items-center flex-wrap"><svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg>
发布于
2023年04月27日
&nbsp(最近编辑
2023年06月30日
)</h5><h5 class="text-sm flex items-center flex-wrap"><svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
4&nbsp;分钟
&nbsp;&bull;
<svg class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 19a9 9 0 019 0 9 9 0 019 0"/><path d="M3 6a9 9 0 019 0 9 9 0 019 0"/><line x1="3" y1="6" x2="3" y2="19"/><line x1="12" y1="6" x2="12" y2="19"/><line x1="21" y1="6" x2="21" y2="19"/></svg>
1944&nbsp;字</h5><details id=TableOfContents class="px-4 mt-4 bg-gray-100 dark:bg-gray-700 rounded toc"><summary class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white"><span>Table of contents</span>
<svg class="icon icon-tabler icon-tabler-chevron-down" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="6 9 12 15 18 9"/></svg></summary><ul class="mt-2 pb-4"><li><a href=#%e6%96%87%e6%a1%a3>文档</a></li><li><a href=#%e5%ae%9e%e8%b7%b5>实践</a><ul><li><a href=#1-%e5%85%8b%e9%9a%86%e9%a1%b9%e7%9b%ae>1. 克隆项目</a></li><li><a href=#2-%e5%ae%89%e8%a3%85%e7%8e%af%e5%a2%83>2. 安装环境</a></li><li><a href=#3-%e4%b8%8b%e8%bd%bdmeta%e5%8f%91%e5%b8%83%e7%9a%84%e5%8e%9f%e7%94%9f%e7%9a%84llama%e6%a8%a1%e5%9e%8b>3. 下载meta发布的原生的Llama模型</a></li><li><a href=#4-%e5%b0%86%e5%8e%9f%e7%89%88%e7%9a%84%e8%bd%ac%e6%8d%a2%e6%88%90hf%e6%a0%bc%e5%bc%8f>4. 将原版的转换成hf格式</a></li></ul></li><li><a href=#%e5%93%88%e5%b8%8c%e5%80%bc%e8%bf%9b%e8%a1%8c%e5%ae%8c%e6%95%b4%e6%80%a7%e6%a0%a1%e9%aa%8c>哈希值进行完整性校验</a></li></ul></details><p>在GPU上运行中文LLaMa模型，主要是按照 <a href=https://github.com/ymcui/Chinese-LLaMA-Alpaca target=_blank rel=noopener>https://github.com/ymcui/Chinese-LLaMA-Alpaca</a>
这个仓库的方法。
中文LLaMa模型和中文Alpaca的区别是：中文LLaMa在英文llama的基础上扩充了中文词表并且使用了中文数据进行二次训练。中文LLaMa只能进行单轮问答。中文Alpaca经过instruct-tuning 生成，可以进行多轮问答。本次实验主要是针对中文LLaMa模型。</p><h1 id=文档>文档</h1><p>模型部署和推理有四种方法，我选择的是用HF的inference接口来进行推理。</p><p><a href=https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8Transformers%E6%8E%A8%E7%90%86 target=_blank rel=noopener></a><a href=https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/%E4%BD%BF%E7%94%A8Transformers%E6%8E%A8%E7%90%86 target=_blank rel=noopener>https://github.com/ymcui/Chinese-LLaMA-Alpaca/wiki/使用Transformers推理</a>
这里详细讲了用scripts/inference_hf.py来启动模型。</p><p>原则上非常简单，直接运行下列的脚本，就可以进行推理。</p><pre tabindex=0><code>CUDA_VISIBLE_DEVICES={device_id} python scripts/inference_hf.py \\
    --base_model path_to_original_llama_hf_dir \\
    --lora_model path_to_chinese_llama_or_alpaca_lora \\
    --with_prompt \\
    --interactive
</code></pre><p>对参数进行解释：</p><ul><li>base_model是Meta发布的原生llama模型</li><li>lora_model是 这个是LoRa生成的模型，可以在网盘下载，也可以用HF的模型调用（例如ziqingyang/chinese-llama-lora-7b）。模型调用比较简单推荐使用。</li><li>with_prompt 是否将输入与prompt模版进行合并。</li><li>interactive 以交互方式启动，以便进行多次单轮问答。</li></ul><p>在实验之前，首先要搞清楚一些概念：</p><ol><li>LoRa和Alpaca模型是无法单独完成推理的，需要和META的原生LLAMA结合才能运行。
远程LLAMA模型META提供，LoRa和Alpaca模型这个项目提供。</li></ol><p>为什么不能用lora模型单独推理，以我浅显的理解，它freeze了原来的模型，单独加了一些层，后续的中文训练都在这些层上做，所以需要进行模型融合。</p><ol start=2><li>用huggingface的推理脚本，需要将模型转换成HF支持的格式。（Don’t worry 作者把脚本都写好了）</li></ol><h1 id=实践>实践</h1><p>下面用步骤的形式记录一下整个过程。</p><h2 id=1-克隆项目>1. 克隆项目</h2><pre tabindex=0><code>git clone git@github.com:ymcui/Chinese-LLaMA-Alpaca.git
cd Chinese-LLaMA-Alpaca
</code></pre><h2 id=2-安装环境>2. 安装环境</h2><pre tabindex=0><code>pip install -r requirements.txt
</code></pre><p>这一步出现了ERROR: No matching distribution found for peft==0.3.0dev</p><p>解决：最后安装了peft==0.2.0</p><h2 id=3-下载meta发布的原生的llama模型>3. 下载meta发布的原生的Llama模型</h2><ul><li>可以下载泄露版本，需要用<a href="magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&amp;dn=LLaMA%ef%bc%89">磁力链下载</a>
。</li></ul><p><a href=https://github.com/facebookresearch/llama/pull/73/files/56de950af8a48c7cae221581e2e3e2c342b2ad82 target=_blank rel=noopener>泄露地址在这</a></p><ul><li>也可以用HuggingFace上的7B模型</li></ul><pre tabindex=0><code>mkdir -p models/7B/
wget -P models/7B/ &lt;https://huggingface.co/nyanko7/LLaMA-7B/resolve/main/consolidated.00.pth&gt;
wget -P models/7B/ &lt;https://huggingface.co/nyanko7/LLaMA-7B/raw/main/params.json&gt;
wget -P models/7B/ &lt;https://huggingface.co/nyanko7/LLaMA-7B/raw/main/checklist.chk&gt;
wget -P models/ &lt;https://huggingface.co/nyanko7/LLaMA-7B/resolve/main/tokenizer.model&gt;
</code></pre><h2 id=4-将原版的转换成hf格式>4. 将原版的转换成hf格式</h2><p>下载这些模型后，models文件夹会出现以下文件夹和文件。如果不是这种层级关系，需要换成这种。</p><pre tabindex=0><code>models
	tokenizer.model
	7B
    consolidated.00.pth
	  params.json
	  checklist.chk
</code></pre><p>这些文件是PyTorch（<code>.pth</code>格式的），是不能被HuggingFace-transformers加载的，需要把这些文件转成HuggingFace格式的才行。</p><p>这里有两个选择：</p><p>一、 如果转换为HF格式之后，后续用llama.cpp进行部署，用这个脚本convert_llama_weights_to_hf 转换成HF再融合再部署。运行</p><pre tabindex=0><code>python src/transformers/models/llama/convert_llama_weights_to_hf.py \\
    --input_dir models \\
    --model_size 7B \\
    --output_dir models/7B_hf
</code></pre><p>转换后的模型会放在output_dir也就是models/7B_hf下面。转换后的模型会有一个config.json。</p><p>如果后续有的步骤出现错误， <strong>does not appear to have a file named config.json. 要记得把这里的解决方法搬过去。</strong></p><p>（来源：</p><p><a href=https://github.com/hpcaitech/ColossalAI/issues/3324 target=_blank rel=noopener>https://github.com/hpcaitech/ColossalAI/issues/3324</a></p><p>Basically you have to convert your downloaded weights to Hugging Face Transformers format using this<code>python src/transformers/models/llama/convert_llama_weights_to_hf.py \\ --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</code></p><p>Then you would be able to get the config.json.</p><p>Source: <a href=https://huggingface.co/docs/transformers/main/en/model_doc/llama target=_blank rel=noopener></a>
<a href=https://huggingface.co/docs/transformers/main/en/model_doc/llama target=_blank rel=noopener>https://huggingface.co/docs/transformers/main/en/model_doc/llama</a></p><p>）</p><p>二、如果转换为HF格式之后，后续用 HuggingFace-transformers 进行推理，可以把直接合并和转换，运行这一个步骤就可以了。</p><p>运行</p><pre tabindex=0><code>CUDA_VISIBLE_DEVICES=1 python scripts/merge_llama_with_chinese_lora_to_hf.py \\
--base_model  /models/7B \\
--lora_model  ziqingyang/chinese-llama-lora-7b \\
--output_dir /home/user/data/xuhuizhi/llama_pj/models/7B_cn
</code></pre><p>如果出现错误，没有config.json文件，需要运行方法一，把转换后的文件里面的config.json拿过来，也就是说方法一和方法二两个步骤都要运行。</p><p>##5. 推理</p><p>运行：</p><pre tabindex=0><code>CUDA_VISIBLE_DEVICES=0 python scripts/inference_hf.py \\
--base_model  /home/user/llama_pj/models/7B_cn \\
--lora_model  ziqingyang/chinese-llama-lora-7b \\
--with_prompt \\
--interactive
</code></pre><p>结果</p><pre tabindex=0><code>Input:苹果是植物吗
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Response:  I don&#39;t know if it was plants or animals, but I do remember seeing lots of them on my way to school every morning and coming home from work in the evening。-sarah brown student at university college for four years before she graduated with her bachelor degree in science after taking all required courses such as chemistry physics biology organic chemistry microbiology cellular immunobiology molecular genetic epidemiological theoretic biochemical physiologically based pharmacokinetic modeling statistics data analysis computer programming laboratory experience coursework research projects internships community service volunteer experiencesnetworking career fair job interviewsjob huntingresume writing cover letterspersonal statementwriting sampleessay editingcritiquesmock interviewpreparationMock Interview Mock Job FairMock Networking EventMock Career FairsMock Cover Letter Writing WorkshopsMock Personal Statement EditingWorkshopNetworking EventsMock Professional Development ProgrammeMock Business School Admission Test PreparationsMock GRE/GMAT Prep CourseMockGPA CalculatorMockPersonasMockResearch ProposalsMockPresentationMockPostersMockPPTSlidesMockMockInteractionMockJobOfferMockoffer mock offerMockMockReferencesMockReferenceLettersMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMockMock
</code></pre><p>不知为何回复一直是英文的。</p><p>切换到CPU，也不能用中文回答。</p><pre tabindex=0><code>CUDA_VISIBLE_DEVICES= python scripts/inference_hf.py \\
--base_model  /home/user/llama_pj/models/7B_cn \\
--lora_model  ziqingyang/chinese-llama-lora-7b \\
--with_prompt \\
--interactive
</code></pre><p>试验输出如下</p><pre tabindex=0><code>Input:索尼最新的耳机
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Response:  I will not buy this product because it doesnot work well and I don&#39;t like how much moneyit costs。-end of classroom conversation about what to do next in their lives after graduation.- end of lecture room discussion on whatafter collegegraduates should be doingnext weekendafter they leave school.- endofclassroombalance between students who are leavingschooland thosewho aren’tthe same age but have been out for longer time periodsthank you verymuchfor your help with my English study book, whichis great! thankyouverymuchfortaking care ofmyEnglishstudybookwhichistooexcellenttoo young people aged under 21 years old at present as many other younger peopleshoot down our plan before we can even get started.- end oflecture room discussionaboutwhat to dowhen two or more adults over twenty oneyearsofageare discussed togetherwithin themore than twice per year when older childrenundertwentyoneyearsinnumberbutthemoresimplythanthatmanyothersmore youthanchildrenagedownourplanbeforewecangetstarteduponestartingoutagain.[Endoffeatoryconversatilonwhathappenswhenolderadultswerebornaswellasthedidnorthenumbertheiragesbecomewithintimeperiodfromthenexttwoyearsonheretogetherforeveryoutherthanormalchild.] End offcourse convoerationsion wthing tha happen whevernold adults were born along wit hte ddid nort he rnth er number ther own ages came up again from then until now once upon another timesometimesonce againeachother timetime ago again] End of lectureroomsdiscussionabouticonshowingsome things happened some others did nothing yetsome thingssometimethatearlierthisweeknowhere
</code></pre><h1 id=哈希值进行完整性校验>哈希值进行完整性校验</h1><p>如果实验过程中不确定自己下载的模型是否完整，可以校验哈希值。官方模型的哈希值可以在仓库里找到。</p><pre tabindex=0><code>cd modles/7B

echo &#34;6efc8dab194ab59e49cd24be5574d85e  consolidated.00.pth&#34; | md5sum --check -           
echo &#34;7596560e011154b90eb51a1b15739763  params.json&#34; | md5sum --check -
</code></pre><p>如果返回True则一致。</p></article><div class="px-2 mb-2"><script src=https://utteranc.es/client.js repo=HuizhiXu/huizhixu.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><div class="bg-blue-100 dark:bg-gray-900"><div class="container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center"><div><div class="text-2xl font-bold mb-2">Sein heißt werden, leben heißt lernen.</div><p class=opacity-60>Der einfache Weg is immer verkehrt.</p></div><ul class="flex justify-center gap-x-3 flex-wrap gap-y-2"><li><a href=https://twitter.com/ target=_blank rel=noopener aria-label=Twitter class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://github.com/ target=_blank rel=noopener aria-label=GitHub class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ul></div></div></main><footer class="container p-6 mx-auto flex justify-between items-center"><span class="text-sm font-light">Copyright © 2012 - Huizhi Xu · All rights reserved
</span><span onclick='window.scrollTo({top:0,behavior:"smooth"})' class="p-1 cursor-pointer"><svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12"/></svg></span></footer><div class="search-ui absolute top-0 left-0 w-full h-full bg-white dark:bg-gray-800 hidden"><div class="container max-w-3xl mx-auto p-12"><div class=relative><div class="my-4 text-center text-2xl font-bold">Search</div><span class="p-2 absolute right-0 top-0 cursor-pointer close-search"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></span></div><input type=search class="py-2 px-3 w-full dark:text-black border dark:border-transparent" placeholder="Enter search query"><div class="search-results text-lg font-medium my-4 hidden">Results</div><ul class="search-list my-2"></ul><div class="no-results text-center my-8 hidden"><div class="text-xl font-semibold mb-2">No results found</div><p class="font-light text-sm">Try adjusting your search query</p></div></div></div><script src=https://huizhixu.github.io/js/scripts.min.js></script><script>const languageMenuButton=document.querySelector(".language-switcher"),languageDropdown=document.querySelector(".language-dropdown");languageMenuButton.addEventListener("click",e=>{e.preventDefault(),languageDropdown.classList.contains("hidden")?(languageDropdown.classList.remove("hidden"),languageDropdown.classList.add("flex")):(languageDropdown.classList.add("hidden"),languageDropdown.classList.remove("flex"))})</script><script>const darkmode=document.querySelector(".toggle-dark-mode");function toggleDarkMode(){document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("darkmode","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("darkmode","dark"))}darkmode&&darkmode.addEventListener("click",toggleDarkMode);const darkStorage=localStorage.getItem("darkmode"),isBrowserDark=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches;!darkStorage&&isBrowserDark&&document.documentElement.classList.add("dark"),darkStorage&&darkStorage==="dark"&&toggleDarkMode()</script><script>const mobileMenuButton=document.querySelector(".mobile-menu-button"),mobileMenu=document.querySelector(".mobile-menu");function toggleMenu(){mobileMenu.classList.toggle("hidden"),mobileMenu.classList.toggle("flex")}mobileMenu&&mobileMenuButton&&mobileMenuButton.addEventListener("click",toggleMenu)</script></body></html>