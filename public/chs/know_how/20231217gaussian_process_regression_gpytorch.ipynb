{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Gaussian Process Regression with GPyTorch\n",
        "date: '2023-12-17T17:01:50+08:00'\n",
        "tags:\n",
        "  - tech\n",
        "  - bayesian\n",
        "format: hugo-md\n",
        "html-math-method: webtex\n",
        "thumbnail: 'https://picsum.photos/id/307/400/250'\n",
        "---"
      ],
      "id": "247d9d10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这个例子主要是利用GPytorch，来实现高斯过程回归。\n",
        "\n",
        "# 计算Mean\n",
        "\n",
        "1. zero mean function ```gpytorch.means.ZeroMean()```\n",
        "2. constant mean function ```gpytorch.means.ConstantMean()```\n",
        "3. linear mean function ```gpytorch.means.LinearMean()```\n",
        "\n",
        "# 计算Covariance\n",
        "1. RBFKernel ```gpytorch.kernels.RBFKernel()```\n",
        "2. adding a scaling coefficient: ```kernels.ScaleKernel(gpytorch.kernels.RBFKernel())```\n",
        "\n",
        "一般会在核函数的输出上添加缩放系数。\n",
        "\n",
        "在核函数的输出上添加缩放系数是为了调整核函数的影响力。  \n",
        "\n",
        "例如，如果我们希望某个核函数的输出对预测结果的贡献更大，我们可以使用较大的缩放系数。相反，如果我们希望某个核函数的输出对预测结果的贡献较小，我们可以使用较小的缩放系数。  \n",
        "通过在核函数的输出上应用kernels.ScaleKernel()，我们可以乘以一个固定的缩放因子，以增加或减小核函数的输出。\n",
        "\n",
        "\n",
        "\n",
        "# exact GP and approximate GP\n",
        "1. Exact inference applies when the closed-form expression of the posterior is available. \n",
        "We can simple and quick to compute the posterior distribution using ```gpytorch.models.ExactGP```.\n",
        "2. Approximate inference applies when the posterior distribution involves high-dimensional integrals.\n",
        "It is difficult and time-consuming to compute.  In such cases we use ```gpytorch.models.ApproximateGP```.\n",
        "\n",
        "### exact GP\n",
        "$$\n",
        "f(x) = -\\cos(\\pi x) + \\sin(4 \\pi x)\n",
        "$$\n"
      ],
      "id": "09c4cf69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def f(x, noise=0):\n",
        "    \"\"\" \n",
        "    objective function\n",
        "    \"\"\"\n",
        "    return -torch.cos(np.pi * x) + torch.sin(4 * np.pi * x) + noise * torch.randn(*x.shape)"
      ],
      "id": "78004225",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# observation noise\n",
        "noise = 0.1\n",
        "# number of observations\n",
        "N =10\n",
        "# initial observations upon initiation 生成一个等间距的函数调用\n",
        "X_init = torch.linspace(0.05,0.95,N) \n",
        "y_init = f(X_init, noise)\n",
        "print(X_init)\n",
        "print(y_init)\n",
        "# plot noisy observations\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(X_init.numpy(), y_init.numpy(), 'kx', mew=2)"
      ],
      "id": "19746f81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPRegressor\n",
        "\n",
        "概率分布和边缘分布的区别：\n",
        "\n",
        "1. 概率分布 $p(f | x)$：这是指给定输入变量 $x$ 的情况下，目标变量 $f$ 的概率分布。在监督学习中，我们通常使用概率模型来建模输入与输出之间的关系。$p(f | x)$ 描述了模型对于给定输入 $x$ 的输出 $f$ 的不确定性。常见的例子是高斯过程模型，其中 $p(f | x)$ 是一个高斯分布。\n",
        "2. 边缘分布 $p(y | x)$：这是指给定输入变量 $x$ 的情况下，目标变量 $y$ 的概率分布。边缘分布是通过对概率分布 $p(f | x)$ 进行积分或求和得到的，其中 $y$ 是通过对 $f$ 进行某种函数变换得到的。在监督学习中，$y$ 通常是观测到的目标变量，而 $f$ 是模型对于给定输入 $x$ 的预测值。\n"
      ],
      "id": "07835b36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gpytorch\n",
        "\n",
        "class GPRegressor(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_inputs, train_targets, mean, kernel, likelihood=None):\n",
        "        if likelihood is None:\n",
        "            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "        # initiate the superclass ExactGP to refresh the posterior \n",
        "        super().__init__(train_inputs, train_targets, likelihood)\n",
        "        # store attributes\n",
        "        self.mean = mean\n",
        "        self.kernel = kernel\n",
        "        self.likelihood = likelihood\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Return:\n",
        "            a posterior multivariate normal distribution\n",
        "        \"\"\"\n",
        "        # mean and kernel are stored as attributes\n",
        "        mean_x = self.mean(x)\n",
        "        covar_x = self.kernel(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \"\"\"\n",
        "        compute the marginal predictive distribution of y given x\n",
        "        \"\"\"\n",
        "        # set the model to evaluation mode\n",
        "        self.eval()\n",
        "        # perform inference without gradient propagation\n",
        "        with torch.no_grad():  # 在预测阶段，不需要计算梯度，因为只有前向传播\n",
        "            # get posterior distribution p(f|x)\n",
        "            pred = self(x)\n",
        "            # convert posterior distribution p(f|x) to p(y|x)\n",
        "            return self.likelihood(pred)"
      ],
      "id": "67e4b5f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_model(model, xlim = None):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    X_train = model.train_inputs[0].cpu().numpy()\n",
        "    y_train = model.train_targets.cpu().numpy()\n",
        "    print(X_train)\n",
        "    print(y_train)\n",
        "\n",
        "    # obtain range of x axis\n",
        "    if xlim is None:\n",
        "        xmin = float(X_train.min())\n",
        "        xmax = float(X_train.max())\n",
        "        x_range = xmax - xmin\n",
        "        xlim = [xmin - 0.05 * x_range, xmax + 0.05 * x_range]\n",
        "\n",
        "\n",
        "    model_tensor_example = list(model.parameters())[0]  \n",
        "    print(model_tensor_example)\n",
        "    # The .to() method is used to specify the target device.\n",
        "    # .to(model_tensor_example)将张量转换为与 model_tensor_example 张量相同的设备上\n",
        "    X_plot = torch.linspace(xlim[0],xlim[1], 200).to(model_tensor_example)\n",
        "\n",
        "    # generate predictive posterior distribution\n",
        "    model.eval()\n",
        "    predictive_distribution = model.predict(X_plot)\n",
        "    # obtain mean, upper and lower bounds   \n",
        "    lower, upper = predictive_distribution.confidence_region()\n",
        "    prediction = predictive_distribution.mean.cpu().numpy()\n",
        "    X_plot = X_plot.numpy()\n",
        "    plt.scatter(X_train, y_train, marker='x', c='k')\n",
        "    plt.plot(X_plot, prediction)\n",
        "    plt.fill_between(X_plot, lower, upper, alpha=0.1)\n",
        "    plt.xlabel('x', fontsize=14)\n",
        "    plt.ylabel('y', fontsize=14)\n",
        "\n",
        "\n",
        "mean_fn = gpytorch.means.ConstantMean()\n",
        "kernel_fn = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "model = GPRegressor(X_init, y_init, mean_fn, kernel_fn)\n",
        "plot_model(model)"
      ],
      "id": "c8ba66cc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nn3.10",
      "language": "python",
      "display_name": "nn3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}