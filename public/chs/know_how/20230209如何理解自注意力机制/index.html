<!doctype html><html lang=chs itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon href=../../../favicon.svg><title>2023-02-09 如何理解自注意力机制 - 徐慧志的个人博客</title><meta name=description content="Attention is all you need"><meta name=author content="Huizhi"><meta name=generator content="Hugo 0.152.2"><link rel=stylesheet href="/css/styles.min.cc1204abf55b2794a944c9970dcfbbedd8cddb0c0451f7d9b088371efe0b6248.css" integrity crossorigin=anonymous><meta property="og:url" content="https://huizhixu.github.io/chs/know_how/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"><meta property="og:site_name" content="徐慧志的个人博客"><meta property="og:title" content="2023-02-09 如何理解自注意力机制"><meta property="og:description" content="Attention is all you need"><meta property="og:locale" content="chs"><meta property="og:type" content="article"><meta property="article:section" content="know_how"><meta property="article:published_time" content="2023-02-09T08:31:50+08:00"><meta property="article:modified_time" content="2023-03-30T00:00:00+00:00"><meta property="article:tag" content="Tech"><meta property="article:tag" content="Ai"><meta name=twitter:card content="summary"><meta name=twitter:title content="2023-02-09 如何理解自注意力机制"><meta name=twitter:description content="Attention is all you need"><meta itemprop=name content="2023-02-09 如何理解自注意力机制"><meta itemprop=description content="Attention is all you need"><meta itemprop=datePublished content="2023-02-09T08:31:50+08:00"><meta itemprop=dateModified content="2023-03-30T00:00:00+00:00"><meta itemprop=wordCount content="2348"><meta itemprop=keywords content="Tech,Ai"><meta name=lang content="chs"></head><body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><header class="container flex justify-between md:justify-between gap-4 flex-wrap p-6 mx-auto relative"><a href=https://huizhixu.github.io/chs/ class="capitalize font-extrabold text-2xl"><img src=../../../blist-logo.png alt=徐慧志的个人博客 class="h-8 max-w-full">
</a><button class="mobile-menu-button md:hidden">
<svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="4" y1="8" x2="20" y2="8"/><line x1="4" y1="16" x2="20" y2="16"/></svg></button><ul class="mobile-menu absolute z-10 px-6 pb-6 md:p-0 top-full left-0 w-full md:w-auto md:relative hidden md:flex flex-col md:flex-row items-end md:items-center gap-4 lg:gap-6 bg-white dark:bg-gray-800"><li><a href=../../../chs/know_how/>技术</a></li><li><a href=../../../chs/life/>生活见闻</a></li><li><a href=../../../chs/page/about/>关于</a></li><li><a href=../../../chs/link/>宝藏集结</a></li><li><a href=../../../chs/tags/>分类</a></li><li class="relative cursor-pointer"><span class="language-switcher flex items-center gap-2"><svg width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><line x1="3.6" y1="9" x2="20.4" y2="9"/><line x1="3.6" y1="15" x2="20.4" y2="15"/><path d="M11.5 3a17 17 0 000 18"/><path d="M12.5 3a17 17 0 010 18"/></svg>
<a>语言</a>
<svg width="14" height="14" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12" transform="rotate(180 12 12)"/></svg></span><div class="language-dropdown absolute top-full mt-2 left-0 flex-col gap-2 bg-gray-100 dark:bg-gray-900 dark:text-white z-10 hidden"><a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href=../../../en/ lang=en>English</a>
<a class="px-3 py-2 hover:bg-gray-200 dark:hover:bg-gray-700" href=../../../de/ lang=de>Deutsch</a></div></li><li class="grid place-items-center"><span class="open-search inline-block cursor-pointer"><svg width="20" height="20" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></span></li><li class="grid place-items-center"><span class="toggle-dark-mode inline-block cursor-pointer"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="3"/><line x1="12" y1="5" x2="12" y2="5.01"/><line x1="17" y1="7" x2="17" y2="7.01"/><line x1="19" y1="12" x2="19" y2="12.01"/><line x1="17" y1="17" x2="17" y2="17.01"/><line x1="12" y1="19" x2="12" y2="19.01"/><line x1="7" y1="17" x2="7" y2="17.01"/><line x1="5" y1="12" x2="5" y2="12.01"/><line x1="7" y1="7" x2="7" y2="7.01"/></svg></span></li></ul></header><main class=flex-1><article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4"><h1 class="text-2xl font-bold mb-2">2023-02-09 如何理解自注意力机制</h1><h5 class="text-sm flex items-center flex-wrap"><svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg>
发布于
2023年02月09日
&nbsp(最近编辑
2023年03月30日
)</h5><h5 class="text-sm flex items-center flex-wrap"><svg class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
5&nbsp;分钟
&nbsp;&bull;
<svg class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 19a9 9 0 019 0 9 9 0 019 0"/><path d="M3 6a9 9 0 019 0 9 9 0 019 0"/><line x1="3" y1="6" x2="3" y2="19"/><line x1="12" y1="6" x2="12" y2="19"/><line x1="21" y1="6" x2="21" y2="19"/></svg>
2348&nbsp;字</h5><details id=TableOfContents class="px-4 mt-4 bg-gray-100 dark:bg-gray-700 rounded toc"><summary class="flex items-center font-bold py-2 px-4 cursor-pointer justify-between select-none text-black dark:text-white"><span>Table of contents</span>
<svg class="icon icon-tabler icon-tabler-chevron-down" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="6 9 12 15 18 9"/></svg></summary><ul class="mt-2 pb-4"><li><a href=#%e7%90%86%e8%a7%a3%e8%be%93%e5%85%a5%e4%b8%8e%e8%be%93%e5%87%ba>理解输入与输出</a></li><li><a href=#%e4%b8%80%e4%b8%aavector%e5%af%b9%e5%ba%94%e4%b8%80%e4%b8%aalabel%e7%9a%84%e6%83%85%e5%86%b5%e5%8d%b3%e8%be%93%e5%85%a5%e5%92%8c%e8%be%93%e5%87%ba%e4%b8%80%e6%a0%b7%e5%a4%9a%e4%b9%9f%e5%8f%ab%e5%81%9asequence-labeling>一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling</a></li><li><a href=#self-attention>Self-attention</a><ul><li><a href=#%e8%bf%90%e4%bd%9c%e5%8e%9f%e7%90%86>运作原理：</a></li></ul></li><li><a href=#qkv%e4%bb%8e%e7%9f%a9%e9%98%b5%e7%9a%84%e8%a7%92%e5%ba%a6%e7%9c%8bself-attention>$QKV$（从矩阵的角度看self-attention）</a></li><li><a href=#%e6%80%8e%e4%b9%88%e5%be%97%e5%88%b0wqwk%e5%92%8cwv>怎么得到$W^q$、$W^k$和$W^v$</a></li><li><a href=#%e8%87%aa%e6%b3%a8%e6%84%8f%e6%a8%a1%e5%9d%97%e8%ae%a1%e7%ae%97%e4%b8%a4%e4%b8%aa%e5%90%91%e9%87%8f%e7%9a%84%e5%85%b3%e8%81%94%e6%80%a7>自注意模块计算两个向量的关联性</a><ul><li><a href=#%e5%86%85%e7%a7%af%e6%96%b9%e6%b3%95><strong>内积方法</strong></a></li><li><a href=#%e7%9b%b8%e5%8a%a0%e6%96%b9%e6%b3%95><strong>相加方法</strong></a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83>参考：</a></li></ul></details><h2 id=理解输入与输出>理解输入与输出</h2><ul><li>输入有可能是一个 vector，有可能是多个 vector</li><li>输出：<ul><li>一个序列对应一个 label。the whole sequence has a label<ul><li>例子：在情感分析里面，This is good 对应的输入是多个 vector，输出为 positive，是一个vector。</li></ul></li><li>一个 vector 对应一个 label。一个序列对应多个 label。<ul><li>例子：在词性标注里面，This is good 对应的输入是多个 vector，输出为 代词，动词，形容词。</li></ul></li><li>模型决定 label 的个数。seq2seq 任务<ul><li>例子：在机器翻译里面，This is good 对应的输入是3个 vector，中文翻译是”不错“，输出为2个 vector。</li></ul></li></ul></li></ul><h2 id=一个vector对应一个label的情况即输入和输出一样多也叫做sequence-labeling>一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling</h2><ul><li>例子： I saw a saw</li><li>如何解决 sequence labeling 的问题：用 fully connected network 对每一个 input vector 进行作用</li><li>弊端：<ul><li>用 fully connected network 来输出，假设对 I saw a saw 做词性标注。对于 FC 层来说，两个 saw没有什么不同，但是他们实际上一个是动词，一个是名词。</li></ul></li><li>解决思路：考虑更多的上下文。每一个 fc 层，都对所有的输入作用。或者给他一个 window，作用于相邻的几个 input vector。但是作用还是有限，计算也很复杂。</li></ul><p>我们想考虑整个 sequence，但是不想把 sequence 所有的数据都包括在里面，就有了 self-attention。</p><h2 id=self-attention>Self-attention</h2><p>如果要考虑上下文的信息，输入可以扔进 self-attention 之后，生成 vector with context，然后丢进FC层，然后再过一次 self-attention，然后再丢进 FC 层。</p><p>可以这样理解，self-attention 获得上下文信息，FC 层专注于局部信息，交替使用。</p><h3 id=运作原理>运作原理：</h3><ul><li>输入：是一串 vector，可以是整个 network 的输入，也可以是 FC 层的输出，这里用 $a$ 表示。</li><li>输出：一串 vector，这里用 $b$ 表示。每一个 $b$ 都对所有 $a$ 作用。（这个图乍一看很像上面的 FC，但其实不一样）</li></ul><p><img src=../../../img/20230209/0.png alt=0></p><p>问题就变成了，输入是 $a$ 时，如何计算 $b$？用 $b^1$来说明，具体分三步。</p><ul><li>如何产生 $b^1$ 这个向量：<ul><li><p>第一步：找出 sequence 里面跟 $a^1$ 相关的其他向量。（FC 层会把 sequence 所有的向量都包括进来，这里只包括相关的向量，这就是不同点。）这个机制叫做自注意机制。</p><ul><li>每一个向量跟 $a^1$ 相关联的程度，我们用数值 $\alpha$ 来表示。</li><li>Self-attention 的 module 怎么决定两个向量的关联性呢？<ul><li>内积：dot product （方法原理在文章最后面）</li><li>相加：additive（方法原理在文章最后面）</li></ul></li><li>这里的做法是：分别计算 $a^1$ 与 $a^2$， $a^3$，$a^4$ 的关联性</li><li>把 $a^1$ 乘以 $W^q$，得到 $q^1$ ，$q^1$ 就是 query——搜寻。</li><li>把 $a^2$ 乘以 $W^k$，得到 $k^2$，$k^2$ 就是 key——键。把 $a^3$ 乘以 $W^k$，得到 $k^3$，把 $a^4$ 乘以 $W^k$，得到 $k^4$。</li><li>那么 $\alpha_{1,2} =q^1\cdot k^2$，关联性就算出来了。$\alpha_{1,2}$ 也叫 attention score。</li><li>同理可以算出 $\alpha_{1,3}，$$\alpha_{1,4}$。</li><li>一般我们也会算 $q^1$ 和 $k^1$ 的关联性，也就是 $\alpha_{1,1}$。</li></ul><p><img src=../../../img/20230209/1.png alt=1></p></li><li><p>第二步：对 $\alpha_{1,1}$，$\alpha_{1,2}$，$\alpha_{1,3}$，$\alpha_{1,4}$进行Softmax 操作，得到$\alpha^\prime$。</p><ul><li>为什么要用 Softmax：Softmax 最常见，这里也可以用其他函数，可以用 relu 等。</li></ul></li><li><p>第三步：根据$\alpha^\prime$，在 sequence 里面抽取重要的信息。</p><ul><li>做法：将 $a^1$，$a^2$，$a^3$，$a^4$ 分别乘以 $W^v$，得到 $v^1，$$v^2，$$v^3$，$v^4$。 （其实这里$v^1，v^2，v^3，v^4$就是$a^1，a^2，a^3，a^4$自己本身的等价表示）</li><li>将 $v^1$ 到 $v^4$ 都乘以分别的 $\alpha^\prime$， 然后再相加。</li></ul><p>$$
b^1 = \sum_i \alpha\prime_{1,i} v^i
$$</p><ul><li><p>如果 $a^1$ 和 $a^2$ 的关联性很强，$\alpha^\prime_{1,2}$ 的值很大，那么经过 weighted sum 得到的 $b^1$ 的值就可能会接近 $v^2$。</p><p>也就是说，谁的 attention score 越大，谁的 v 就会 dominate 抽出来的结果。</p></li></ul><p><img src=../../../img/20230209/2.png alt=2></p></li></ul></li></ul><h2 id=qkv从矩阵的角度看self-attention>$QKV$（从矩阵的角度看self-attention）</h2><ol><li>把 $a^1$ 到 $a^4$ 看成是矩阵 $I$ 的列，$q^1$ 到 $q^4$ 是矩阵 $Q$ 的列，那么可以转化成矩阵间的运算。</li></ol><p>矩阵$I$乘以$Wq$，得到矩阵$Q$，$Q$的4列就是$q^1$到$q^4$。</p><p>同样，输入矩阵I乘以$W^k$，得到矩阵$K$，$K$的4列就是$k^1$到$k^4$。</p><p>输入$I$乘上三个不同的矩阵，就得到了$QKV$。</p><p><img src=../../../img/20230209/3.png alt=3></p><ol><li><p>每一个 $q$ 会和每一个 $k$ 计算内积，得到 attention score。</p><ol><li><p>矩阵和向量相乘：</p><ol><li>矩阵 $K$ 和 $q^1$ 内积，得到 $a^1$ 和其他输入的相关性。</li><li>矩阵 $K$ 和 $q^2$ 内积，得到 $a^2$ 和其他输入的相关性。</li><li>合起来，就是</li></ol><p>$$
\begin{array}{cc}<br>\alpha_{1,1} & \alpha_{2,1} & \alpha_{3,1} & \alpha_{4,1} \
\alpha_{1,2} & \alpha_{2,2} & \alpha_{3,2} & \alpha_{4,2} \
\alpha_{1,3} & \alpha_{2,3} & \alpha_{3,3} & \alpha_{4,3}\
\alpha_{1,4} & \alpha_{2,4} & \alpha_{3,4} & \alpha_{4,4}
\end{array} =</p><p>\begin{array}{cc}
k^1 \
k^2 \
k^3 \
k^4 \<br>\end{array}</p><p>\begin{array}{cc}
q^1 &
q^2 &
q^3 &
q^4 &
\end{array}
$$</p><p>简写为</p><p>$$
A = K^T \cdot Q
$$</p><p><img src=../../../img/20230209/4.png alt=4></p></li></ol></li><li><p>对 $A$ 做Softmax，得到 $A^\prime$</p></li><li><p>$V$乘以$A^\prime$得到B</p><ul><li>$v^1$乘上 $\alpha^\prime_{1,1}$， 加上$v^2$乘上 $\alpha^\prime_{1,2}$，等等，得到 $b^1$</li><li>$v^1$乘上 $\alpha^\prime_{1,1}$， 加上$v^2$乘上 $\alpha^\prime_{1,2}$，等等，得到 $b^2$</li><li>O 就是 Output</li></ul></li></ol><p><img src=../../../img/20230209/5.png alt=5></p><p>总结：先产生QKV，根据Q找出相关的位置，再对V做weighted sum。</p><p><img src=../../../img/20230209/6.png alt=6></p><h2 id=怎么得到wqwk和wv>怎么得到$W^q$、$W^k$和$W^v$</h2><p>初始化一个矩阵，通过training data 在训练过程中更新。</p><h2 id=自注意模块计算两个向量的关联性>自注意模块计算两个向量的关联性</h2><h3 id=内积方法><strong>内积方法</strong></h3><p>左边的向量乘以 $W^q$ 矩阵，右边的向量乘以 $W^k$ 矩阵，得到 $q$ 和 $k$ 这两个向量。然后做内积，得到一个标量。</p><p>$$
\alpha = q \cdot k
$$</p><h3 id=相加方法><strong>相加方法</strong></h3><p>左边的向量乘以 $W^q$ 矩阵，右边的向量乘以 $W^k$ 矩阵，得到 $q$ 和 $k$ 这两个向量。然后相加丢到tanh，再乘以 $W$，得到 $\alpha$。</p><p><img src=../../../img/20230209/7.png alt=7></p><h2 id=参考>参考：</h2><p>李宏毅老师的讲课视频</p><p><a href="https://www.bilibili.com/video/BV1v3411r78R?p=2&amp;vd_source=bbd38c44460fafa204a3540d4f8a2657" target=_blank rel=noopener>11.【李宏毅机器学习2021】自注意力机制 (Self-attention) (下)_哔哩哔哩_bilibili</a></p></article><div class="px-2 mb-2"><script src=https://utteranc.es/client.js repo=HuizhiXu/huizhixu.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div><div class="bg-blue-100 dark:bg-gray-900"><div class="container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center"><div><div class="text-2xl font-bold mb-2">Sein heißt werden, leben heißt lernen.</div><p class=opacity-60>Der einfache Weg is immer verkehrt.</p></div><ul class="flex justify-center gap-x-3 flex-wrap gap-y-2"><li><a href=https://twitter.com/ target=_blank rel=noopener aria-label=Twitter class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://github.com/ target=_blank rel=noopener aria-label=GitHub class="p-1 inline-block rounded-full border border-transparent text-gray-500 hover:text-gray-800 hover:border-gray-800 cursor-pointer transition-colors dark:text-gray-600 dark:hover:border-gray-300 dark:hover:text-gray-300"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ul></div></div></main><footer class="container p-6 mx-auto flex justify-between items-center"><span class="text-sm font-light">Copyright © 2012 - Huizhi Xu · All rights reserved
</span><span onclick='window.scrollTo({top:0,behavior:"smooth"})' class="p-1 cursor-pointer"><svg width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M18 15l-6-6-6 6h12"/></svg></span></footer><div class="search-ui absolute top-0 left-0 w-full h-full bg-white dark:bg-gray-800 hidden"><div class="container max-w-3xl mx-auto p-12"><div class=relative><div class="my-4 text-center text-2xl font-bold">Search</div><span class="p-2 absolute right-0 top-0 cursor-pointer close-search"><svg width="24" height="24" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></span></div><input type=search class="py-2 px-3 w-full dark:text-black border dark:border-transparent" placeholder="Enter search query"><div class="search-results text-lg font-medium my-4 hidden">Results</div><ul class="search-list my-2"></ul><div class="no-results text-center my-8 hidden"><div class="text-xl font-semibold mb-2">No results found</div><p class="font-light text-sm">Try adjusting your search query</p></div></div></div><script src=https://huizhixu.github.io/js/scripts.min.js></script><script>const languageMenuButton=document.querySelector(".language-switcher"),languageDropdown=document.querySelector(".language-dropdown");languageMenuButton.addEventListener("click",e=>{e.preventDefault(),languageDropdown.classList.contains("hidden")?(languageDropdown.classList.remove("hidden"),languageDropdown.classList.add("flex")):(languageDropdown.classList.add("hidden"),languageDropdown.classList.remove("flex"))})</script><script>const darkmode=document.querySelector(".toggle-dark-mode");function toggleDarkMode(){document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("darkmode","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("darkmode","dark"))}darkmode&&darkmode.addEventListener("click",toggleDarkMode);const darkStorage=localStorage.getItem("darkmode"),isBrowserDark=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches;!darkStorage&&isBrowserDark&&document.documentElement.classList.add("dark"),darkStorage&&darkStorage==="dark"&&toggleDarkMode()</script><script>const mobileMenuButton=document.querySelector(".mobile-menu-button"),mobileMenu=document.querySelector(".mobile-menu");function toggleMenu(){mobileMenu.classList.toggle("hidden"),mobileMenu.classList.toggle("flex")}mobileMenu&&mobileMenuButton&&mobileMenuButton.addEventListener("click",toggleMenu)</script></body></html>