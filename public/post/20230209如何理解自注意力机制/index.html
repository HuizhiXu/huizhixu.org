<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage"><head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <link rel="icon" href="/favicon.ico">

  <title>
  2023-02-09 如何理解自注意力机制 - HuizhiXu 的个人博客
  </title>
  <meta name="description" content="Attention is all you need" /><meta name="generator" content="Hugo 0.115.4"><link
    rel="stylesheet"
    href="https://huizhixu.github.io/css/styles.min.a8628a7949d76d4d5a8696640e2a604a1f7c9b8690f3511812a99462f2193a69.css"
    integrity="sha256-qGKKeUnXbU1ahpZkDipgSh98m4aQ81EYEqmUYvIZOmk="
  />
  
  

  <meta property="og:title" content="2023-02-09 如何理解自注意力机制" />
<meta property="og:description" content="Attention is all you need" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://huizhixu.github.io/post/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-02-09T08:31:50+08:00" />
<meta property="article:modified_time" content="2023-02-09T08:31:50+08:00" />

  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="2023-02-09 如何理解自注意力机制"/>
<meta name="twitter:description" content="Attention is all you need"/>

  <meta itemprop="name" content="2023-02-09 如何理解自注意力机制">
<meta itemprop="description" content="Attention is all you need"><meta itemprop="datePublished" content="2023-02-09T08:31:50+08:00" />
<meta itemprop="dateModified" content="2023-02-09T08:31:50+08:00" />
<meta itemprop="wordCount" content="399">
<meta itemprop="keywords" content="" />

  
</head>
<body class="dark:bg-gray-800 dark:text-white relative flex flex-col min-h-screen"><header class="container flex justify-between md:justify-between gap-4 flex-wrap p-6 mx-auto relative">
  <a href="https://huizhixu.github.io/" class="capitalize font-extrabold text-2xl">
    
    HuizhiXu 的个人博客
    
  </a>
  <button class="mobile-menu-button md:hidden">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <line x1="4" y1="8" x2="20" y2="8" />
      <line x1="4" y1="16" x2="20" y2="16" />
    </svg>
  </button>
  <ul class="mobile-menu absolute z-10 px-6 pb-6 md:p-0 top-full left-0 w-full md:w-auto md:relative hidden md:flex flex-col md:flex-row items-end md:items-center gap-4 lg:gap-6 bg-white dark:bg-gray-800">

    

    

    

    
  </ul>
</header>
<main class="flex-1">
  
  

  

  <article class="prose lg:prose-lg mx-auto my-8 dark:prose-dark px-4">

    <h1 class="text-2xl font-bold mb-2">2023-02-09 如何理解自注意力机制</h1>
    
    <h5 class="text-sm flex items-center flex-wrap">
      <svg xmlns="http://www.w3.org/2000/svg" class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <rect x="4" y="5" width="16" height="16" rx="2" />
        <line x1="16" y1="3" x2="16" y2="7" />
        <line x1="8" y1="3" x2="8" y2="7" />
        <line x1="4" y1="11" x2="20" y2="11" />
        <rect x="8" y="15" width="2" height="2" />
      </svg>
      Posted on 
  
    February 9, 2023
  


      
        &nbsp;&bull;&nbsp;
      
      <svg xmlns="http://www.w3.org/2000/svg" class="mr-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <circle cx="12" cy="12" r="9" />
        <polyline points="12 7 12 12 15 15" />
      </svg>
      2&nbsp;minutes
      &nbsp;&bull;
      <svg xmlns="http://www.w3.org/2000/svg" class="mx-1" width="16" height="16" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
        <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
        <path d="M3 19a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
        <path d="M3 6a9 9 0 0 1 9 0a9 9 0 0 1 9 0" />
        <line x1="3" y1="6" x2="3" y2="19" />
        <line x1="12" y1="6" x2="12" y2="19" />
        <line x1="21" y1="6" x2="21" y2="19" />
      </svg>
      399&nbsp;words
      
        
      
    </h5>
    

    

    <h1 id="如何理解自注意力机制">如何理解自注意力机制</h1>
<h2 id="理解输入与输出">理解输入与输出</h2>
<ul>
<li>输入有可能是一个 vector，有可能是多个 vector</li>
<li>输出：
<ul>
<li>一个序列对应一个 label。the whole sequence has a label
<ul>
<li>例子：在情感分析里面，This is good 对应的输入是多个 vector，输出为 positive，是一个vector。</li>
</ul>
</li>
<li>一个 vector 对应一个 label。一个序列对应多个 label。
<ul>
<li>例子：在词性标注里面，This is good 对应的输入是多个 vector，输出为 代词，动词，形容词。</li>
</ul>
</li>
<li>模型决定 label 的个数。seq2seq 任务
<ul>
<li>例子：在机器翻译里面，This is good 对应的输入是3个 vector，中文翻译是”不错“，输出为2个 vector。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="一个vector对应一个label的情况即输入和输出一样多也叫做sequence-labeling">一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling</h2>
<ul>
<li>例子： I saw a saw</li>
<li>如何解决 sequence labeling 的问题：用 fully connected network 对每一个 input vector 进行作用</li>
<li>弊端：
<ul>
<li>用 fully connected network 来输出，假设对 I saw a saw 做词性标注。对于 FC 层来说，两个 saw没有什么不同，但是他们实际上一个是动词，一个是名词。</li>
</ul>
</li>
<li>解决思路：考虑更多的上下文。每一个 fc 层，都对所有的输入作用。或者给他一个 window，作用于相邻的几个 input vector。但是作用还是有限，计算也很复杂。</li>
</ul>
<p>我们想考虑整个 sequence，但是不想把 sequence 所有的数据都包括在里面，就有了 self-attention。</p>
<h2 id="self-attention">Self-attention</h2>
<p>如果要考虑上下文的信息，输入可以扔进 self-attention 之后，生成 vector with context，然后丢进FC层，然后再过一次 self-attention，然后再丢进 FC 层。</p>
<p>可以这样理解，self-attention 获得上下文信息，FC 层专注于局部信息，交替使用。</p>
<h3 id="运作原理">运作原理：</h3>
<ul>
<li>输入：是一串 vector，可以是整个 network 的输入，也可以是 FC 层的输出，这里用 $a$ 表示。</li>
<li>输出：一串 vector，这里用  $b$ 表示。每一个 $b$  都对所有 $a$ 作用。（这个图乍一看很像上面的 FC，但其实不一样）</li>
</ul>
<p><img src="/img/20230209/0.png" alt="0"></p>
<p>问题就变成了，输入是 $a$ 时，如何计算 $b$？用 $b^1$来说明，具体分三步。</p>
<ul>
<li>如何产生 $b^1$ 这个向量：
<ul>
<li>
<p>第一步：找出 sequence 里面跟 $a^1$ 相关的其他向量。（FC 层会把 sequence 所有的向量都包括进来，这里只包括相关的向量，这就是不同点。）这个机制叫做自注意机制。</p>
<ul>
<li>每一个向量跟 $a^1$ 相关联的程度，我们用数值 $\alpha$ 来表示。</li>
<li>Self-attention 的 module 怎么决定两个向量的关联性呢？
<ul>
<li>内积：dot product （方法原理在文章最后面）</li>
<li>相加：additive（方法原理在文章最后面）</li>
</ul>
</li>
<li>这里的做法是：分别计算 $a^1$ 与 $a^2$， $a^3$，$a^4$ 的关联性</li>
<li>把 $a^1$ 乘以 $W^q$，得到 $q^1$ ，$q^1$ 就是 query——搜寻。</li>
<li>把 $a^2$ 乘以 $W^k$，得到 $k^2$，$k^2$ 就是 key——键。把 $a^3$ 乘以 $W^k$，得到 $k^3$，把 $a^4$ 乘以 $W^k$，得到 $k^4$。</li>
<li>那么 $\alpha_{1,2} =q^1\cdot k^2$，关联性就算出来了。$\alpha_{1,2}$ 也叫 attention score。</li>
<li>同理可以算出 $\alpha_{1,3}，$$\alpha_{1,4}$。</li>
<li>一般我们也会算 $q^1$ 和 $k^1$ 的关联性，也就是 $\alpha_{1,1}$。</li>
</ul>
<p><img src="/img/20230209/1.png" alt="1"></p>
</li>
<li>
<p>第二步：对 $\alpha_{1,1}$，$\alpha_{1,2}$，$\alpha_{1,3}$，$\alpha_{1,4}$进行Softmax 操作，得到$\alpha^\prime$。</p>
<ul>
<li>为什么要用 Softmax：Softmax 最常见，这里也可以用其他函数，可以用 relu 等。</li>
</ul>
</li>
<li>
<p>第三步：根据$\alpha^\prime$，在 sequence 里面抽取重要的信息。</p>
<ul>
<li>做法：将 $a^1$，$a^2$，$a^3$，$a^4$ 分别乘以 $W^v$，得到 $v^1，$$v^2，$$v^3$，$v^4$。  （其实这里$v^1，v^2，v^3，v^4$就是$a^1，a^2，a^3，a^4$自己本身的等价表示）</li>
<li>将 $v^1$ 到 $v^4$ 都乘以分别的 $\alpha^\prime$， 然后再相加。</li>
</ul>
<p>$$
b^1  = \sum_i \alpha\prime_{1,i} v^i
$$</p>
<ul>
<li>
<p>如果 $a^1$ 和 $a^2$ 的关联性很强，$\alpha^\prime_{1,2}$ 的值很大，那么经过 weighted sum 得到的 $b^1$ 的值就可能会接近 $v^2$。</p>
<p>也就是说，谁的 attention score 越大，谁的 v 就会 dominate 抽出来的结果。</p>
</li>
</ul>
<p><img src="/img/20230209/2.png" alt="2"></p>
</li>
</ul>
</li>
</ul>
<h2 id="qkv从矩阵的角度看self-attention">$QKV$（从矩阵的角度看self-attention）</h2>
<ol>
<li>把 $a^1$ 到 $a^4$ 看成是矩阵 $I$ 的列，$q^1$ 到 $q^4$ 是矩阵 $Q$ 的列，那么可以转化成矩阵间的运算。</li>
</ol>
<p>矩阵$I$乘以$Wq$，得到矩阵$Q$，$Q$的4列就是$q^1$到$q^4$。</p>
<p>同样，输入矩阵I乘以$W^k$，得到矩阵$K$，$K$的4列就是$k^1$到$k^4$。</p>
<p>输入$I$乘上三个不同的矩阵，就得到了$QKV$。</p>
<p><img src="/img/20230209/3.png" alt="3"></p>
<ol>
<li>
<p>每一个 $q$ 会和每一个 $k$ 计算内积，得到 attention score。</p>
<ol>
<li>
<p>矩阵和向量相乘：</p>
<ol>
<li>矩阵 $K$ 和 $q^1$ 内积，得到 $a^1$ 和其他输入的相关性。</li>
<li>矩阵 $K$ 和 $q^2$ 内积，得到 $a^2$ 和其他输入的相关性。</li>
<li>合起来，就是</li>
</ol>
<p>$$
\begin{array}{cc}<br>
\alpha_{1,1} &amp; \alpha_{2,1} &amp; \alpha_{3,1} &amp; \alpha_{4,1} \
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{3,2} &amp; \alpha_{4,2} \
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp; \alpha_{4,3}\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp; \alpha_{4,4}
\end{array} =</p>
<p>\begin{array}{cc}
k^1 \
k^2 \
k^3 \
k^4 \<br>
\end{array}</p>
<p>\begin{array}{cc}
q^1 &amp;
q^2 &amp;
q^3 &amp;
q^4 &amp;
\end{array}
$$</p>
<p>简写为</p>
<p>$$
A = K^T \cdot Q
$$</p>
<p><img src="/img/20230209/4.png" alt="4"></p>
</li>
</ol>
</li>
<li>
<p>对 $A$ 做Softmax，得到 $A^\prime$</p>
</li>
<li>
<p>$V$乘以$A^\prime$得到B</p>
<ul>
<li>$v^1$乘上 $\alpha^\prime_{1,1}$， 加上$v^2$乘上 $\alpha^\prime_{1,2}$，等等，得到 $b^1$</li>
<li>$v^1$乘上 $\alpha^\prime_{1,1}$， 加上$v^2$乘上 $\alpha^\prime_{1,2}$，等等，得到 $b^2$</li>
<li>O 就是 Output</li>
</ul>
</li>
</ol>
<p><img src="/img/20230209/5.png" alt="5"></p>
<p>总结：先产生QKV，根据Q找出相关的位置，再对V做weighted sum。</p>
<p><img src="/img/20230209/6.png" alt="6"></p>
<h2 id="怎么得到wqwk和wv">怎么得到$W^q$、$W^k$和$W^v$</h2>
<p>初始化一个矩阵，通过training data 在训练过程中更新。</p>
<h2 id="自注意模块计算两个向量的关联性">自注意模块计算两个向量的关联性</h2>
<h3 id="内积方法"><strong>内积方法</strong></h3>
<p>左边的向量乘以 $W^q$ 矩阵，右边的向量乘以 $W^k$ 矩阵，得到 $q$ 和 $k$ 这两个向量。然后做内积，得到一个标量。</p>
<p>$$
\alpha = q \cdot k
$$</p>
<h3 id="相加方法"><strong>相加方法</strong></h3>
<p>左边的向量乘以 $W^q$ 矩阵，右边的向量乘以 $W^k$ 矩阵，得到 $q$ 和 $k$ 这两个向量。然后相加丢到tanh，再乘以 $W$，得到 $\alpha$。</p>
<p><img src="/img/20230209/7.png" alt="7"></p>
<h2 id="参考">参考：</h2>
<p>李宏毅老师的讲课视频</p>
<p><a href="https://www.bilibili.com/video/BV1v3411r78R?p=2&amp;vd_source=bbd38c44460fafa204a3540d4f8a2657" target="_blank" rel="noopener">11.【李宏毅机器学习2021】自注意力机制 (Self-attention) (下)_哔哩哔哩_bilibili</a>
</p>

  </article><div class="bg-pink-50 dark:bg-gray-900">
  <div class="container px-4 py-12 mx-auto max-w-4xl grid grid-cols-1 md:grid-cols-2 gap-4 items-center">
    <div>
      <div class="text-2xl font-bold mb-2"></div>
      <p class="opacity-60"></p>
    </div>

    <ul class="flex justify-center gap-x-3 flex-wrap gap-y-2">
      
    </ul>
  </div>
</div>

    </main><footer class="container p-6 mx-auto flex justify-between items-center">
  <span class="text-sm font-light">
    
    2023 © HuizhiXu 的个人博客
    
  </span>
  <span onclick="window.scrollTo({top: 0, behavior: 'smooth'})" class="p-1 cursor-pointer">
    <svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 24 24" stroke-width="1.5"
      stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M18 15l-6 -6l-6 6h12" />
    </svg>
  </span>
</footer>









<script>
  const mobileMenuButton = document.querySelector('.mobile-menu-button')
  const mobileMenu = document.querySelector('.mobile-menu')
  function toggleMenu() {
    mobileMenu.classList.toggle('hidden');
    mobileMenu.classList.toggle('flex');
  }
  if(mobileMenu && mobileMenuButton){
    mobileMenuButton.addEventListener('click', toggleMenu)
  }
</script>
</body>
</html>
