<!DOCTYPE html>
<html lang="zh-cn">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <script type="application/javascript" src='https://huizhixu.github.io/js/theme-mode.js'></script>
    <link rel="stylesheet" href='https://huizhixu.github.io/css/frameworks.min.css' />
    <link rel="stylesheet" href='https://huizhixu.github.io/css/github.min.css' />
    <link rel="stylesheet" href='https://huizhixu.github.io/css/github-style.css' />
    <link rel="stylesheet" href='https://huizhixu.github.io/css/light.css' />
    <link rel="stylesheet" href='https://huizhixu.github.io/css/dark.css' />
    <link rel="stylesheet" href='https://huizhixu.github.io/css/syntax.css' />
    <title>2023-02-09 如何理解自注意力机制 - HuizhiXu 的个人博客</title>
    
    <link rel="icon" type="image/x-icon" href='/images/poirot.png'>
    
    <meta name="theme-color" content="#1e2327">

    
    <meta name="description"
  content="Attention is all you need." />
<meta name="keywords"
  content='' />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://huizhixu.github.io/post/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" />


<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="2023-02-09 如何理解自注意力机制 - HuizhiXu 的个人博客" />
<meta name="twitter:description"
  content="Attention is all you need." />
<meta name="twitter:site" content="https://huizhixu.github.io/" />
<meta name="twitter:creator" content="" />
<meta name="twitter:image"
  content="https://huizhixu.github.io/">


<meta property="og:type" content="article" />
<meta property="og:title" content="2023-02-09 如何理解自注意力机制 - HuizhiXu 的个人博客">
<meta property="og:description"
  content="Attention is all you need." />
<meta property="og:url" content="https://huizhixu.github.io/post/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" />
<meta property="og:site_name" content="2023-02-09 如何理解自注意力机制" />
<meta property="og:image"
  content="https://huizhixu.github.io/">
<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">

<meta property="article:published_time" content="2023-02-09 08:31:50 &#43;0800 CST" />










</head>

<body>
  <div style="position: relative">
    <header class="Header js-details-container Details px-3 px-md-4 px-lg-5 flex-wrap flex-md-nowrap open Details--on">
        <div class="Header-item mobile-none" style="margin-top: -4px; margin-bottom: -4px;">
            <a class="Header-link" href="https://huizhixu.github.io/">
                <svg class="octicon" height="32" viewBox="0 0 16 16" version="1.1" width="32">
                    <path fill-rule="evenodd"
                          d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z">
                    </path>
                </svg>
            </a>
        </div>
        <div class="Header-item d-md-none">
            <button class="Header-link btn-link js-details-target" type="button"
                    onclick="document.querySelector('#header-search').style.display = document.querySelector('#header-search').style.display == 'none'? 'block': 'none'">
                <svg height="24" class="octicon octicon-three-bars" viewBox="0 0 16 16" version="1.1" width="24">
                    <path fill-rule="evenodd"
                          d="M1 2.75A.75.75 0 011.75 2h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 2.75zm0 5A.75.75 0 011.75 7h12.5a.75.75 0 110 1.5H1.75A.75.75 0 011 7.75zM1.75 12a.75.75 0 100 1.5h12.5a.75.75 0 100-1.5H1.75z">
                    </path>
                </svg>
            </button>
        </div>
        <div style="display: none;" id="header-search"
             class="Header-item Header-item--full flex-column flex-md-row width-full flex-order-2 flex-md-order-none mr-0 mr-md-3 mt-3 mt-md-0 Details-content--hidden-not-important d-md-flex">
            <div
                    class="Header-search header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to">
                <div class="position-relative">
                    <form target="_blank" action="https://www.google.com/search" accept-charset="UTF-8" method="get"
                          autocomplete="off">
                        <label
                                class="Header-search-label form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center">
                            <input type="text"
                                   class="Header-search-input form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable"
                                   name="q" value="" placeholder="Search" autocomplete="off">
                            <input type="hidden" name="q" value="site:https://huizhixu.github.io/">
                        </label>
                    </form>
                </div>
            </div>
        </div>

        <div class="Header-item Header-item--full flex-justify-center d-md-none position-relative">
            <a class="Header-link " href="https://huizhixu.github.io/">
                <svg class="octicon octicon-mark-github v-align-middle" height="32" viewBox="0 0 16 16" version="1.1"
                     width="32">
                    <path fill-rule="evenodd"
                          d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z">
                    </path>
                </svg>
            </a>
        </div>
        <div class="Header-item" style="margin-right: 0;">
            <a href="javascript:void(0)" class="Header-link no-select" onclick="switchTheme()">
                <svg style="fill: var(--color-profile-color-modes-toggle-moon);" class="no-select" viewBox="0 0 16 16"
                     version="1.1" width="16" height="16">
                    <path fill-rule="evenodd" clip-rule="evenodd"
                          d="M4.52208 7.71754C7.5782 7.71754 10.0557 5.24006 10.0557 2.18394C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961C9.95801 1.07727 10.3495 0.771159 10.6474 0.99992C12.1153 2.12716 13.0615 3.89999 13.0615 5.89383C13.0615 9.29958 10.3006 12.0605 6.89485 12.0605C3.95334 12.0605 1.49286 10.001 0.876728 7.24527C0.794841 6.87902 1.23668 6.65289 1.55321 6.85451C2.41106 7.40095 3.4296 7.71754 4.52208 7.71754Z">
                    </path>
                </svg>
            </a>
        </div>
    </header>
</div>

<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
  
<div>
  <main>
    <div class="gisthead pagehead bg-gray-light pb-0 pt-3 mb-4">
      <div class="px-0">
        <div class="mb-3 d-flex px-3 px-md-3 px-lg-5">
          <div class="flex-auto min-width-0 width-fit mr-3">
            <div class="d-flex">
              <div class="d-none d-md-block">
                <a class="avatar mr-2 flex-shrink-0" href="https://huizhixu.github.io/">
                  <img class=" avatar-user"
                    src="https://huizhixu.github.io/images/poirot.png"
                    width="32" height="32"></a>
              </div>
              <div class="d-flex flex-column">
                <h1 class="break-word f3 text-normal mb-md-0 mb-1">
                  <span class="author">
                    <a href="https://huizhixu.github.io/">Huizhi</a></span><span
                    class="path-divider">/</span><strong class="css-truncate-target mr-1" style="max-width: 410px"><a
                      href="https://huizhixu.github.io/post/20230209%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">2023-02-09 如何理解自注意力机制</a></strong>
                </h1>
                <div class="note m-0">
                  Created <relative-time datetime="Thu, 09 Feb 2023 08:31:50 &#43;0800"
                    class="no-wrap">
                    Thu, 09 Feb 2023 08:31:50 &#43;0800</relative-time>

                  
                  <span class="file-info-divider"></span>
                  Modified <relative-time datetime="Wed, 15 Feb 2023 14:16:48 &#43;0800"
                    class="no-wrap">
                    Wed, 15 Feb 2023 14:16:48 &#43;0800</relative-time>
                  
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container-lg px-3 new-discussion-timeline">
      <div class="repository-content gist-content">
        <div>
          <div class="js-gist-file-update-container js-task-list-container file-box">
            <div id="file-pytest" class="file my-2">
              <div id="post-header" class="file-header d-flex flex-md-items-center flex-items-start sticky-header" style="z-index: 2">
                <div class="file-info d-flex flex-md-items-center flex-items-start flex-order-1 flex-auto">
                  <div class="text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1 mt-2 mt-md-0">
                    
                    <summary id="toc-toggle" onclick="clickToc()" class="btn btn-octicon m-0 mr-2 p-2">
                      <svg aria-hidden="true" viewBox="0 0 16 16" height="16" width="16" class="octicon octicon-list-unordered">
                        <path fill-rule="evenodd" d="M2 4a1 1 0 100-2 1 1 0 000 2zm3.75-1.5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zm0 5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zm0 5a.75.75 0 000 1.5h8.5a.75.75 0 000-1.5h-8.5zM3 8a1 1 0 11-2 0 1 1 0 012 0zm-1 6a1 1 0 100-2 1 1 0 000 2z"></path>
                      </svg>
                    </summary>
                    <details-menu class="SelectMenu" id="toc-details" style="display: none;">
                      <div class="SelectMenu-modal rounded-3 mt-1" style="max-height: 340px;">
                        <div class="SelectMenu-list SelectMenu-list--borderless p-2" style="overscroll-behavior: contain;" id="toc-list">
                        </div>
                      </div>
                    </details-menu>
                      3115 Words
                    

                  </div>
                  <div class="file-actions flex-order-2 pt-0">
                    
                  </div>
                </div>
              </div>


              <div class="Box-body px-5 pb-5" style="z-index: 1">
                <article class="markdown-body entry-content container-lg"><h1 id="如何理解自注意力机制">如何理解自注意力机制</h1>
<h1 id="理解输入与输出">理解输入与输出</h1>
<ul>
<li>输入有可能是一个vector，有可能是多个vector</li>
<li>输出：
<ul>
<li>一个序列对应一个label。the whole sequence has a label
<ul>
<li>例子：在情感分析里面，This is good对应的输入是多个vector，输出为positive，是一个vector。</li>
</ul>
</li>
<li>一个vector对应一个label。一个序列对应多个label。
<ul>
<li>例子：在词性标注里面，This is good对应的输入是多个vector，输出为 代词，动词，形容词。</li>
</ul>
</li>
<li>模型决定Label的个数。seq2seq任务
<ul>
<li>例子：在机器翻译里面，This is good对应的输入是3个vector，中文翻译是”不错“，输出为2个vector。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="一个vector对应一个label的情况即输入和输出一样多也叫做sequence-labeling">一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling</h1>
<ul>
<li>例子： I saw a saw</li>
<li>如何解决sequence labeling的问题：用fully connected network对每一个input vector进行作用</li>
<li>弊端：
<ul>
<li>用fully connected network来输出，假设对I saw a saw做词性标注。对于FC层来说，两个saw没有什么不同，但是他们实际上一个是动词，一个是名词。</li>
</ul>
</li>
<li>解决思路：考虑更多的上下文。每一个fc层，都对所有的输入作用。或者给他一个window，作用于相邻的几个input vector。但是作用还是有限，计算也很复杂。</li>
</ul>
<p>我们想考虑整个sequence，但是不想把sequence所有的数据都包括在里面，就有了self-attention。</p>
<h1 id="self-attention">Self-attention</h1>
<p>如果要考虑上下文的信息，输入可以扔进self-attention之后，生成vector with context，然后丢进FC层，然后再过一次self-attention，然后再丢进FC层。</p>
<p>可以这样理解，self-attention获得上下文信息，FC层专注于局部信息，交替使用。</p>
<h2 id="运作原理">运作原理：</h2>
<ul>
<li>输入：是一串vector，可以是整个network的输入，也可以是FC层的输出，这里用a表示。</li>
<li>输出：一串vector，这里用b表示。每一个b都对所有a作用。（这个图乍一看很像上面的FC，但其实不一样）</li>
</ul>
<p><img src="/20230209/20230209_1.png" alt="输入和输出"></p>
<ul>
<li>如何产生$b^1$这个向量：
<ul>
<li>
<p>第一步：找出sequence里面跟$a^1$相关的其他向量。（FC层会把sequence所有的向量都包括进来，这里只包括相关的向量，这就是不同点。）这个机制叫做自注意机制。</p>
<ul>
<li>每一个向量跟$a^1$相关联的程度，我们用数值$\alpha$来表示。</li>
<li>Self-attention的module怎么决定两个向量的关联性呢？
<ul>
<li>内积：dot product （方法原理在文章最后面）</li>
<li>相加：additive（方法原理在文章最后面）</li>
</ul>
</li>
<li>这里的做法是：分别计算 $a^1$ 与 $a^2$， $a^3$，$a^4$ 的关联性</li>
<li>把$a^1$乘以$W^q$，得到$q^1$ ，$q^1$就是query，搜寻。</li>
<li>把$a^2$乘以$W^k$，得到$k^2$，$k^2$就是key，键。把$a^3$乘以$W^k$，得到$k^3$，把$a^4$乘以$W^k$，得到$k^4$。</li>
<li>那么$\alpha_{1,2} =q^1\cdot k^2$，关联性就算出来了。$\alpha_{1,2}$也叫attention score。</li>
<li>同理可以算出$\alpha_{1,3}，$$\alpha_{1,4}$。</li>
<li>一般我们也会算$q^1$和$k^1$的关联性，也就是$\alpha_{1,1}$。</li>
</ul>
<p><img src="/20230209/20230209_3.png" alt="attention结构"></p>
</li>
<li>
<p>第二步：对$\alpha_{1,1}$，$\alpha_{1,2}$，$\alpha_{1,3}$，$\alpha_{1,4}$进行softmax操作，得到$\alpha \prime$。</p>
<ul>
<li>为什么要用softmax：softmax最常见，这里也可以用其他函数，可以用relu等。（这一步没懂）</li>
</ul>
</li>
<li>
<p>第三步：根据$\alpha\prime$，在sequence里面抽取重要的信息。</p>
<ul>
<li>做法：将$a^1$，$a^2$，$a^3$，$a^4$分别乘以$W^v$，得到$v^1，$$v^2，$$v^3$，$v^4$。  （其实这里$v^1，v^2，v^3，v^4$就是$a^1，a^2，a^3，a^4$自己的等价表示）</li>
<li>将$v^1$到$v^4$都乘以分别的$\alpha \prime$， 然后再相加。</li>
</ul>
<p>$$
b^1  = \sum_i \alpha\prime_{1,i} v^i
$$</p>
<ul>
<li>
<p>如果$a^1$和$a^2$的关联性很强，$\alpha\prime_{1,2}$的值很大，那么经过weighted sum得到的$b^1$的值就可能会接近$v^2$。</p>
<p>也就是说，谁的attention score越大，谁的v就会dominate抽出来的结果。（这里也没有看懂~~~）</p>
</li>
</ul>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-08_17.59.23.png" alt="截屏2023-02-08 17.59.23.png"></p>
</li>
</ul>
</li>
</ul>
<h1 id="heading"></h1>
<h1 id="qkv从矩阵的角度看self-attention">QKV（从矩阵的角度看self-attention）</h1>
<ol>
<li>把a^1到a^4看成是矩阵I的列，q^1到q^4是矩阵Q的列，那么可以转化成矩阵间的运算。</li>
</ol>
<p>矩阵I乘以Wq，得到矩阵Q，Q的4列就是q1到q4。</p>
<p>同样，输入矩阵I乘以Wk，得到矩阵K，K的4列就是k1到k4。</p>
<p>输入I乘上三个不同的矩阵，就得到了QKV。</p>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_10.29.47.png" alt="截屏2023-02-09 10.29.47.png"></p>
<ol>
<li>
<p>每一个q会和每一个k计算内积，得到attention score。</p>
<ol>
<li>
<p>矩阵和向量相乘：</p>
<ol>
<li>矩阵K和q^1内积，得到a^1和其他输入的相关性。</li>
<li>矩阵K和q^2内积，得到a^2和其他输入的相关性。</li>
<li>合起来，就是</li>
</ol>
<p>$$
\begin{array}{cc}
\alpha_{1,1} &amp; \alpha_{2,1} &amp;   \alpha_{3,1} &amp; \alpha_{4,1} \
\alpha_{1,2} &amp; \alpha_{2,2} &amp; \alpha_{3,2} &amp; \alpha_{4,2} \
\alpha_{1,3} &amp; \alpha_{2,3} &amp; \alpha_{3,3} &amp; \alpha_{4,3}\
\alpha_{1,4} &amp; \alpha_{2,4} &amp; \alpha_{3,4} &amp; \alpha_{4,4}
\end{array} =
\begin{array}{cc}
k^1 \
k^2 \
k^3 \
k^4 \<br>
\end{array}</p>
<p>\begin{array}{cc}
q^1 &amp;
q^2 &amp;
q^3 &amp;
q^4 &amp;
\end{array}
$$</p>
<p>简写为</p>
<p>$$
A = K^T \sdot Q
$$</p>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.09.24.png" alt="截屏2023-02-09 17.09.24.png"></p>
</li>
</ol>
</li>
<li>
<p>对$A$做softmax，得到$A^\prime$c</p>
</li>
<li>
<p>V乘以A‘得到B</p>
<ul>
<li>v1乘上weight \alpha\prime， 加上v2乘上weight \alpha\prime，等等，得到b1</li>
<li>v1乘上weight \alpha\prime， 加上v2乘上weight \alpha\prime，等等，得到b2</li>
<li>O就是Output</li>
</ul>
</li>
</ol>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.18.40.png" alt="截屏2023-02-09 17.18.40.png"></p>
<p>总结：先产生QKV，根据Q找出相关的位置，再对V做weighted sum。</p>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.22.53.png" alt="截屏2023-02-09 17.22.53.png"></p>
<h1 id="怎么得到wqwk和wv">怎么得到Wq、Wk和Wv</h1>
<p>初始化一个矩阵，通过training data 在训练过程中更新。</p>
<h1 id="multi-head-self-attention">Multi-head Self-attention</h1>
<h2 id="为什么需要多的head">为什么需要多的head？</h2>
<h3 id="关于相关性">关于相关性</h3>
<ul>
<li>在self-attention里面，是根据Q来找相关的K，但是相关性有很多种不同的形式和定义。</li>
<li>所以应该要有多个Q，不同的Q负责不同种类的相关性。</li>
</ul>
<h3 id="如何设计多头">如何设计多头？</h3>
<ul>
<li>a乘上Wq得到q，q再乘上其他两个矩阵得到q_{i,1}和q_{i,2}
<ul>
<li>i代表的是位置，1和2代表的是第几个q。这里两个q负责两种不同的相关性。</li>
</ul>
</li>
<li>同理，有两个q，就要有两个k和两个v</li>
</ul>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.38.22.png" alt="截屏2023-02-09 17.38.22.png"></p>
<h3 id="多头如何做自注意力">多头如何做自注意力？</h3>
<ul>
<li>q_1只管和k_1相乘，不用管是哪个位置的。（这里有位置i和j）</li>
<li>也只和v_1相乘，对不同位置的v1做weighted sum，得到b_1</li>
</ul>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.41.37.png" alt="截屏2023-02-09 17.41.37.png"></p>
<ul>
<li>然后对b1和b2做矩阵相乘，得到b</li>
</ul>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.44.55.png" alt="截屏2023-02-09 17.44.55.png"></p>
<h1 id="self-attention的弊端">Self-attention的弊端：</h1>
<ul>
<li>没有考虑到位置信息
<ul>
<li>每一个input vector出现在sequence的最前面还是最后面在模型里面是体现不出来的。（位置i和j是没有差别的）</li>
<li>位置很重要:
<ul>
<li>例子：POS动词不会出现在句首</li>
</ul>
</li>
</ul>
</li>
<li>如何解决：
<ul>
<li>positional encoding
<ul>
<li>设定一个vector，叫做positional vector  e^i</li>
<li>把e_i加上a_i，作为输入</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="positional-encoding">positional encoding</h1>
<ul>
<li>e_i长什么样
<ul>
<li>人类为每个位置设置一个专属的vector。（handcrafted）</li>
<li>论文里面，是sin 和cos 设置的。（可以有其他的方法）</li>
<li></li>
</ul>
</li>
</ul>
<pre><code>    ![截屏2023-02-09 17.59.29.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.59.29.png)
</code></pre>
<h1 id="需要多少head">需要多少head？</h1>
<h1 id="自注意模块计算两个向量的关联性">自注意模块计算两个向量的关联性</h1>
<h2 id="内积方法"><strong>内积方法</strong></h2>
<p>左边的向量乘以W_q矩阵，右边的向量乘以W_k矩阵，得到q和k这两个向量。然后做内积，得到一个scala标量。</p>
<p>$$
\alpha = q /dot k
$$</p>
<p>(妈耶为什么我Latex全都忘光了)</p>
<h2 id="相加方法"><strong>相加方法</strong></h2>
<p>左边的向量乘以W_q矩阵，右边的向量乘以W_k矩阵，得到q和k这两个向量。然后相加丢到tanh，再乘以W，得到alpha。</p>
<p><img src="%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-08_17.01.34.png" alt="截屏2023-02-08 17.01.34.png"></p>
<h1 id="参考">参考：</h1>
<p>李宏毅老师的讲课视频</p>
<p><a href="https://www.bilibili.com/video/BV1v3411r78R?p=2&amp;vd_source=bbd38c44460fafa204a3540d4f8a2657">11.【李宏毅机器学习2021】自注意力机制 (Self-attention) (下)_哔哩哔哩_bilibili</a></p>
</article>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </main>
</div>
<script type="application/javascript" src='https://huizhixu.github.io/js/toc.js'></script>
<link rel="stylesheet" href='https://huizhixu.github.io/css/toc.css' />


  <div class="footer container-xl width-full p-responsive">
  <div
    class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light ">
    <a aria-label="Homepage" title="GitHub" class="footer-octicon d-none d-lg-block mr-lg-4" href="https://huizhixu.github.io/">
      <svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" version="1.1" width="24">
        <path fill-rule="evenodd"
          d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z">
        </path>
      </svg>
    </a>
    <ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0">
      
      <li class="mr-3 mr-lg-0">Theme by <a href='https://github.com/MeiK2333/github-style'>github-style</a></li>
      
    </ul>
  </div>
  <div class="d-flex flex-justify-center pb-6">
    <span class="f6 text-gray-light"></span>
  </div>


</div>
</body>

<script type="application/javascript" src="https://huizhixu.github.io/js/github-style.js"></script>




</html>