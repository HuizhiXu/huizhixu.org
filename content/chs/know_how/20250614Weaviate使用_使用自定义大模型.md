---
title: "2025-06-14 Weaviate使用（二） 使用自定义模型"
date: 2025-06-14T12:50:33.865857
tags: ['tech']
description: ""
---

如果不想用Ollama启用的向量模型，想用自定义的模型有多种方法，下面介绍两种：一种是通过huggingface或者其他的框架导入，另一种是直接调用已有的向量调用的服务（例如用FastAPI启动）。

把向量服务或者大模型封装成服务的好处是：

1. 功能解耦：大模型服务或者向量服务独立出来，在替换的时候无需更改代码
1. 环境解耦：大模型服务通常需要更多的资源。


这时的docker compose file 非常简单，不再需要ENABLE_MODULES: 'text2vec-ollama,generative-ollama' 这一行。



```javascript
---
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.30.6
    ports:
    - 8080:8080
    - 50051:50051
    volumes:
    - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      ENABLE_API_BASED_MODULES: 'true'
      CLUSTER_HOSTNAME: 'node1'
volumes:
  weaviate_data:
...

```



### 通过hugging face导入

```javascript
def vectorize(texts: List[str]):
   
    from FlagEmbedding import BGEM3FlagModel

    model = BGEM3FlagModel('BAAI/bge-m3')
    embeddings = model.encode(texts)["dense_vecs"]
    

    return embeddings
```



### 通过API调用

```javascript
class CustomEmbedding(EmbedGen):
    def generate_embeddings(self, texts: List[str]) -> np.array:
        import requests
        import json
        payload = json.dumps(
            {
                "input": texts,
            }
        )
        headers = {"Content-Type": "application/json"}

        response = requests.request("POST", url=EMBED_URL, headers=headers, data=payload)

        embeddings = [i["embedding"] for i in response.json()["data"]]
        embeddings = np.array(embeddings)

        return embeddings
 
```









 