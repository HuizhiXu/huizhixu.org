---
title: "2023-02-09 如何理解自注意力机制"  
date: 2023-02-09T08:31:50+08:00  
draft: false  
pin: false 
summary: "Attention is all you need."  
---


# 如何理解自注意力机制


# 理解输入与输出

- 输入有可能是一个vector，有可能是多个vector
- 输出：
    - 一个序列对应一个label。the whole sequence has a label
        - 例子：在情感分析里面，This is good对应的输入是多个vector，输出为positive，是一个vector。
    - 一个vector对应一个label。一个序列对应多个label。
        - 例子：在词性标注里面，This is good对应的输入是多个vector，输出为 代词，动词，形容词。
    - 模型决定Label的个数。seq2seq任务
        - 例子：在机器翻译里面，This is good对应的输入是3个vector，中文翻译是”不错“，输出为2个vector。

# 一个vector对应一个label的情况，即输入和输出一样多，也叫做sequence labeling

- 例子： I saw a saw
- 如何解决sequence labeling的问题：用fully connected network对每一个input vector进行作用
- 弊端：
    - 用fully connected network来输出，假设对I saw a saw做词性标注。对于FC层来说，两个saw没有什么不同，但是他们实际上一个是动词，一个是名词。
- 解决思路：考虑更多的上下文。每一个fc层，都对所有的输入作用。或者给他一个window，作用于相邻的几个input vector。但是作用还是有限，计算也很复杂。

我们想考虑整个sequence，但是不想把sequence所有的数据都包括在里面，就有了self-attention。

# Self-attention

如果要考虑上下文的信息，输入可以扔进self-attention之后，生成vector with context，然后丢进FC层，然后再过一次self-attention，然后再丢进FC层。

可以这样理解，self-attention获得上下文信息，FC层专注于局部信息，交替使用。

## 运作原理：

- 输入：是一串vector，可以是整个network的输入，也可以是FC层的输出，这里用a表示。
- 输出：一串vector，这里用b表示。每一个b都对所有a作用。（这个图乍一看很像上面的FC，但其实不一样）

![输入和输出](/20230209/20230209_1.png)

- 如何产生$b^1$这个向量：
    - 第一步：找出sequence里面跟$a^1$相关的其他向量。（FC层会把sequence所有的向量都包括进来，这里只包括相关的向量，这就是不同点。）这个机制叫做自注意机制。
        - 每一个向量跟$a^1$相关联的程度，我们用数值$\alpha$来表示。
        - Self-attention的module怎么决定两个向量的关联性呢？
            - 内积：dot product （方法原理在文章最后面）
            - 相加：additive（方法原理在文章最后面）
        - 这里的做法是：分别计算 $a^1$ 与 $a^2$， $a^3$，$a^4$ 的关联性
        - 把$a^1$乘以$W^q$，得到$q^1$ ，$q^1$就是query，搜寻。
        - 把$a^2$乘以$W^k$，得到$k^2$，$k^2$就是key，键。把$a^3$乘以$W^k$，得到$k^3$，把$a^4$乘以$W^k$，得到$k^4$。
        - 那么$\alpha_{1,2} =q^1\cdot k^2$，关联性就算出来了。$\alpha_{1,2}$也叫attention score。
        - 同理可以算出$\alpha_{1,3}，$$\alpha_{1,4}$。
        - 一般我们也会算$q^1$和$k^1$的关联性，也就是$\alpha_{1,1}$。
        
        ![attention结构](/20230209/20230209_3.png)
        
    
    - 第二步：对$\alpha_{1,1}$，$\alpha_{1,2}$，$\alpha_{1,3}$，$\alpha_{1,4}$进行softmax操作，得到$\alpha \prime$。
        - 为什么要用softmax：softmax最常见，这里也可以用其他函数，可以用relu等。（这一步没懂）
    - 第三步：根据$\alpha\prime$，在sequence里面抽取重要的信息。
        - 做法：将$a^1$，$a^2$，$a^3$，$a^4$分别乘以$W^v$，得到$v^1，$$v^2，$$v^3$，$v^4$。  （其实这里$v^1，v^2，v^3，v^4$就是$a^1，a^2，a^3，a^4$自己的等价表示）
        - 将$v^1$到$v^4$都乘以分别的$\alpha \prime$， 然后再相加。
        
        $$
        b^1  = \sum_i \alpha\prime_{1,i} v^i
        $$
        
        - 如果$a^1$和$a^2$的关联性很强，$\alpha\prime_{1,2}$的值很大，那么经过weighted sum得到的$b^1$的值就可能会接近$v^2$。
            
            也就是说，谁的attention score越大，谁的v就会dominate抽出来的结果。（这里也没有看懂~~~）
            
        
        ![截屏2023-02-08 17.59.23.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-08_17.59.23.png)
        
    

# 

# QKV（从矩阵的角度看self-attention）

1. 把a^1到a^4看成是矩阵I的列，q^1到q^4是矩阵Q的列，那么可以转化成矩阵间的运算。

矩阵I乘以Wq，得到矩阵Q，Q的4列就是q1到q4。

同样，输入矩阵I乘以Wk，得到矩阵K，K的4列就是k1到k4。

输入I乘上三个不同的矩阵，就得到了QKV。

![截屏2023-02-09 10.29.47.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_10.29.47.png)

1. 每一个q会和每一个k计算内积，得到attention score。 
    1. 矩阵和向量相乘：
        1. 矩阵K和q^1内积，得到a^1和其他输入的相关性。
        2. 矩阵K和q^2内积，得到a^2和其他输入的相关性。
        3. 合起来，就是
        
        $$
        \begin{array}{cc}
        \alpha_{1,1} & \alpha_{2,1} &   \alpha_{3,1} & \alpha_{4,1} \\ 
        \alpha_{1,2} & \alpha_{2,2} & \alpha_{3,2} & \alpha_{4,2} \\
        \alpha_{1,3} & \alpha_{2,3} & \alpha_{3,3} & \alpha_{4,3}\\
        \alpha_{1,4} & \alpha_{2,4} & \alpha_{3,4} & \alpha_{4,4}
        \end{array} = 
        \begin{array}{cc}
           k^1 \\ 
           k^2 \\
         k^3 \\ 
         k^4 \\  
        \end{array}
        
        \begin{array}{cc}
           q^1 &
           q^2 &
         q^3 &
         q^4 & 
        \end{array}
        $$
        
        简写为
        
        $$
        A = K^T \sdot Q 
        $$
        
        ![截屏2023-02-09 17.09.24.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.09.24.png)
        
    

1. 对$A$做softmax，得到$A^\prime$c
2. V乘以A‘得到B
    - v1乘上weight \alpha\prime， 加上v2乘上weight \alpha\prime，等等，得到b1
    - v1乘上weight \alpha\prime， 加上v2乘上weight \alpha\prime，等等，得到b2
    - O就是Output

![截屏2023-02-09 17.18.40.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.18.40.png)

总结：先产生QKV，根据Q找出相关的位置，再对V做weighted sum。

![截屏2023-02-09 17.22.53.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.22.53.png)

# 怎么得到Wq、Wk和Wv

初始化一个矩阵，通过training data 在训练过程中更新。

# Multi-head Self-attention

## 为什么需要多的head？

### 关于相关性

- 在self-attention里面，是根据Q来找相关的K，但是相关性有很多种不同的形式和定义。
- 所以应该要有多个Q，不同的Q负责不同种类的相关性。

### 如何设计多头？

- a乘上Wq得到q，q再乘上其他两个矩阵得到q_{i,1}和q_{i,2}
    - i代表的是位置，1和2代表的是第几个q。这里两个q负责两种不同的相关性。
- 同理，有两个q，就要有两个k和两个v

![截屏2023-02-09 17.38.22.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.38.22.png)

### 多头如何做自注意力？

- q_1只管和k_1相乘，不用管是哪个位置的。（这里有位置i和j）
- 也只和v_1相乘，对不同位置的v1做weighted sum，得到b_1

![截屏2023-02-09 17.41.37.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.41.37.png)

- 然后对b1和b2做矩阵相乘，得到b

![截屏2023-02-09 17.44.55.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.44.55.png)

# Self-attention的弊端：

- 没有考虑到位置信息
    - 每一个input vector出现在sequence的最前面还是最后面在模型里面是体现不出来的。（位置i和j是没有差别的）
    - 位置很重要:
        - 例子：POS动词不会出现在句首
- 如何解决：
    - positional encoding
        - 设定一个vector，叫做positional vector  e^i
        - 把e_i加上a_i，作为输入

# positional encoding

- e_i长什么样
    - 人类为每个位置设置一个专属的vector。（handcrafted）
    - 论文里面，是sin 和cos 设置的。（可以有其他的方法）
    - 
        
        ![截屏2023-02-09 17.59.29.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-09_17.59.29.png)
        

# 需要多少head？

# 自注意模块计算两个向量的关联性

## **内积方法**

左边的向量乘以W_q矩阵，右边的向量乘以W_k矩阵，得到q和k这两个向量。然后做内积，得到一个scala标量。

$$
\alpha = q /dot k 
$$

(妈耶为什么我Latex全都忘光了)

## **相加方法**

左边的向量乘以W_q矩阵，右边的向量乘以W_k矩阵，得到q和k这两个向量。然后相加丢到tanh，再乘以W，得到alpha。

![截屏2023-02-08 17.01.34.png](%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2060054d4448134accaab9f50cdda8e28f/%25E6%2588%25AA%25E5%25B1%258F2023-02-08_17.01.34.png)

# 参考：

李宏毅老师的讲课视频

[11.【李宏毅机器学习2021】自注意力机制 (Self-attention) (下)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1v3411r78R?p=2&vd_source=bbd38c44460fafa204a3540d4f8a2657)