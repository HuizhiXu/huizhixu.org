---
author: "Huizhi"
title:  "2023-06-02 季读——2023年（二）"  
date: 2023-06-02T18:31:50+08:00  
lastmod: 2023-06-30
description: "向量数据库很好用。"
tags: ["emoji"]
draft: false
pin: false
thumbnail: https://picsum.photos/id/62/400/250
---


今天早上醒来，看到朋友圈大家都在（给娃）庆祝儿童节，发现居然已经到了6月了。想起自己的季读还没写多少。😄

由于行业技术的快速发展，这个季度基本都在一个提高认知、拓宽眼界的技术氛围里。因此，我在这段时间里阅读的大部分文章都与技术有关。

1. 《Prompt Tuning的万字综述》 （https://wjn1996.blog.csdn.net/article/details/120607050）

读了四个多小时，没有全部读完。之前对Prompt Tuning的理解仅仅停留在第三章：如何构建模板，但是到第三章只是入门，后面还有更多的细节。最精髓的就是一句话：prompt的本质是参数有效性学习。

2. 《LLM as Controller 无限拓展LLM的能力边界》（https://zhuanlan.zhihu.com/p/626736120）

作者的抽象能力很强，它描述了LangChain那一套如何运作的，让我很受启发。

它把大模型回答问题的这个过程抽象成一个系统，这个系统只包括LLM和Agent，其中LLM理解输入并且将输入转化成不同的指令，Agent接收指令并行动。假设LLM理解能力很强，100%理解输入的意思，Agent 力量很强，100%能够执行命令。那么，理论上这个LLM+Agent的组合能够做任何事情。

那这样的话，可以进一步缩小空间，问题变成下面两个问题：第一，LLM对输入进行理解之后，转变成怎样的指令，才能被更好地被Agent执行？第二，Agent要去哪里执行指令，才能找到更好的回答？

下面是一些例子，越往下，功能越多，也越难做。这一层一层叠加，真是牛啊~

Visual ChatGPT：单一任务——agent去一些基于视觉的模型里面找答案。

HuggingGPT：多重任务——agent根据不同的输入去不同的模型（基于hf hub）里面找答案。

Toolformer：多重任务——agent的范围更大，是网络上的不同的API（例如谷歌搜索、谷歌翻译）等。

AutoGPT：多重任务——llm和agent能自我迭代。agent会不断反馈，llm根据反馈的答案调整生成更好的指令，形成正向反馈。

3. 《它帮大语言模型消除“幻觉”，一个月内三家向量数据库创业公司获新融资》（https://mp.weixin.qq.com/s/Fhz2O03JkdqZWug2cF7v_A）

为啥大家的目光最近会聚集在向量数据库上面呢？主要是由于大模型的缺陷。

向量数据库是怎么用的呢？以下是一个理解。

假设我们现在有一堆文档，内容是某个保险领域的所有条款。用户提出一些问题，例如，用户问：老人在什么情况下可以投某种保险？我们想要GPT4在这堆文档中找出答案，回答用户。

首先，要知道的是，GPT4输入的token长度是有限制的。大模型只能输入几千个token，但这堆保险文档有几百万个token，大模型它没法一次读啊。

很容易想到，大模型没法一次读，那就拆解让它读多次就好啦~ 

但是，拆解也是不OK的。主要原因是一，这几百万个token截断之后再拼起来的效果不好。二，太贵了，太慢了。调用GPT4几万次只为回答一个问题，没有人会这么做。

那么就用到向量数据库了。向量数据库会存向量，也是就一堆拥有很多中括号和小数的数值。它一般用来做相似度查找。

我们可以把上面的文档都存在向量数据库里，把用户的问题也转化为向量，然后去搜相似的文本。文本找出来了之后，再传给大模型，让它去分析，给出答案。

这样大模型的输入是不是一下子从百万级变成了万千级，而且向量数据库搜索的效率也很快，所以理论上整个流程就打通了。

这个本地知识库，其实在每个领域都能应用，特别是文本资料很多的法律、保险、金融领域。

但是，向量数据库是一个中间产物。如果我们基于某一个专业领域的大量数据训练了一个大模型，它本身读了很多这个领域的知识，所有的知识点它都了然于心，那么就不需要向量数据库了。或者，如果大模型的输入支持百万、千万数量级的token，也不需要向量数据库了。未来有一天或许能实现呢？

4. 《最早出发的中国大模型创业者：“贫穷限制了我们的想象力”》

这篇文章我读了好多遍，虽然我经历没有周博士那么多，看得没有那么远，但在一些方面也感同身受。还蛮佩服他19年出来创业的，因为后面就是AI寒冬了。那时AI领域像是一潭死气沉沉的冬水，大家发现算法也不是万能的呀，还不如规则和廉价人工好使（狗头）。然后今年，情况明显不一样了，突然变成了so-called ”AI盛世“。但是大模型真的能快速落地吗？

特别是在周博士在的金融领域，有两个限制：一是对结果要求非常准确；二是国内的企业（专指国企央企）有信创的需求。这种情况下，如何去研发大模型，要研发怎样的大模型呢？

他这里面说“贫穷限制了想象力”，在1980年做开发的时候需要考虑节省算力。刚好《黑客与画家》里面也写了“他们在编程的时候需要删去一部分代码，为了节省内存”，但是大模型需要的就是海量数据+超乎想象的算力。

看完文章，我深深地感受到：时代的局限带给单个个体的影响，近乎一种残忍，哪怕是超级有能力的人，也无法跳出禁锢，上一代人没有能力在千亿参数层级去想象，这就已经决定了结果。

5. 《疯狂的幻方：一家隐形AI巨头的大模型之路》（https://mp.weixin.qq.com/s/T-ccVKG_LS4OvUXQIfsoeg）

一家宣称不做垂类和应用，只做研究的公司。钱和算力都有，就是不知道有没有技术，哈哈持怀疑态度。（真羡慕他们的算力啊~ ）

最近的技术变化得实在太快了。在非常tough地学习了一天的新知识之后，精疲力尽地，第二天早上起来，发现又出来新的东西了。而且很有可能，昨天学的过时了。。。

所以，也看了不会过时的文章和书：

6. 《从权力和垄断的演化机制，看投资(一）》

这篇文章的最重要的话就是：

权力斗争的关键就是 i) 在关键环节上，尽可能让自己不可替代。ii) 同时在与自己合作的关键环节上，确保有替代者可互相制约。

这篇文章是纠结要不要跳槽的那时候读的，让我看清了自己在前公司的地位：我的可替代性太强了。这也提醒我了，要去慢慢地构建自己的生态圈。

7. 《软件工程》——李爱萍 

8. 《黑客与画家》

这两本书的内容下次写吧。